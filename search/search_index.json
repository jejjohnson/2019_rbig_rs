{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RBIG for Spatial-Temporal Exploration of Earth Science Data \u00b6 Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Twitter: jejjohnson Github: jejjohnson Website: jejjohnson.netlify.app Research Journal: jejjohnson.github.io/research_journal Repository: jejjohnson.github.io/rbig_eo This repo has the experiments and reproducible code for all of my experiments using RBIG for analyzing Earth science data. I am primarily focused on using RBIG to measure the information content for datasets with spatial-temporal features. I look at IT measures such as Entropy, Mutual Information and Total Correlation. The strength of RBIG lies in it's ability to handle multivariate and high dimensional datasets. I will be periodically updating this repo as I finish more experiments. I will also make the code even more reproducible as I learn some of the best practices. For now, I would advise you to look at the notebooks. Example Experiments \u00b6 I have included some example experiments in the notebooks folder including the following experiments: Global Information Content (TODO) Spatial-Temporal Analysis of variables Temporal analysis of Drought Indicators Climate Model Comparisons Installation Instructions \u00b6 Firstly , you need to clone the following RBIG repo and install/put in PYTHONPATH git clone https : // github . com / jejjohnson / rbig Secondly , you can create the environment from the .yml file found in the main repo. conda env create - f environment . yml - n myenv source activate myenv Conferences \u00b6 Estimating Information in Earth Data Cubes - Johnson et. al. - EGU 2018 Multivariate Gaussianization in Earth and Climate Sciences - Johnson et. al. - Climate Informatics 2019 - repo Climate Model Intercomparison with Multivariate Information Theoretic Measures - Johnson et. al. - AGU 2019 - slides Journal Articles \u00b6 Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2011) - IEEE Transactions on Neural Networks Information theory measures and RBIG for Spatial-Temporal Data analysis - Johnson et. al. - In progress External Toolboxes \u00b6 RBIG (Rotation-Based Iterative Gaussianization) This is a package I created to implement the RBIG algorithm. This is a multivariate Gaussianization method that allows one to calculate information theoretic measures such as entropy, total correlation and mutual information. More information can be found in the repository esdc_tools . Earth Science Data Cube Tools These are a collection of useful scripts when dealing with datacubes (datasets in xarray Dataset format). I used a few preprocessing methods as well as a Minicuber implementation which transforms the data into spatial and/or temporal features. More information can be found in the repository py_rbig . Data Resources \u00b6 Earth System Data Lab This is sponsered by the earthsystemdatalab . They host a cube on their servers which include over 40+ variables including soil moisture and land surface temperature. They also feature a free-to-use JupyterHub server for easy exploration of th data. Climate Data Store This is a database of climate models implemented by the ECMWF and sponsored by the Copernicus program. I use a few climate models from here by using the CDSAPI. There are some additional instructions to install this package which requires registration and agreeing to some terms of use for each dataset. Contact Information \u00b6 Links to my co-authors' and my information: J. Emmanuel Johnson - Website | Scholar | Twitter | Github Maria Piles - Website | Scholar | Twitter Valero Laparra - Website | Scholar Gustau Camps-Valls - Website | Scholar | Twitter","title":"RBIG4EO"},{"location":"#rbig-for-spatial-temporal-exploration-of-earth-science-data","text":"Author: J. Emmanuel Johnson Email: jemanjohnson34@gmail.com Twitter: jejjohnson Github: jejjohnson Website: jejjohnson.netlify.app Research Journal: jejjohnson.github.io/research_journal Repository: jejjohnson.github.io/rbig_eo This repo has the experiments and reproducible code for all of my experiments using RBIG for analyzing Earth science data. I am primarily focused on using RBIG to measure the information content for datasets with spatial-temporal features. I look at IT measures such as Entropy, Mutual Information and Total Correlation. The strength of RBIG lies in it's ability to handle multivariate and high dimensional datasets. I will be periodically updating this repo as I finish more experiments. I will also make the code even more reproducible as I learn some of the best practices. For now, I would advise you to look at the notebooks.","title":"RBIG for Spatial-Temporal Exploration of Earth Science Data"},{"location":"#example-experiments","text":"I have included some example experiments in the notebooks folder including the following experiments: Global Information Content (TODO) Spatial-Temporal Analysis of variables Temporal analysis of Drought Indicators Climate Model Comparisons","title":"Example Experiments"},{"location":"#installation-instructions","text":"Firstly , you need to clone the following RBIG repo and install/put in PYTHONPATH git clone https : // github . com / jejjohnson / rbig Secondly , you can create the environment from the .yml file found in the main repo. conda env create - f environment . yml - n myenv source activate myenv","title":"Installation Instructions"},{"location":"#conferences","text":"Estimating Information in Earth Data Cubes - Johnson et. al. - EGU 2018 Multivariate Gaussianization in Earth and Climate Sciences - Johnson et. al. - Climate Informatics 2019 - repo Climate Model Intercomparison with Multivariate Information Theoretic Measures - Johnson et. al. - AGU 2019 - slides","title":"Conferences"},{"location":"#journal-articles","text":"Iterative Gaussianization: from ICA to Random Rotations - Laparra et. al. (2011) - IEEE Transactions on Neural Networks Information theory measures and RBIG for Spatial-Temporal Data analysis - Johnson et. al. - In progress","title":"Journal Articles"},{"location":"#external-toolboxes","text":"RBIG (Rotation-Based Iterative Gaussianization) This is a package I created to implement the RBIG algorithm. This is a multivariate Gaussianization method that allows one to calculate information theoretic measures such as entropy, total correlation and mutual information. More information can be found in the repository esdc_tools . Earth Science Data Cube Tools These are a collection of useful scripts when dealing with datacubes (datasets in xarray Dataset format). I used a few preprocessing methods as well as a Minicuber implementation which transforms the data into spatial and/or temporal features. More information can be found in the repository py_rbig .","title":"External Toolboxes"},{"location":"#data-resources","text":"Earth System Data Lab This is sponsered by the earthsystemdatalab . They host a cube on their servers which include over 40+ variables including soil moisture and land surface temperature. They also feature a free-to-use JupyterHub server for easy exploration of th data. Climate Data Store This is a database of climate models implemented by the ECMWF and sponsored by the Copernicus program. I use a few climate models from here by using the CDSAPI. There are some additional instructions to install this package which requires registration and agreeing to some terms of use for each dataset.","title":"Data Resources"},{"location":"#contact-information","text":"Links to my co-authors' and my information: J. Emmanuel Johnson - Website | Scholar | Twitter | Github Maria Piles - Website | Scholar | Twitter Valero Laparra - Website | Scholar Gustau Camps-Valls - Website | Scholar | Twitter","title":"Contact Information"},{"location":"experiments/","text":"Experiments \u00b6 In this document, I outline some of the experiments Experiment I \u00b6 For the first experiment, I would like to see if there is a difference between each of the methods and what interactions are they able to capture. Independent Variables Regions Europe Spain USA Africa Time Periods July 2010 Jan 2010 Variables: Gross Primary Productivity (GPP) Root Soil Moisture (RSM) Land Surface Temperature (LST) Methods: pV Coefficient - (Norm, pV score) CKA w. RBF Kernel - (K Norm, CKA score) RBIG Method - (H, I) Dependent Variables For the dependent variables, we want to see how the spatial and temporal features change the outcome of the scores that we calculate. We will vary the amount of spatial dimensions Spatial v Temporal Dimensions Spatial Dims - (1, 2, 3, 4, 5) Temporal Dims - (1, 2, 3, 4, 5) Hypothesis I'm assuming that the PDF estimation with RBIG will be superior. We are adding a large amount of dimension/features to the inputs. I suspect that this can be potentially useful up to a certain point. Due to the amount of information included, some methods will be able to capture the changes while other methods no. Some things to keep in mind about the methods: RBIG is designed for multivariate/multidimensional data. pV Coefficient is a linear method which cannot capture non-linear relationships CKA is a non-linear method but it may not capture the relationships due to the parameter estimation","title":"Experiments"},{"location":"experiments/#experiments","text":"In this document, I outline some of the experiments","title":"Experiments"},{"location":"experiments/#experiment-i","text":"For the first experiment, I would like to see if there is a difference between each of the methods and what interactions are they able to capture. Independent Variables Regions Europe Spain USA Africa Time Periods July 2010 Jan 2010 Variables: Gross Primary Productivity (GPP) Root Soil Moisture (RSM) Land Surface Temperature (LST) Methods: pV Coefficient - (Norm, pV score) CKA w. RBF Kernel - (K Norm, CKA score) RBIG Method - (H, I) Dependent Variables For the dependent variables, we want to see how the spatial and temporal features change the outcome of the scores that we calculate. We will vary the amount of spatial dimensions Spatial v Temporal Dimensions Spatial Dims - (1, 2, 3, 4, 5) Temporal Dims - (1, 2, 3, 4, 5) Hypothesis I'm assuming that the PDF estimation with RBIG will be superior. We are adding a large amount of dimension/features to the inputs. I suspect that this can be potentially useful up to a certain point. Due to the amount of information included, some methods will be able to capture the changes while other methods no. Some things to keep in mind about the methods: RBIG is designed for multivariate/multidimensional data. pV Coefficient is a linear method which cannot capture non-linear relationships CKA is a non-linear method but it may not capture the relationships due to the parameter estimation","title":"Experiment I"},{"location":"ideas/","text":"Ideas \u00b6 Main Project \u00b6 Abstract For the main project, I will be investigating how we can RBIG to examine the spatial-temporal relationships within the data Applications \u00b6 Drought Indicators \u00b6 Abstract As a potential application, I would like to investigate how we can use Information theory measures to look at drought variable indicators. Climate Models \u00b6 Abstract I will look into how we can compare different climate models. Part I : we can look at different measurements and Part II : we can look at how the spatial-temporal relationships can affect the comparison metrics.","title":"Ideas"},{"location":"ideas/#ideas","text":"","title":"Ideas"},{"location":"ideas/#main-project","text":"Abstract For the main project, I will be investigating how we can RBIG to examine the spatial-temporal relationships within the data","title":"Main Project"},{"location":"ideas/#applications","text":"","title":"Applications"},{"location":"ideas/#drought-indicators","text":"Abstract As a potential application, I would like to investigate how we can use Information theory measures to look at drought variable indicators.","title":"Drought Indicators"},{"location":"ideas/#climate-models","text":"Abstract I will look into how we can compare different climate models. Part I : we can look at different measurements and Part II : we can look at how the spatial-temporal relationships can affect the comparison metrics.","title":"Climate Models"},{"location":"todo/","text":"To-Do \u00b6 Logistics \u00b6 - Build Website - Do Readme - Add Notebooks - Add Writing Tasks - Clean Code","title":"TODO"},{"location":"todo/#to-do","text":"","title":"To-Do"},{"location":"todo/#logistics","text":"- Build Website - Do Readme - Add Notebooks - Add Writing Tasks - Clean Code","title":"Logistics"},{"location":"notebooks/climate/0_visualize/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Visually Comparing Climate Models \u00b6 Summary \u00b6 In this notebook, I will be comparing three climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 CMIP5 I will be looking at the following variables: Mean Sea Level Pressure (CMIP5, ERA5, NCEP) Surface Pressure (ERA5, NCEP) I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures. If these climate models are that similar, then they should exhibit similar IT measures. Preprocessing Steps \u00b6 Regridded Spatially The ERA5 had the coarsest spatial resolution (2.5 x 2.5). I regridded the NCEP from (0.25 x 0.25) to (2.5 x 2.5). I regridded the CMIP5 from (2 x 2.5) to (2.5 x 2.5). Temporal Resolution ERA5 and NCEP go from 1980-2019 CMIP5 goes from 2006-2018 For comparing ERA5 vs CMIP5 and NCEP vs CMIPF, I found the same time components Measures \u00b6 I'm measuring the following: Entropy - expected uncertainty Total Correlation - amount of redundant information between features Mutual Information - amount of information shared between variables Data \u00b6 Inputs I'm taking each year as is. Each spatial location is a sample and each year is a feature. My inputs are: X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} Outputs All my information theory measures are in nats. They are scalars. Hypothesis \u00b6 Simple: The ERA5 and the NCEP model should be more similar than the CMIP5 model compared to each of them. Data - Climate Models \u00b6 import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../\" sys . path . insert ( 0 , f ' { source_path } ' ) # Import RBIG Helper from src.models.train_models import run_rbig_models # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) # from esdc.preprocessing import normalize_temporal import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing # Visualization Tools from src.data.climate.loader import ResultsLoader from src.visualization.climate import PlotResults import seaborn as sns import matplotlib.pyplot as plt # plt.style.use('ggplot') plt . style . use ([ 'seaborn-poster' , 'fivethirtyeight' ]) % matplotlib inline % load_ext autoreload % autoreload 2 data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\" Results \u00b6 Mean Sea Level Pressure \u00b6 CMIP5 vs ERA5 vs NCEP \u00b6 variables = [ 'mslp_era_cmip' , 'mslp_ncep_cmip' ] filenames = [ f \" { results_path }{ variable } .csv\" for variable in variables ] results = ResultsLoader ( filenames ) . load_dataframes () # initializer plotter plotter = PlotResults ( results ) plotter . results = plotter . results [ plotter . results [ 'year' ] < 2019 ] Entropy \u00b6 ent1_fig , ent1_ax = plotter . plot_entropy () Total Correlation \u00b6 tc1_fig , tc1_ax = plotter . plot_total_correlation () Mutual Information \u00b6 This is the MI between CMIP5 and the two models (ERA5 and NCEP) mi1_fig , mi1_ax = plotter . plot_mutual_information (( 'model' , [ 'cmip5' ])) NCEP vs ERA5 \u00b6 Mean Sea Level Pressure \u00b6 variables = [ 'mslp_ncep_era' ] filenames = [ f \" { results_path }{ variable } .csv\" for variable in variables ] results = ResultsLoader ( filenames ) . load_dataframes () # initializer plotter plotter = PlotResults ( results ) plotter . results = plotter . results [ plotter . results [ 'year' ] < 2017 ] Entropy \u00b6 plotter . plot_entropy (); Total Correlation \u00b6 plotter . plot_total_correlation (); Mutual Information \u00b6 The MI between ERA5 and the NCAR_NCEP_DOE_2 model. plotter . plot_mutual_information ( omit_models = ( 'model' , [ 'ncar_ncep_doe_2' ])); Surface Pressure \u00b6 variables = [ 'sp_ncep_era' ] filenames = [ f \" { results_path }{ variable } .csv\" for variable in variables ] results = ResultsLoader ( filenames ) . load_dataframes () # initializer plotter plotter = PlotResults ( results ) plotter . results = plotter . results [ plotter . results [ 'year' ] < 2018 ] Entropy \u00b6 plotter . plot_entropy (); Total Correlation \u00b6 plotter . plot_total_correlation (); Mutual Information \u00b6 The MI between ERA5 and NCEP. plotter . plot_mutual_information ( omit_models = ( 'model' , [ 'ncar_ncep_doe_2' ]));","title":"0 visualize"},{"location":"notebooks/climate/0_visualize/#visually-comparing-climate-models","text":"","title":"Visually Comparing Climate Models"},{"location":"notebooks/climate/0_visualize/#summary","text":"In this notebook, I will be comparing three climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 CMIP5 I will be looking at the following variables: Mean Sea Level Pressure (CMIP5, ERA5, NCEP) Surface Pressure (ERA5, NCEP) I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures. If these climate models are that similar, then they should exhibit similar IT measures.","title":"Summary"},{"location":"notebooks/climate/0_visualize/#preprocessing-steps","text":"Regridded Spatially The ERA5 had the coarsest spatial resolution (2.5 x 2.5). I regridded the NCEP from (0.25 x 0.25) to (2.5 x 2.5). I regridded the CMIP5 from (2 x 2.5) to (2.5 x 2.5). Temporal Resolution ERA5 and NCEP go from 1980-2019 CMIP5 goes from 2006-2018 For comparing ERA5 vs CMIP5 and NCEP vs CMIPF, I found the same time components","title":"Preprocessing Steps"},{"location":"notebooks/climate/0_visualize/#measures","text":"I'm measuring the following: Entropy - expected uncertainty Total Correlation - amount of redundant information between features Mutual Information - amount of information shared between variables","title":"Measures"},{"location":"notebooks/climate/0_visualize/#data","text":"Inputs I'm taking each year as is. Each spatial location is a sample and each year is a feature. My inputs are: X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} Outputs All my information theory measures are in nats. They are scalars.","title":"Data"},{"location":"notebooks/climate/0_visualize/#hypothesis","text":"Simple: The ERA5 and the NCEP model should be more similar than the CMIP5 model compared to each of them.","title":"Hypothesis"},{"location":"notebooks/climate/0_visualize/#data-climate-models","text":"import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../\" sys . path . insert ( 0 , f ' { source_path } ' ) # Import RBIG Helper from src.models.train_models import run_rbig_models # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) # from esdc.preprocessing import normalize_temporal import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing # Visualization Tools from src.data.climate.loader import ResultsLoader from src.visualization.climate import PlotResults import seaborn as sns import matplotlib.pyplot as plt # plt.style.use('ggplot') plt . style . use ([ 'seaborn-poster' , 'fivethirtyeight' ]) % matplotlib inline % load_ext autoreload % autoreload 2 data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\"","title":"Data - Climate Models"},{"location":"notebooks/climate/0_visualize/#results","text":"","title":"Results"},{"location":"notebooks/climate/0_visualize/#mean-sea-level-pressure","text":"","title":"Mean Sea Level Pressure"},{"location":"notebooks/climate/0_visualize/#cmip5-vs-era5-vs-ncep","text":"variables = [ 'mslp_era_cmip' , 'mslp_ncep_cmip' ] filenames = [ f \" { results_path }{ variable } .csv\" for variable in variables ] results = ResultsLoader ( filenames ) . load_dataframes () # initializer plotter plotter = PlotResults ( results ) plotter . results = plotter . results [ plotter . results [ 'year' ] < 2019 ]","title":"CMIP5 vs ERA5 vs NCEP"},{"location":"notebooks/climate/0_visualize/#entropy","text":"ent1_fig , ent1_ax = plotter . plot_entropy ()","title":"Entropy"},{"location":"notebooks/climate/0_visualize/#total-correlation","text":"tc1_fig , tc1_ax = plotter . plot_total_correlation ()","title":"Total Correlation"},{"location":"notebooks/climate/0_visualize/#mutual-information","text":"This is the MI between CMIP5 and the two models (ERA5 and NCEP) mi1_fig , mi1_ax = plotter . plot_mutual_information (( 'model' , [ 'cmip5' ]))","title":"Mutual Information"},{"location":"notebooks/climate/0_visualize/#ncep-vs-era5","text":"","title":"NCEP vs ERA5"},{"location":"notebooks/climate/0_visualize/#mean-sea-level-pressure_1","text":"variables = [ 'mslp_ncep_era' ] filenames = [ f \" { results_path }{ variable } .csv\" for variable in variables ] results = ResultsLoader ( filenames ) . load_dataframes () # initializer plotter plotter = PlotResults ( results ) plotter . results = plotter . results [ plotter . results [ 'year' ] < 2017 ]","title":"Mean Sea Level Pressure"},{"location":"notebooks/climate/0_visualize/#entropy_1","text":"plotter . plot_entropy ();","title":"Entropy"},{"location":"notebooks/climate/0_visualize/#total-correlation_1","text":"plotter . plot_total_correlation ();","title":"Total Correlation"},{"location":"notebooks/climate/0_visualize/#mutual-information_1","text":"The MI between ERA5 and the NCAR_NCEP_DOE_2 model. plotter . plot_mutual_information ( omit_models = ( 'model' , [ 'ncar_ncep_doe_2' ]));","title":"Mutual Information"},{"location":"notebooks/climate/0_visualize/#surface-pressure","text":"variables = [ 'sp_ncep_era' ] filenames = [ f \" { results_path }{ variable } .csv\" for variable in variables ] results = ResultsLoader ( filenames ) . load_dataframes () # initializer plotter plotter = PlotResults ( results ) plotter . results = plotter . results [ plotter . results [ 'year' ] < 2018 ]","title":"Surface Pressure"},{"location":"notebooks/climate/0_visualize/#entropy_2","text":"plotter . plot_entropy ();","title":"Entropy"},{"location":"notebooks/climate/0_visualize/#total-correlation_2","text":"plotter . plot_total_correlation ();","title":"Total Correlation"},{"location":"notebooks/climate/0_visualize/#mutual-information_2","text":"The MI between ERA5 and NCEP. plotter . plot_mutual_information ( omit_models = ( 'model' , [ 'ncar_ncep_doe_2' ]));","title":"Mutual Information"},{"location":"notebooks/climate/1.1_compare_similar/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Comparing Two Climate Models \u00b6 In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures. Data - Climate Models \u00b6 import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Import RBIG Helper from src.models.train_models import run_rbig_models # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) from esdc.standardize import normalize_temporal from esdc.transform import regrid_data import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\" ERA5 \u00b6 era5_data = xr . open_dataset ( f \" { data_path } ERA5.nc\" ) era5_data = era5_data . rename ({ 'msl' : 'mslp' , 'latitude' : 'lat' , 'longitude' : 'lon' }) # era5_data = era5_data.rename({'latitude': 'lat'}) # era5_data.attrs['model_id'] = 'era5' # rescale model from 0.25 to 2.5 degrees # era5_data = era5_data.coarsen(lat=10, lon=10, boundary='pad').mean() era5_data . attrs [ 'model_id' ] = 'era5' era5_data <xarray.Dataset> Dimensions: (lat: 721, lon: 1440, time: 487) Coordinates: * lon (lon) float32 0.0 0.25 0.5 0.75 1.0 ... 359.0 359.25 359.5 359.75 * lat (lat) float32 90.0 89.75 89.5 89.25 ... -89.25 -89.5 -89.75 -90.0 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-07-01 Data variables: mslp (time, lat, lon) float32 ... sp (time, lat, lon) float32 ... Attributes: Conventions: CF-1.6 history: 2019-10-07 09:20:10 GMT by grib_to_netcdf-2.10.0: /opt/ecmw... model_id: era5 NCAR-NCEP-DOE-II \u00b6 ncep_data = xr . open_mfdataset ( f \" { data_path } *mon.mean.nc\" ) ncep_data = ncep_data . rename ({ 'pres' : 'sp' }) ncep_data . attrs [ 'model_id' ] = 'ncar_ncep_doe_2' ncep_data <xarray.Dataset> Dimensions: (lat: 73, lon: 144, nbnds: 2, time: 489) Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Dimensions without coordinates: nbnds Data variables: mslp (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> time_bnds (time, nbnds) datetime64[ns] dask.array<chunksize=(489, 2), meta=np.ndarray> pr_wtr (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> sp (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> Attributes: Conventions: CF-1.0 title: Monthly NCEP/DOE Reanalysis 2 history: created 2002/03 by Hoop (netCDF2.3) comments: Data is from \\nNCEP/DOE AMIP-II Reanalysis (Reanalysis-2)... platform: Model source: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Model institution: National Centers for Environmental Prediction dataset_title: NCEP-DOE AMIP-II Reanalysis References: https://www.esrl.noaa.gov/psd/data/gridded/data.ncep.rean... source_url: http://www.cpc.ncep.noaa.gov/products/wesley/reanalysis2/ model_id: ncar_ncep_doe_2 Regridding \u00b6 era5_data_regrid = regrid_data ( ncep_data , era5_data ) Create weight file: nearest_s2d_721x1440_73x144.nc Remove file nearest_s2d_721x1440_73x144.nc era5_data_regrid . attrs = era5_data . attrs era5_data_regrid = xr . Dataset () era5_data_regrid = xr . Dataset () era5_data_regrid [ 'sp' ] = era5_regrid era5_data_regrid . attrs = era5_data . attrs era5_data_regrid <xarray.Dataset> Dimensions: (lat: 73, lon: 144, time: 487) Coordinates: * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-07-01 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 Data variables: sp (time, lat, lon) float64 1.027e+05 1.027e+05 ... 6.859e+04 Attributes: Conventions: CF-1.6 history: 2019-10-07 09:20:10 GMT by grib_to_netcdf-2.10.0: /opt/ecmw... model_id: era5 CMIP5 \u00b6 cmip5_data = xr . open_dataset ( f \" { data_path } CMIP5.nc\" ) cmip5_data = cmip5_data . rename ({ 'psl' : 'mslp' }) # rescale model from 0.25 to 2.5 degrees # cmip5_data = cmip5_data.coarsen(lat=1, boundary='pad').mean() cmip5_data . attrs [ 'model_id' ] = 'cmip5' cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 90, lon: 144, time: 240) Coordinates: * time (time) object 2006-01-16 12:00:00 ... 2025-12-16 12:00:00 * lat (lat) float64 -89.0 -87.0 -85.0 -83.0 ... 83.0 85.0 87.0 89.0 * lon (lon) float64 1.25 3.75 6.25 8.75 ... 351.2 353.8 356.2 358.8 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object ... lat_bnds (lat, bnds) float64 ... lon_bnds (lon, bnds) float64 ... mslp (time, lat, lon) float32 ... Attributes: institution: NASA/GISS (Goddard Institute for Space Studies) N... institute_id: NASA-GISS experiment_id: rcp85 source: GISS-E2-R-E135RCP85aF40oQ32 Atmosphere: GISS-E2; ... model_id: cmip5 forcing: GHG, LU, Sl, Vl, BC, OC, SA, Oz (also includes or... parent_experiment_id: historical parent_experiment_rip: r1i1p1 branch_time: 2006.0 contact: Kenneth Lo (cdkkl@giss.nasa.gov) references: www.giss.nasa.gov/research/modeling initialization_method: 1 physics_version: 1 tracking_id: 71ff3d6b-02eb-470f-a25d-5e79c1b8c1b5 product: output experiment: RCP8.5 frequency: mon creation_date: 2011-08-30T18:58:55Z history: 2011-08-30T18:58:55Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: GISS-E2-R model output prepared for CMIP5 RCP8.5 parent_experiment: historical modeling_realm: atmos realization: 1 cmor_version: 2.5.7 cmip5_regrid = regrid_data ( ncep_data . mslp , cmip5_data . mslp ) cmip5_data_regrid = xr . Dataset () cmip5_data_regrid [ 'mslp' ] = cmip5_regrid cmip5_data_regrid . attrs = cmip5_data . attrs cmip5_data_regrid Reuse existing file: nearest_s2d_90x144_73x144.nc Remove file nearest_s2d_90x144_73x144.nc <xarray.Dataset> Dimensions: (lat: 73, lon: 144, time: 240) Coordinates: * time (time) object 2006-01-16 12:00:00 ... 2025-12-16 12:00:00 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 Data variables: mslp (time, lat, lon) float64 9.993e+04 9.993e+04 ... 9.985e+04 Attributes: institution: NASA/GISS (Goddard Institute for Space Studies) N... institute_id: NASA-GISS experiment_id: rcp85 source: GISS-E2-R-E135RCP85aF40oQ32 Atmosphere: GISS-E2; ... model_id: cmip5 forcing: GHG, LU, Sl, Vl, BC, OC, SA, Oz (also includes or... parent_experiment_id: historical parent_experiment_rip: r1i1p1 branch_time: 2006.0 contact: Kenneth Lo (cdkkl@giss.nasa.gov) references: www.giss.nasa.gov/research/modeling initialization_method: 1 physics_version: 1 tracking_id: 71ff3d6b-02eb-470f-a25d-5e79c1b8c1b5 product: output experiment: RCP8.5 frequency: mon creation_date: 2011-08-30T18:58:55Z history: 2011-08-30T18:58:55Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: GISS-E2-R model output prepared for CMIP5 RCP8.5 parent_experiment: historical modeling_realm: atmos realization: 1 cmor_version: 2.5.7 Experiment I - Comparing Climate Models \u00b6 Mean Sea Level Pressure \u00b6 ERA5 vs NCEP \u00b6 # Experiment class class ClimateEntropy : def __init__ ( self , save_path : None , variable : str = 'mslp' , save_name = None , mi : bool = True ): self . variable = variable self . results_path = save_path self . results_df = pd . DataFrame () self . save_name = save_name self . mi = mi def run_experiment ( self , climate_model1 : pd . DataFrame , climate_model2 : pd . DataFrame ) -> None : \"\"\"Training loop that goes through each year and calculates the entropy, total correlation and mutual information between the two models.\"\"\" time_length = len ( climate_model1 . groupby ( 'time.year' )) # Normalize BEFORE the individual calculations climate_model1 [ self . variable ] = normalize_temporal ( climate_model1 [ self . variable ]) model1_id = climate_model1 . attrs [ 'model_id' ] model2_id = climate_model2 . attrs [ 'model_id' ] climate_model2 [ self . variable ] = normalize_temporal ( climate_model2 [ self . variable ]) with tqdm ( zip ( climate_model1 . groupby ( 'time.year' ), climate_model2 . groupby ( 'time.year' ) ), total = time_length ) as progress_bar : for imodel1 , imodel2 in progress_bar : # Update params in progress bar # Transform to dataframe, remove spatial dimensions X1 = self . _get_time_features ( imodel1 [ 1 ][ self . variable ]) X2 = self . _get_time_features ( imodel2 [ 1 ][ self . variable ]) # Normalize inputs min_max_scaler = preprocessing . StandardScaler () X1 = min_max_scaler . fit_transform ( X1 . values ) X2 = min_max_scaler . fit_transform ( X2 . values ) dims = X1 . shape [ 1 ] # ============================= # Calculate Mutual Information # ============================= if self . mi == False : mi_ = None mi_t_ = None else : mi_ , mi_t_ = run_rbig_models ( X1 , X2 , measure = 'mi' , verbose = None ) # Update params in progress bar postfix = dict ( ) # ======================================== # Calculate Entropy and Total Correlation # ======================================== # Model I tc1_ , h1_ , h_t1_ = run_rbig_models ( X1 , measure = 't' , verbose = None ) self . _update_results ( model = model1_id , year = imodel1 [ 0 ], h_time = h_t1_ , tc = tc1_ , h = h1_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Model II tc2_ , h2_ , h_t2_ = run_rbig_models ( X2 , measure = 't' , verbose = None ) self . _update_results ( model = model2_id , year = imodel2 [ 0 ], h_time = h_t2_ , tc = tc2_ , h = h2_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Update params in progress bar postfix = dict ( year = imodel1 [ 0 ], mi = f \" { mi_ : .3f } \" if self . mi is True else None , h1 = f \" { h1_ : .3f } \" , tc1 = f \" { tc1_ : .3f } \" , h2 = f \" { h2_ : .3f } \" , tc2 = f \" { tc2_ : .3f } \" , ) progress_bar . set_postfix ( postfix ) return None def _get_time_features ( self , data_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"This function collapses the spatial dimensions as pivots. This allows us to only consider time as the input feature.\"\"\" return data_df . to_dataframe () . unstack ( level = 0 ) . reset_index () . drop ( columns = [ 'lat' , 'lon' ]) . dropna () def _update_results ( self , model , year , tc , h , h_time , mi , mi_time , dims ): \"\"\"appends new values to the results dataframe.\"\"\" self . results_df = self . results_df . append ({ 'model' : model , 'year' : year , 'tc' : tc , 'h' : h , 'h_time' : h_time , 'mi' : mi , 'mi_time' : mi_time , 'dims' : dims , }, ignore_index = True ) if self . results_path is not None : self . _save_results () return self def _save_results ( self ): \"\"\"Saves the dataframe to the assigned results path.\"\"\" self . results_df . to_csv ( f \" { self . results_path }{ self . variable } _ { self . save_name } .csv\" ) return None # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_ncep' ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid , ncep_data ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 41/41 [1:43:03<00:00, 150.83s/it, year=2019, mi=4.766, h1=-1.313, tc1=9.551, h2=-3.049, tc2=13.532] # extract results results_df = short_decade_exp . results_df ERA5 vs CMIP5 \u00b6 2006 - 01 - 16 , 2025 - 12 - 16 , 1979 - 01 - 01 , 2019 - 07 - 01 # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:08<00:00, 150.61s/it, year=2019, mi=3.290, h1=-1.509, tc1=9.747, h2=-0.807, tc2=8.142] NCEP vs CMIP5 \u00b6 # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'ncep_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( ncep_data . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:04<00:00, 150.30s/it, year=2019, mi=3.552, h1=-1.495, tc1=9.760, h2=-0.807, tc2=8.142]","title":"1.1 compare similar"},{"location":"notebooks/climate/1.1_compare_similar/#comparing-two-climate-models","text":"In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures.","title":"Comparing Two Climate Models"},{"location":"notebooks/climate/1.1_compare_similar/#data-climate-models","text":"import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Import RBIG Helper from src.models.train_models import run_rbig_models # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) from esdc.standardize import normalize_temporal from esdc.transform import regrid_data import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\"","title":"Data - Climate Models"},{"location":"notebooks/climate/1.1_compare_similar/#era5","text":"era5_data = xr . open_dataset ( f \" { data_path } ERA5.nc\" ) era5_data = era5_data . rename ({ 'msl' : 'mslp' , 'latitude' : 'lat' , 'longitude' : 'lon' }) # era5_data = era5_data.rename({'latitude': 'lat'}) # era5_data.attrs['model_id'] = 'era5' # rescale model from 0.25 to 2.5 degrees # era5_data = era5_data.coarsen(lat=10, lon=10, boundary='pad').mean() era5_data . attrs [ 'model_id' ] = 'era5' era5_data <xarray.Dataset> Dimensions: (lat: 721, lon: 1440, time: 487) Coordinates: * lon (lon) float32 0.0 0.25 0.5 0.75 1.0 ... 359.0 359.25 359.5 359.75 * lat (lat) float32 90.0 89.75 89.5 89.25 ... -89.25 -89.5 -89.75 -90.0 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-07-01 Data variables: mslp (time, lat, lon) float32 ... sp (time, lat, lon) float32 ... Attributes: Conventions: CF-1.6 history: 2019-10-07 09:20:10 GMT by grib_to_netcdf-2.10.0: /opt/ecmw... model_id: era5","title":"ERA5"},{"location":"notebooks/climate/1.1_compare_similar/#ncar-ncep-doe-ii","text":"ncep_data = xr . open_mfdataset ( f \" { data_path } *mon.mean.nc\" ) ncep_data = ncep_data . rename ({ 'pres' : 'sp' }) ncep_data . attrs [ 'model_id' ] = 'ncar_ncep_doe_2' ncep_data <xarray.Dataset> Dimensions: (lat: 73, lon: 144, nbnds: 2, time: 489) Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Dimensions without coordinates: nbnds Data variables: mslp (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> time_bnds (time, nbnds) datetime64[ns] dask.array<chunksize=(489, 2), meta=np.ndarray> pr_wtr (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> sp (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> Attributes: Conventions: CF-1.0 title: Monthly NCEP/DOE Reanalysis 2 history: created 2002/03 by Hoop (netCDF2.3) comments: Data is from \\nNCEP/DOE AMIP-II Reanalysis (Reanalysis-2)... platform: Model source: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Model institution: National Centers for Environmental Prediction dataset_title: NCEP-DOE AMIP-II Reanalysis References: https://www.esrl.noaa.gov/psd/data/gridded/data.ncep.rean... source_url: http://www.cpc.ncep.noaa.gov/products/wesley/reanalysis2/ model_id: ncar_ncep_doe_2","title":"NCAR-NCEP-DOE-II"},{"location":"notebooks/climate/1.1_compare_similar/#regridding","text":"era5_data_regrid = regrid_data ( ncep_data , era5_data ) Create weight file: nearest_s2d_721x1440_73x144.nc Remove file nearest_s2d_721x1440_73x144.nc era5_data_regrid . attrs = era5_data . attrs era5_data_regrid = xr . Dataset () era5_data_regrid = xr . Dataset () era5_data_regrid [ 'sp' ] = era5_regrid era5_data_regrid . attrs = era5_data . attrs era5_data_regrid <xarray.Dataset> Dimensions: (lat: 73, lon: 144, time: 487) Coordinates: * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-07-01 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 Data variables: sp (time, lat, lon) float64 1.027e+05 1.027e+05 ... 6.859e+04 Attributes: Conventions: CF-1.6 history: 2019-10-07 09:20:10 GMT by grib_to_netcdf-2.10.0: /opt/ecmw... model_id: era5","title":"Regridding"},{"location":"notebooks/climate/1.1_compare_similar/#cmip5","text":"cmip5_data = xr . open_dataset ( f \" { data_path } CMIP5.nc\" ) cmip5_data = cmip5_data . rename ({ 'psl' : 'mslp' }) # rescale model from 0.25 to 2.5 degrees # cmip5_data = cmip5_data.coarsen(lat=1, boundary='pad').mean() cmip5_data . attrs [ 'model_id' ] = 'cmip5' cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 90, lon: 144, time: 240) Coordinates: * time (time) object 2006-01-16 12:00:00 ... 2025-12-16 12:00:00 * lat (lat) float64 -89.0 -87.0 -85.0 -83.0 ... 83.0 85.0 87.0 89.0 * lon (lon) float64 1.25 3.75 6.25 8.75 ... 351.2 353.8 356.2 358.8 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object ... lat_bnds (lat, bnds) float64 ... lon_bnds (lon, bnds) float64 ... mslp (time, lat, lon) float32 ... Attributes: institution: NASA/GISS (Goddard Institute for Space Studies) N... institute_id: NASA-GISS experiment_id: rcp85 source: GISS-E2-R-E135RCP85aF40oQ32 Atmosphere: GISS-E2; ... model_id: cmip5 forcing: GHG, LU, Sl, Vl, BC, OC, SA, Oz (also includes or... parent_experiment_id: historical parent_experiment_rip: r1i1p1 branch_time: 2006.0 contact: Kenneth Lo (cdkkl@giss.nasa.gov) references: www.giss.nasa.gov/research/modeling initialization_method: 1 physics_version: 1 tracking_id: 71ff3d6b-02eb-470f-a25d-5e79c1b8c1b5 product: output experiment: RCP8.5 frequency: mon creation_date: 2011-08-30T18:58:55Z history: 2011-08-30T18:58:55Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: GISS-E2-R model output prepared for CMIP5 RCP8.5 parent_experiment: historical modeling_realm: atmos realization: 1 cmor_version: 2.5.7 cmip5_regrid = regrid_data ( ncep_data . mslp , cmip5_data . mslp ) cmip5_data_regrid = xr . Dataset () cmip5_data_regrid [ 'mslp' ] = cmip5_regrid cmip5_data_regrid . attrs = cmip5_data . attrs cmip5_data_regrid Reuse existing file: nearest_s2d_90x144_73x144.nc Remove file nearest_s2d_90x144_73x144.nc <xarray.Dataset> Dimensions: (lat: 73, lon: 144, time: 240) Coordinates: * time (time) object 2006-01-16 12:00:00 ... 2025-12-16 12:00:00 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 Data variables: mslp (time, lat, lon) float64 9.993e+04 9.993e+04 ... 9.985e+04 Attributes: institution: NASA/GISS (Goddard Institute for Space Studies) N... institute_id: NASA-GISS experiment_id: rcp85 source: GISS-E2-R-E135RCP85aF40oQ32 Atmosphere: GISS-E2; ... model_id: cmip5 forcing: GHG, LU, Sl, Vl, BC, OC, SA, Oz (also includes or... parent_experiment_id: historical parent_experiment_rip: r1i1p1 branch_time: 2006.0 contact: Kenneth Lo (cdkkl@giss.nasa.gov) references: www.giss.nasa.gov/research/modeling initialization_method: 1 physics_version: 1 tracking_id: 71ff3d6b-02eb-470f-a25d-5e79c1b8c1b5 product: output experiment: RCP8.5 frequency: mon creation_date: 2011-08-30T18:58:55Z history: 2011-08-30T18:58:55Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: GISS-E2-R model output prepared for CMIP5 RCP8.5 parent_experiment: historical modeling_realm: atmos realization: 1 cmor_version: 2.5.7","title":"CMIP5"},{"location":"notebooks/climate/1.1_compare_similar/#experiment-i-comparing-climate-models","text":"","title":"Experiment I - Comparing Climate Models"},{"location":"notebooks/climate/1.1_compare_similar/#mean-sea-level-pressure","text":"","title":"Mean Sea Level Pressure"},{"location":"notebooks/climate/1.1_compare_similar/#era5-vs-ncep","text":"# Experiment class class ClimateEntropy : def __init__ ( self , save_path : None , variable : str = 'mslp' , save_name = None , mi : bool = True ): self . variable = variable self . results_path = save_path self . results_df = pd . DataFrame () self . save_name = save_name self . mi = mi def run_experiment ( self , climate_model1 : pd . DataFrame , climate_model2 : pd . DataFrame ) -> None : \"\"\"Training loop that goes through each year and calculates the entropy, total correlation and mutual information between the two models.\"\"\" time_length = len ( climate_model1 . groupby ( 'time.year' )) # Normalize BEFORE the individual calculations climate_model1 [ self . variable ] = normalize_temporal ( climate_model1 [ self . variable ]) model1_id = climate_model1 . attrs [ 'model_id' ] model2_id = climate_model2 . attrs [ 'model_id' ] climate_model2 [ self . variable ] = normalize_temporal ( climate_model2 [ self . variable ]) with tqdm ( zip ( climate_model1 . groupby ( 'time.year' ), climate_model2 . groupby ( 'time.year' ) ), total = time_length ) as progress_bar : for imodel1 , imodel2 in progress_bar : # Update params in progress bar # Transform to dataframe, remove spatial dimensions X1 = self . _get_time_features ( imodel1 [ 1 ][ self . variable ]) X2 = self . _get_time_features ( imodel2 [ 1 ][ self . variable ]) # Normalize inputs min_max_scaler = preprocessing . StandardScaler () X1 = min_max_scaler . fit_transform ( X1 . values ) X2 = min_max_scaler . fit_transform ( X2 . values ) dims = X1 . shape [ 1 ] # ============================= # Calculate Mutual Information # ============================= if self . mi == False : mi_ = None mi_t_ = None else : mi_ , mi_t_ = run_rbig_models ( X1 , X2 , measure = 'mi' , verbose = None ) # Update params in progress bar postfix = dict ( ) # ======================================== # Calculate Entropy and Total Correlation # ======================================== # Model I tc1_ , h1_ , h_t1_ = run_rbig_models ( X1 , measure = 't' , verbose = None ) self . _update_results ( model = model1_id , year = imodel1 [ 0 ], h_time = h_t1_ , tc = tc1_ , h = h1_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Model II tc2_ , h2_ , h_t2_ = run_rbig_models ( X2 , measure = 't' , verbose = None ) self . _update_results ( model = model2_id , year = imodel2 [ 0 ], h_time = h_t2_ , tc = tc2_ , h = h2_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Update params in progress bar postfix = dict ( year = imodel1 [ 0 ], mi = f \" { mi_ : .3f } \" if self . mi is True else None , h1 = f \" { h1_ : .3f } \" , tc1 = f \" { tc1_ : .3f } \" , h2 = f \" { h2_ : .3f } \" , tc2 = f \" { tc2_ : .3f } \" , ) progress_bar . set_postfix ( postfix ) return None def _get_time_features ( self , data_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"This function collapses the spatial dimensions as pivots. This allows us to only consider time as the input feature.\"\"\" return data_df . to_dataframe () . unstack ( level = 0 ) . reset_index () . drop ( columns = [ 'lat' , 'lon' ]) . dropna () def _update_results ( self , model , year , tc , h , h_time , mi , mi_time , dims ): \"\"\"appends new values to the results dataframe.\"\"\" self . results_df = self . results_df . append ({ 'model' : model , 'year' : year , 'tc' : tc , 'h' : h , 'h_time' : h_time , 'mi' : mi , 'mi_time' : mi_time , 'dims' : dims , }, ignore_index = True ) if self . results_path is not None : self . _save_results () return self def _save_results ( self ): \"\"\"Saves the dataframe to the assigned results path.\"\"\" self . results_df . to_csv ( f \" { self . results_path }{ self . variable } _ { self . save_name } .csv\" ) return None # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_ncep' ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid , ncep_data ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 41/41 [1:43:03<00:00, 150.83s/it, year=2019, mi=4.766, h1=-1.313, tc1=9.551, h2=-3.049, tc2=13.532] # extract results results_df = short_decade_exp . results_df","title":"ERA5 vs NCEP"},{"location":"notebooks/climate/1.1_compare_similar/#era5-vs-cmip5","text":"2006 - 01 - 16 , 2025 - 12 - 16 , 1979 - 01 - 01 , 2019 - 07 - 01 # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:08<00:00, 150.61s/it, year=2019, mi=3.290, h1=-1.509, tc1=9.747, h2=-0.807, tc2=8.142]","title":"ERA5 vs CMIP5"},{"location":"notebooks/climate/1.1_compare_similar/#ncep-vs-cmip5","text":"# Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'ncep_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( ncep_data . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:04<00:00, 150.30s/it, year=2019, mi=3.552, h1=-1.495, tc1=9.760, h2=-0.807, tc2=8.142]","title":"NCEP vs CMIP5"},{"location":"notebooks/climate/1.2_compare_all/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../\" sys . path . insert ( 0 , f ' { source_path } ' ) # Import RBIG Helper from src.models.train_models import run_rbig_models # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) from esdc.preprocessing import normalize_temporal import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\" CMIP5 \u00b6 cmip5_data = xr . open_dataset ( f \" { data_path } CMIP5.nc\" ) cmip5_data = cmip5_data . rename ({ 'psl' : 'mslp' }) # rescale model from 0.25 to 2.5 degrees cmip5_data = cmip5_data . coarsen ( lat = 1 , boundary = 'pad' ) . mean () cmip5_data . attrs [ 'model_id' ] = 'cmip5' cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 90, lon: 144, time: 240) Coordinates: * time (time) object 2006-01-16 12:00:00 ... 2025-12-16 12:00:00 * lat (lat) float64 -89.0 -87.0 -85.0 -83.0 ... 83.0 85.0 87.0 89.0 * lon (lon) float64 1.25 3.75 6.25 8.75 ... 351.2 353.8 356.2 358.8 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object 2006-01-01 00:00:00 ... 2026-01-01 00:00:00 lat_bnds (lat, bnds) float64 -90.0 -88.0 -88.0 -86.0 ... 88.0 88.0 90.0 lon_bnds (lon, bnds) float64 0.0 2.5 2.5 5.0 ... 355.0 357.5 357.5 360.0 mslp (time, lat, lon) float32 99793.75 99793.75 ... 99897.87 99897.87 Attributes: model_id: cmip5 # cmip5_data.isel(time=0).mslp.plot() cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 90, lon: 144, time: 240) Coordinates: * time (time) object 2006-01-16 12:00:00 ... 2025-12-16 12:00:00 * lat (lat) float64 -89.0 -87.0 -85.0 -83.0 ... 83.0 85.0 87.0 89.0 * lon (lon) float64 1.25 3.75 6.25 8.75 ... 351.2 353.8 356.2 358.8 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object 2006-01-01 00:00:00 ... 2026-01-01 00:00:00 lat_bnds (lat, bnds) float64 -90.0 -88.0 -88.0 -86.0 ... 88.0 88.0 90.0 lon_bnds (lon, bnds) float64 0.0 2.5 2.5 5.0 ... 355.0 357.5 357.5 360.0 mslp (time, lat, lon) float32 99793.75 99793.75 ... 99897.87 99897.87 Attributes: model_id: cmip5","title":"1.2 compare all"},{"location":"notebooks/climate/1.2_compare_all/#cmip5","text":"cmip5_data = xr . open_dataset ( f \" { data_path } CMIP5.nc\" ) cmip5_data = cmip5_data . rename ({ 'psl' : 'mslp' }) # rescale model from 0.25 to 2.5 degrees cmip5_data = cmip5_data . coarsen ( lat = 1 , boundary = 'pad' ) . mean () cmip5_data . attrs [ 'model_id' ] = 'cmip5' cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 90, lon: 144, time: 240) Coordinates: * time (time) object 2006-01-16 12:00:00 ... 2025-12-16 12:00:00 * lat (lat) float64 -89.0 -87.0 -85.0 -83.0 ... 83.0 85.0 87.0 89.0 * lon (lon) float64 1.25 3.75 6.25 8.75 ... 351.2 353.8 356.2 358.8 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object 2006-01-01 00:00:00 ... 2026-01-01 00:00:00 lat_bnds (lat, bnds) float64 -90.0 -88.0 -88.0 -86.0 ... 88.0 88.0 90.0 lon_bnds (lon, bnds) float64 0.0 2.5 2.5 5.0 ... 355.0 357.5 357.5 360.0 mslp (time, lat, lon) float32 99793.75 99793.75 ... 99897.87 99897.87 Attributes: model_id: cmip5 # cmip5_data.isel(time=0).mslp.plot() cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 90, lon: 144, time: 240) Coordinates: * time (time) object 2006-01-16 12:00:00 ... 2025-12-16 12:00:00 * lat (lat) float64 -89.0 -87.0 -85.0 -83.0 ... 83.0 85.0 87.0 89.0 * lon (lon) float64 1.25 3.75 6.25 8.75 ... 351.2 353.8 356.2 358.8 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object 2006-01-01 00:00:00 ... 2026-01-01 00:00:00 lat_bnds (lat, bnds) float64 -90.0 -88.0 -88.0 -86.0 ... 88.0 88.0 90.0 lon_bnds (lon, bnds) float64 0.0 2.5 2.5 5.0 ... 355.0 357.5 357.5 360.0 mslp (time, lat, lon) float32 99793.75 99793.75 ... 99897.87 99897.87 Attributes: model_id: cmip5","title":"CMIP5"},{"location":"notebooks/climate/2.1_regridding/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Import RBIG Helper from src.models.train_models import run_rbig_models # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) from esdc.standardize import normalize_temporal from esdc.transform import regrid_data import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\" ncep_data = xr . open_mfdataset ( f \" { data_path } *mon.mean.nc\" ) ncep_data = ncep_data . rename ({ 'pres' : 'sp' }) ncep_data . attrs [ 'model_id' ] = 'ncar_ncep_doe_2' ncep_data <xarray.Dataset> Dimensions: (lat: 73, lon: 144, nbnds: 2, time: 489) Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Dimensions without coordinates: nbnds Data variables: mslp (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> time_bnds (time, nbnds) datetime64[ns] dask.array<chunksize=(489, 2), meta=np.ndarray> pr_wtr (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> sp (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> Attributes: Conventions: CF-1.0 title: Monthly NCEP/DOE Reanalysis 2 history: created 2002/03 by Hoop (netCDF2.3) comments: Data is from \\nNCEP/DOE AMIP-II Reanalysis (Reanalysis-2)... platform: Model source: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Model institution: National Centers for Environmental Prediction dataset_title: NCEP-DOE AMIP-II Reanalysis References: https://www.esrl.noaa.gov/psd/data/gridded/data.ncep.rean... source_url: http://www.cpc.ncep.noaa.gov/products/wesley/reanalysis2/ model_id: ncar_ncep_doe_2 era5_data = xr . open_dataset ( f \" { data_path } ERA5.nc\" ) era5_data = era5_data . rename ({ 'msl' : 'mslp' , 'latitude' : 'lat' , 'longitude' : 'lon' }) # era5_data = era5_data.rename({'latitude': 'lat'}) # era5_data.attrs['model_id'] = 'era5' # rescale model from 0.25 to 2.5 degrees # era5_data = era5_data.coarsen(lat=10, lon=10, boundary='pad').mean() era5_data . attrs [ 'model_id' ] = 'era5' era5_data <xarray.Dataset> Dimensions: (lat: 721, lon: 1440, time: 487) Coordinates: * lon (lon) float32 0.0 0.25 0.5 0.75 1.0 ... 359.0 359.25 359.5 359.75 * lat (lat) float32 90.0 89.75 89.5 89.25 ... -89.25 -89.5 -89.75 -90.0 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-07-01 Data variables: mslp (time, lat, lon) float32 ... sp (time, lat, lon) float32 ... Attributes: Conventions: CF-1.6 history: 2019-10-07 09:20:10 GMT by grib_to_netcdf-2.10.0: /opt/ecmw... model_id: era5 import xesmf as xe era5_data_regrid = xr . Dataset ( { \"lat\" : ([ \"lat\" ], ncep_data . lat ), \"lon\" : ([ \"lon\" ], ncep_data . lon )} ) method = 'conservative' regridder = xe . Regridder ( ncep_data , era5_data_regrid , method , reuse_weights = True ) --------------------------------------------------------------------------- KeyError Traceback (most recent call last) ~/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xarray/core/dataset.py in _construct_dataarray (self, name) 1150 try : -> 1151 variable = self . _variables [ name ] 1152 except KeyError : KeyError : 'lon_b' During handling of the above exception, another exception occurred: KeyError Traceback (most recent call last) <ipython-input-9-73cf5d2042a7> in <module> 4 era5_data_regrid , 5 method , ----> 6 reuse_weights = True 7 ) ~/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xesmf/frontend.py in __init__ (self, ds_in, ds_out, method, periodic, filename, reuse_weights) 135 self._grid_in, shape_in = ds_to_ESMFgrid(ds_in, 136 need_bounds = self . need_bounds , --> 137 periodic = periodic 138 ) 139 self._grid_out, shape_out = ds_to_ESMFgrid(ds_out, ~/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xesmf/frontend.py in ds_to_ESMFgrid (ds, need_bounds, periodic, append) 65 66 if need_bounds : ---> 67 lon_b = np . asarray ( ds [ 'lon_b' ] ) 68 lat_b = np . asarray ( ds [ 'lat_b' ] ) 69 lon_b , lat_b = as_2d_mesh ( lon_b , lat_b ) ~/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xarray/core/dataset.py in __getitem__ (self, key) 1241 1242 if hashable ( key ) : -> 1243 return self . _construct_dataarray ( key ) 1244 else : 1245 return self . _copy_listed ( np . asarray ( key ) ) ~/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xarray/core/dataset.py in _construct_dataarray (self, name) 1152 except KeyError : 1153 _, name, variable = _get_virtual_variable( -> 1154 self . _variables , name , self . _level_coords , self . dims 1155 ) 1156 ~/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xarray/core/dataset.py in _get_virtual_variable (variables, key, level_vars, dim_sizes) 144 ref_var = dim_var . to_index_variable ( ) . get_level_variable ( ref_name ) 145 else : --> 146 ref_var = variables [ ref_name ] 147 148 if var_name is None : KeyError : 'lon_b'","title":"2.1 regridding"},{"location":"notebooks/climate/amip/0_data_download/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Comparing Two Climate Models \u00b6 In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures. Data - Climate Models \u00b6 import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.rcp import DataDownloader as DDRCP from src.data.climate.historical import DataDownloader as DDHist from src.data.climate.rcp import DataLoader # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) from esdc.standardize import normalize_temporal from esdc.grid import regrid_data import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\" Download Datasets \u00b6 downloader = DDRCP () downloader . download_all () 2019-12-04 17:20:49,332 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels 2019-12-04 17:20:51,321 INFO Request is completed 2019-12-04 17:20:51,322 INFO Downloading http://136.156.132.210/cache-compute-0005/cache/data2/4ad75011-e089-4a3c-b528-066055bae5a8-psl_Amon_inmcm4_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/inmcm4/inmcm4.zip (94M) 2019-12-04 17:20:58,958 INFO Download rate 12.3M/s 2019-12-04 17:21:00,135 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/inmcm4/inmcm4.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/inmcm4/inmcm4.nc 2019-12-04 17:21:00,782 INFO Downloading http://136.156.132.110/cache-compute-0001/cache/data5/07399125-c76c-449b-a5e8-3c770f44aef6-psl_Amon_ACCESS1-0_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_0/access1_0.zip (121.1M) 2019-12-04 17:21:12,223 INFO Download rate 10.6M/s 2019-12-04 17:21:12,849 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_0/access1_0.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_0/access1_0.nc 2019-12-04 17:21:13,628 INFO Downloading http://136.156.132.110/cache-compute-0001/cache/data5/27edf3e3-eb27-457f-9d08-129b97eeea14-psl_Amon_ACCESS1-3_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_3/access1_3.zip (121.1M) 2019-12-04 17:21:24,205 INFO Download rate 11.5M/s 2019-12-04 17:21:25,051 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_3/access1_3.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_3/access1_3.nc 2019-12-04 17:21:25,808 INFO Downloading http://136.156.132.110/cache-compute-0001/cache/data5/ca3a0414-b487-4ca6-b38c-0cd08ee6102c-psl_Amon_IPSL-CM5B-LR_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_lr/ipsl_cm5a_lr.zip (40.1M) 2019-12-04 17:21:30,184 INFO Download rate 9.2M/s 2019-12-04 17:21:31,063 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_lr/ipsl_cm5a_lr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_lr/ipsl_cm5a_lr.nc 2019-12-04 17:21:32,817 INFO Downloading http://136.156.132.110/cache-compute-0001/cache/data5/secureopendap-1575473908.9167163.nc/psl_Amon_IPSL-CM5A-MR_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_mr/ipsl_cm5a_mr.zip (89.6M) 2019-12-04 17:21:41,259 INFO Download rate 10.6M/s 2019-12-04 17:21:42,011 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_mr/ipsl_cm5a_mr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_mr/ipsl_cm5a_mr.nc 2019-12-04 17:21:42,741 INFO Downloading http://136.156.132.110/cache-compute-0001/cache/data5/c5c60a43-f042-43e8-9095-c50b18021372-psl_Amon_MPI-ESM-LR_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_lr/mpi_esm_lr.zip (80.2M) 2019-12-04 17:21:51,528 INFO Download rate 9.1M/s 2019-12-04 17:21:51,901 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_lr/mpi_esm_lr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_lr/mpi_esm_lr.nc 2019-12-04 17:21:52,956 INFO Downloading http://136.156.132.110/cache-compute-0001/cache/data5/b5b75fb5-95f3-44c0-a1e1-d2ec99742915-psl_Amon_MPI-ESM-MR_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_mr/mpi_esm_mr.zip (80.2M) 2019-12-04 17:22:03,450 INFO Download rate 7.6M/s 2019-12-04 17:22:08,856 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_mr/mpi_esm_mr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_mr/mpi_esm_mr.nc 2019-12-04 17:22:09,368 INFO Downloading http://136.156.132.110/cache-compute-0001/cache/data5/094f2da0-c121-4c2b-9282-a8ed13f2db60-psl_Amon_NorESM1-M_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/noresm1_m/noresm1_m.zip (60.2M) 2019-12-04 17:22:17,749 INFO Download rate 7.2M/s /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/noresm1_m/noresm1_m.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/noresm1_m/noresm1_m.nc downloader = DDHist () downloader . download_all () 2019-12-04 17:29:09,999 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels 2019-12-04 17:29:10,479 INFO Request is completed 2019-12-04 17:29:10,481 INFO Downloading http://136.156.133.36/cache-compute-0010/cache/data4/0ff1d385-0263-4be4-ad68-b629366d673e-psl_Amon_inmcm4_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/inmcm4/historical_inmcm4.zip (154.3M) 2019-12-04 17:29:22,928 INFO Download rate 12.4M/s 2019-12-04 17:29:23,195 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/inmcm4/amip_inmcm4.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/inmcm4/historical_inmcm4.nc 2019-12-04 17:29:23,912 INFO Downloading http://136.156.132.236/cache-compute-0007/cache/data4/5dcf07e5-38e8-42da-82f1-1155ef2503a7-psl_Amon_ACCESS1-0_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_0/historical_access1_0.zip (198.9M) 2019-12-04 17:29:39,234 INFO Download rate 13M/s 2019-12-04 17:29:39,574 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_0/amip_access1_0.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_0/historical_access1_0.nc 2019-12-04 17:29:39,706 INFO Downloading http://136.156.132.236/cache-compute-0007/cache/data4/4dbbd38c-722a-44ad-a852-12af7277d46f-psl_Amon_ACCESS1-3_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_3/historical_access1_3.zip (198.9M) 2019-12-04 17:29:56,494 INFO Download rate 11.8M/s 2019-12-04 17:29:56,813 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_3/amip_access1_3.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_3/historical_access1_3.nc 2019-12-04 17:29:57,290 INFO Downloading http://136.156.132.236/cache-compute-0007/cache/data4/secureopendap-1575475938.7605283.nc/psl_Amon_IPSL-CM5B-LR_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_lr/historical_ipsl_cm5a_lr.zip (65.9M) 2019-12-04 17:30:04,268 INFO Download rate 9.4M/s 2019-12-04 17:30:04,462 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_lr/amip_ipsl_cm5a_lr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_lr/historical_ipsl_cm5a_lr.nc 2019-12-04 17:30:04,708 INFO Request is queued 2019-12-04 17:30:05,770 INFO Request is running 2019-12-04 17:30:25,852 INFO Request is completed 2019-12-04 17:30:25,854 INFO Downloading http://136.156.133.37/cache-compute-0011/cache/data5/secureopendap-1575477007.4191332.nc/psl_Amon_IPSL-CM5A-MR_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_mr/historical_ipsl_cm5a_mr.zip (147.1M) 2019-12-04 17:30:39,753 INFO Download rate 10.6M/s 2019-12-04 17:30:39,915 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels 2019-12-04 17:30:40,079 INFO Downloading http://136.156.132.236/cache-compute-0007/cache/data4/63c80ed5-e5c6-4dc1-b49f-d860fde6227b-psl_Amon_MPI-ESM-LR_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_lr/historical_mpi_esm_lr.zip (131.7M) /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_mr/amip_ipsl_cm5a_mr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_mr/historical_ipsl_cm5a_mr.nc 2019-12-04 17:30:52,217 INFO Download rate 10.8M/s 2019-12-04 17:30:52,814 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_lr/amip_mpi_esm_lr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_lr/historical_mpi_esm_lr.nc 2019-12-04 17:30:52,998 INFO Downloading http://136.156.132.236/cache-compute-0007/cache/data4/f32b3eb9-af25-4e6f-ac60-c1ef4c813d4c-psl_Amon_MPI-ESM-MR_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_mr/historical_mpi_esm_mr.zip (131.7M) 2019-12-04 17:31:05,457 INFO Download rate 10.6M/s 2019-12-04 17:31:06,003 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_mr/amip_mpi_esm_mr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_mr/historical_mpi_esm_mr.nc 2019-12-04 17:31:06,183 INFO Downloading http://136.156.132.236/cache-compute-0007/cache/data4/5a7ba2a3-45de-40a6-a5a6-d60a4b95c493-psl_Amon_NorESM1-M_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/noresm1_m/historical_noresm1_m.zip (98.8M) 2019-12-04 17:31:16,066 INFO Download rate 10M/s /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/noresm1_m/amip_noresm1_m.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/noresm1_m/historical_noresm1_m.nc Load Datasets \u00b6 loader = DataLoader () dataset = 'ipsl_cm5a_mr' xr_data = loader . load_rcp_data ( dataset ) xr_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 143, lon: 144, time: 3012) Coordinates: * lon (lon) float64 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * lat (lat) float64 -90.0 -88.73 -87.46 -86.2 ... 86.2 87.46 88.73 90.0 * time (time) object 1850-01-16 12:00:00 ... 2100-12-16 12:00:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object dask.array<chunksize=(1872, 2), meta=np.ndarray> lat_bnds (time, lat, bnds) float64 dask.array<chunksize=(1872, 143, 2), meta=np.ndarray> lon_bnds (time, lon, bnds) float64 dask.array<chunksize=(1872, 144, 2), meta=np.ndarray> psl (time, lat, lon) float32 dask.array<chunksize=(1872, 143, 144), meta=np.ndarray> Attributes: institution: IPSL (Institut Pierre Simon Laplace, Paris, France) institute_id: IPSL experiment_id: historical source: IPSL-CM5A-MR (2010) : atmos : LMDZ4 (LMDZ4_v5, 14... model_id: IPSL-CM5A-MR forcing: Nat,Ant,GHG,SA,Oz,LU,SS,Ds,BC,MD,OC,AA parent_experiment_id: piControl parent_experiment_rip: r1i1p1 branch_time: 1850.0 contact: ipsl-cmip5 _at_ ipsl.jussieu.fr Data manager : Se... comment: This 20th century simulation include natural and ... references: Model documentation and further reference availab... initialization_method: 1 physics_version: 1 tracking_id: ee34ce17-a474-47bb-b311-c00df6dfbf7a product: output experiment: historical frequency: mon creation_date: 2011-09-22T20:43:25Z history: 2011-09-22T20:43:25Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: IPSL-CM5A-MR model output prepared for CMIP5 hist... parent_experiment: pre-industrial control modeling_realm: atmos realization: 1 cmor_version: 2.7.1 Testing \u00b6 datasets = [ \"inmcm4\" , \"access1_0\" , \"access1_3\" , \"ipsl_cm5a_mr\" , \"mpi_esm_lr\" , \"mpi_esm_mr\" , \"noresm1_m\" , ] loader = DataLoader () for idataset in datasets : print ( idataset ) data = loader . load_rcp_data ( idataset ) print ( data . psl . shape ) assert ( type ( data ) is xr . Dataset ) inmcm4 (3012, 120, 180) access1_0 (3012, 145, 192) access1_3 (3012, 145, 192) ipsl_cm5a_mr (3012, 143, 144) mpi_esm_lr (3012, 96, 192) mpi_esm_mr (3012, 96, 192) noresm1_m (3012, 96, 144)","title":"0 data download"},{"location":"notebooks/climate/amip/0_data_download/#comparing-two-climate-models","text":"In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures.","title":"Comparing Two Climate Models"},{"location":"notebooks/climate/amip/0_data_download/#data-climate-models","text":"import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.rcp import DataDownloader as DDRCP from src.data.climate.historical import DataDownloader as DDHist from src.data.climate.rcp import DataLoader # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) from esdc.standardize import normalize_temporal from esdc.grid import regrid_data import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\"","title":"Data - Climate Models"},{"location":"notebooks/climate/amip/0_data_download/#download-datasets","text":"downloader = DDRCP () downloader . download_all () 2019-12-04 17:20:49,332 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels 2019-12-04 17:20:51,321 INFO Request is completed 2019-12-04 17:20:51,322 INFO Downloading http://136.156.132.210/cache-compute-0005/cache/data2/4ad75011-e089-4a3c-b528-066055bae5a8-psl_Amon_inmcm4_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/inmcm4/inmcm4.zip (94M) 2019-12-04 17:20:58,958 INFO Download rate 12.3M/s 2019-12-04 17:21:00,135 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/inmcm4/inmcm4.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/inmcm4/inmcm4.nc 2019-12-04 17:21:00,782 INFO Downloading http://136.156.132.110/cache-compute-0001/cache/data5/07399125-c76c-449b-a5e8-3c770f44aef6-psl_Amon_ACCESS1-0_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_0/access1_0.zip (121.1M) 2019-12-04 17:21:12,223 INFO Download rate 10.6M/s 2019-12-04 17:21:12,849 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_0/access1_0.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_0/access1_0.nc 2019-12-04 17:21:13,628 INFO Downloading http://136.156.132.110/cache-compute-0001/cache/data5/27edf3e3-eb27-457f-9d08-129b97eeea14-psl_Amon_ACCESS1-3_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_3/access1_3.zip (121.1M) 2019-12-04 17:21:24,205 INFO Download rate 11.5M/s 2019-12-04 17:21:25,051 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_3/access1_3.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_3/access1_3.nc 2019-12-04 17:21:25,808 INFO Downloading http://136.156.132.110/cache-compute-0001/cache/data5/ca3a0414-b487-4ca6-b38c-0cd08ee6102c-psl_Amon_IPSL-CM5B-LR_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_lr/ipsl_cm5a_lr.zip (40.1M) 2019-12-04 17:21:30,184 INFO Download rate 9.2M/s 2019-12-04 17:21:31,063 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_lr/ipsl_cm5a_lr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_lr/ipsl_cm5a_lr.nc 2019-12-04 17:21:32,817 INFO Downloading http://136.156.132.110/cache-compute-0001/cache/data5/secureopendap-1575473908.9167163.nc/psl_Amon_IPSL-CM5A-MR_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_mr/ipsl_cm5a_mr.zip (89.6M) 2019-12-04 17:21:41,259 INFO Download rate 10.6M/s 2019-12-04 17:21:42,011 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_mr/ipsl_cm5a_mr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_mr/ipsl_cm5a_mr.nc 2019-12-04 17:21:42,741 INFO Downloading http://136.156.132.110/cache-compute-0001/cache/data5/c5c60a43-f042-43e8-9095-c50b18021372-psl_Amon_MPI-ESM-LR_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_lr/mpi_esm_lr.zip (80.2M) 2019-12-04 17:21:51,528 INFO Download rate 9.1M/s 2019-12-04 17:21:51,901 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_lr/mpi_esm_lr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_lr/mpi_esm_lr.nc 2019-12-04 17:21:52,956 INFO Downloading http://136.156.132.110/cache-compute-0001/cache/data5/b5b75fb5-95f3-44c0-a1e1-d2ec99742915-psl_Amon_MPI-ESM-MR_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_mr/mpi_esm_mr.zip (80.2M) 2019-12-04 17:22:03,450 INFO Download rate 7.6M/s 2019-12-04 17:22:08,856 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_mr/mpi_esm_mr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_mr/mpi_esm_mr.nc 2019-12-04 17:22:09,368 INFO Downloading http://136.156.132.110/cache-compute-0001/cache/data5/094f2da0-c121-4c2b-9282-a8ed13f2db60-psl_Amon_NorESM1-M_rcp85_r1i1p1_200601-210012.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/noresm1_m/noresm1_m.zip (60.2M) 2019-12-04 17:22:17,749 INFO Download rate 7.2M/s /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/noresm1_m/noresm1_m.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/noresm1_m/noresm1_m.nc downloader = DDHist () downloader . download_all () 2019-12-04 17:29:09,999 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels 2019-12-04 17:29:10,479 INFO Request is completed 2019-12-04 17:29:10,481 INFO Downloading http://136.156.133.36/cache-compute-0010/cache/data4/0ff1d385-0263-4be4-ad68-b629366d673e-psl_Amon_inmcm4_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/inmcm4/historical_inmcm4.zip (154.3M) 2019-12-04 17:29:22,928 INFO Download rate 12.4M/s 2019-12-04 17:29:23,195 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/inmcm4/amip_inmcm4.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/inmcm4/historical_inmcm4.nc 2019-12-04 17:29:23,912 INFO Downloading http://136.156.132.236/cache-compute-0007/cache/data4/5dcf07e5-38e8-42da-82f1-1155ef2503a7-psl_Amon_ACCESS1-0_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_0/historical_access1_0.zip (198.9M) 2019-12-04 17:29:39,234 INFO Download rate 13M/s 2019-12-04 17:29:39,574 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_0/amip_access1_0.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_0/historical_access1_0.nc 2019-12-04 17:29:39,706 INFO Downloading http://136.156.132.236/cache-compute-0007/cache/data4/4dbbd38c-722a-44ad-a852-12af7277d46f-psl_Amon_ACCESS1-3_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_3/historical_access1_3.zip (198.9M) 2019-12-04 17:29:56,494 INFO Download rate 11.8M/s 2019-12-04 17:29:56,813 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_3/amip_access1_3.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/access1_3/historical_access1_3.nc 2019-12-04 17:29:57,290 INFO Downloading http://136.156.132.236/cache-compute-0007/cache/data4/secureopendap-1575475938.7605283.nc/psl_Amon_IPSL-CM5B-LR_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_lr/historical_ipsl_cm5a_lr.zip (65.9M) 2019-12-04 17:30:04,268 INFO Download rate 9.4M/s 2019-12-04 17:30:04,462 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_lr/amip_ipsl_cm5a_lr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_lr/historical_ipsl_cm5a_lr.nc 2019-12-04 17:30:04,708 INFO Request is queued 2019-12-04 17:30:05,770 INFO Request is running 2019-12-04 17:30:25,852 INFO Request is completed 2019-12-04 17:30:25,854 INFO Downloading http://136.156.133.37/cache-compute-0011/cache/data5/secureopendap-1575477007.4191332.nc/psl_Amon_IPSL-CM5A-MR_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_mr/historical_ipsl_cm5a_mr.zip (147.1M) 2019-12-04 17:30:39,753 INFO Download rate 10.6M/s 2019-12-04 17:30:39,915 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels 2019-12-04 17:30:40,079 INFO Downloading http://136.156.132.236/cache-compute-0007/cache/data4/63c80ed5-e5c6-4dc1-b49f-d860fde6227b-psl_Amon_MPI-ESM-LR_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_lr/historical_mpi_esm_lr.zip (131.7M) /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_mr/amip_ipsl_cm5a_mr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/ipsl_cm5a_mr/historical_ipsl_cm5a_mr.nc 2019-12-04 17:30:52,217 INFO Download rate 10.8M/s 2019-12-04 17:30:52,814 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_lr/amip_mpi_esm_lr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_lr/historical_mpi_esm_lr.nc 2019-12-04 17:30:52,998 INFO Downloading http://136.156.132.236/cache-compute-0007/cache/data4/f32b3eb9-af25-4e6f-ac60-c1ef4c813d4c-psl_Amon_MPI-ESM-MR_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_mr/historical_mpi_esm_mr.zip (131.7M) 2019-12-04 17:31:05,457 INFO Download rate 10.6M/s 2019-12-04 17:31:06,003 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_mr/amip_mpi_esm_mr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/mpi_esm_mr/historical_mpi_esm_mr.nc 2019-12-04 17:31:06,183 INFO Downloading http://136.156.132.236/cache-compute-0007/cache/data4/5a7ba2a3-45de-40a6-a5a6-d60a4b95c493-psl_Amon_NorESM1-M_historical_r1i1p1_185001-200512.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/noresm1_m/historical_noresm1_m.zip (98.8M) 2019-12-04 17:31:16,066 INFO Download rate 10M/s /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/noresm1_m/amip_noresm1_m.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/rcp/noresm1_m/historical_noresm1_m.nc","title":"Download Datasets"},{"location":"notebooks/climate/amip/0_data_download/#load-datasets","text":"loader = DataLoader () dataset = 'ipsl_cm5a_mr' xr_data = loader . load_rcp_data ( dataset ) xr_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 143, lon: 144, time: 3012) Coordinates: * lon (lon) float64 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * lat (lat) float64 -90.0 -88.73 -87.46 -86.2 ... 86.2 87.46 88.73 90.0 * time (time) object 1850-01-16 12:00:00 ... 2100-12-16 12:00:00 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object dask.array<chunksize=(1872, 2), meta=np.ndarray> lat_bnds (time, lat, bnds) float64 dask.array<chunksize=(1872, 143, 2), meta=np.ndarray> lon_bnds (time, lon, bnds) float64 dask.array<chunksize=(1872, 144, 2), meta=np.ndarray> psl (time, lat, lon) float32 dask.array<chunksize=(1872, 143, 144), meta=np.ndarray> Attributes: institution: IPSL (Institut Pierre Simon Laplace, Paris, France) institute_id: IPSL experiment_id: historical source: IPSL-CM5A-MR (2010) : atmos : LMDZ4 (LMDZ4_v5, 14... model_id: IPSL-CM5A-MR forcing: Nat,Ant,GHG,SA,Oz,LU,SS,Ds,BC,MD,OC,AA parent_experiment_id: piControl parent_experiment_rip: r1i1p1 branch_time: 1850.0 contact: ipsl-cmip5 _at_ ipsl.jussieu.fr Data manager : Se... comment: This 20th century simulation include natural and ... references: Model documentation and further reference availab... initialization_method: 1 physics_version: 1 tracking_id: ee34ce17-a474-47bb-b311-c00df6dfbf7a product: output experiment: historical frequency: mon creation_date: 2011-09-22T20:43:25Z history: 2011-09-22T20:43:25Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: IPSL-CM5A-MR model output prepared for CMIP5 hist... parent_experiment: pre-industrial control modeling_realm: atmos realization: 1 cmor_version: 2.7.1","title":"Load Datasets"},{"location":"notebooks/climate/amip/0_data_download/#testing","text":"datasets = [ \"inmcm4\" , \"access1_0\" , \"access1_3\" , \"ipsl_cm5a_mr\" , \"mpi_esm_lr\" , \"mpi_esm_mr\" , \"noresm1_m\" , ] loader = DataLoader () for idataset in datasets : print ( idataset ) data = loader . load_rcp_data ( idataset ) print ( data . psl . shape ) assert ( type ( data ) is xr . Dataset ) inmcm4 (3012, 120, 180) access1_0 (3012, 145, 192) access1_3 (3012, 145, 192) ipsl_cm5a_mr (3012, 143, 144) mpi_esm_lr (3012, 96, 192) mpi_esm_mr (3012, 96, 192) noresm1_m (3012, 96, 144)","title":"Testing"},{"location":"notebooks/climate/amip/1.1_compare_similar-Copy1/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Comparing Two Climate Models \u00b6 In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures. Data - Climate Models \u00b6 import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Import RBIG Helper from src.models.train_models import run_rbig_models # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) from esdc.standardize import normalize_temporal from esdc.transform import regrid_data import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\" ERA5 \u00b6 era5_data = xr . open_dataset ( f \" { data_path } ERA5.nc\" ) era5_data = era5_data . rename ({ 'msl' : 'mslp' , 'latitude' : 'lat' , 'longitude' : 'lon' }) # era5_data = era5_data.rename({'latitude': 'lat'}) # era5_data.attrs['model_id'] = 'era5' # rescale model from 0.25 to 2.5 degrees # era5_data = era5_data.coarsen(lat=10, lon=10, boundary='pad').mean() era5_data . attrs [ 'model_id' ] = 'era5' era5_data <xarray.Dataset> Dimensions: (lat: 721, lon: 1440, time: 487) Coordinates: * lon (lon) float32 0.0 0.25 0.5 0.75 1.0 ... 359.0 359.25 359.5 359.75 * lat (lat) float32 90.0 89.75 89.5 89.25 ... -89.25 -89.5 -89.75 -90.0 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-07-01 Data variables: mslp (time, lat, lon) float32 ... sp (time, lat, lon) float32 ... Attributes: Conventions: CF-1.6 history: 2019-10-07 09:20:10 GMT by grib_to_netcdf-2.10.0: /opt/ecmw... model_id: era5 NCAR-NCEP-DOE-II \u00b6 ncep_data = xr . open_mfdataset ( f \" { data_path } *mon.mean.nc\" ) ncep_data = ncep_data . rename ({ 'pres' : 'sp' }) ncep_data . attrs [ 'model_id' ] = 'ncar_ncep_doe_2' ncep_data <xarray.Dataset> Dimensions: (lat: 73, lon: 144, nbnds: 2, time: 489) Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Dimensions without coordinates: nbnds Data variables: mslp (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> time_bnds (time, nbnds) datetime64[ns] dask.array<chunksize=(489, 2), meta=np.ndarray> pr_wtr (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> sp (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> Attributes: Conventions: CF-1.0 title: Monthly NCEP/DOE Reanalysis 2 history: created 2002/03 by Hoop (netCDF2.3) comments: Data is from \\nNCEP/DOE AMIP-II Reanalysis (Reanalysis-2)... platform: Model source: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Model institution: National Centers for Environmental Prediction dataset_title: NCEP-DOE AMIP-II Reanalysis References: https://www.esrl.noaa.gov/psd/data/gridded/data.ncep.rean... source_url: http://www.cpc.ncep.noaa.gov/products/wesley/reanalysis2/ model_id: ncar_ncep_doe_2 Regridding \u00b6 era5_data_regrid = regrid_data ( ncep_data , era5_data ) Create weight file: nearest_s2d_721x1440_73x144.nc Remove file nearest_s2d_721x1440_73x144.nc era5_data_regrid . attrs = era5_data . attrs era5_data_regrid = xr . Dataset () era5_data_regrid = xr . Dataset () era5_data_regrid [ 'sp' ] = era5_regrid era5_data_regrid . attrs = era5_data . attrs era5_data_regrid <xarray.Dataset> Dimensions: (lat: 73, lon: 144, time: 487) Coordinates: * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-07-01 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 Data variables: sp (time, lat, lon) float64 1.027e+05 1.027e+05 ... 6.859e+04 Attributes: Conventions: CF-1.6 history: 2019-10-07 09:20:10 GMT by grib_to_netcdf-2.10.0: /opt/ecmw... model_id: era5 CMIP5 \u00b6 cmip5_data = xr . open_dataset ( f \" { data_path } CMIP5.nc\" ) cmip5_data = cmip5_data . rename ({ 'psl' : 'mslp' }) # rescale model from 0.25 to 2.5 degrees # cmip5_data = cmip5_data.coarsen(lat=1, boundary='pad').mean() cmip5_data . attrs [ 'model_id' ] = 'cmip5' cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 90, lon: 144, time: 240) Coordinates: * time (time) object 2006-01-16 12:00:00 ... 2025-12-16 12:00:00 * lat (lat) float64 -89.0 -87.0 -85.0 -83.0 ... 83.0 85.0 87.0 89.0 * lon (lon) float64 1.25 3.75 6.25 8.75 ... 351.2 353.8 356.2 358.8 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object ... lat_bnds (lat, bnds) float64 ... lon_bnds (lon, bnds) float64 ... mslp (time, lat, lon) float32 ... Attributes: institution: NASA/GISS (Goddard Institute for Space Studies) N... institute_id: NASA-GISS experiment_id: rcp85 source: GISS-E2-R-E135RCP85aF40oQ32 Atmosphere: GISS-E2; ... model_id: cmip5 forcing: GHG, LU, Sl, Vl, BC, OC, SA, Oz (also includes or... parent_experiment_id: historical parent_experiment_rip: r1i1p1 branch_time: 2006.0 contact: Kenneth Lo (cdkkl@giss.nasa.gov) references: www.giss.nasa.gov/research/modeling initialization_method: 1 physics_version: 1 tracking_id: 71ff3d6b-02eb-470f-a25d-5e79c1b8c1b5 product: output experiment: RCP8.5 frequency: mon creation_date: 2011-08-30T18:58:55Z history: 2011-08-30T18:58:55Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: GISS-E2-R model output prepared for CMIP5 RCP8.5 parent_experiment: historical modeling_realm: atmos realization: 1 cmor_version: 2.5.7 cmip5_regrid = regrid_data ( ncep_data . mslp , cmip5_data . mslp ) cmip5_data_regrid = xr . Dataset () cmip5_data_regrid [ 'mslp' ] = cmip5_regrid cmip5_data_regrid . attrs = cmip5_data . attrs cmip5_data_regrid Reuse existing file: nearest_s2d_90x144_73x144.nc Remove file nearest_s2d_90x144_73x144.nc <xarray.Dataset> Dimensions: (lat: 73, lon: 144, time: 240) Coordinates: * time (time) object 2006-01-16 12:00:00 ... 2025-12-16 12:00:00 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 Data variables: mslp (time, lat, lon) float64 9.993e+04 9.993e+04 ... 9.985e+04 Attributes: institution: NASA/GISS (Goddard Institute for Space Studies) N... institute_id: NASA-GISS experiment_id: rcp85 source: GISS-E2-R-E135RCP85aF40oQ32 Atmosphere: GISS-E2; ... model_id: cmip5 forcing: GHG, LU, Sl, Vl, BC, OC, SA, Oz (also includes or... parent_experiment_id: historical parent_experiment_rip: r1i1p1 branch_time: 2006.0 contact: Kenneth Lo (cdkkl@giss.nasa.gov) references: www.giss.nasa.gov/research/modeling initialization_method: 1 physics_version: 1 tracking_id: 71ff3d6b-02eb-470f-a25d-5e79c1b8c1b5 product: output experiment: RCP8.5 frequency: mon creation_date: 2011-08-30T18:58:55Z history: 2011-08-30T18:58:55Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: GISS-E2-R model output prepared for CMIP5 RCP8.5 parent_experiment: historical modeling_realm: atmos realization: 1 cmor_version: 2.5.7 Experiment I - Comparing Climate Models \u00b6 Mean Sea Level Pressure \u00b6 ERA5 vs NCEP \u00b6 # Experiment class class ClimateEntropy : def __init__ ( self , save_path : None , variable : str = 'mslp' , save_name = None , mi : bool = True ): self . variable = variable self . results_path = save_path self . results_df = pd . DataFrame () self . save_name = save_name self . mi = mi def run_experiment ( self , climate_model1 : pd . DataFrame , climate_model2 : pd . DataFrame ) -> None : \"\"\"Training loop that goes through each year and calculates the entropy, total correlation and mutual information between the two models.\"\"\" time_length = len ( climate_model1 . groupby ( 'time.year' )) # Normalize BEFORE the individual calculations climate_model1 [ self . variable ] = normalize_temporal ( climate_model1 [ self . variable ]) model1_id = climate_model1 . attrs [ 'model_id' ] model2_id = climate_model2 . attrs [ 'model_id' ] climate_model2 [ self . variable ] = normalize_temporal ( climate_model2 [ self . variable ]) with tqdm ( zip ( climate_model1 . groupby ( 'time.year' ), climate_model2 . groupby ( 'time.year' ) ), total = time_length ) as progress_bar : for imodel1 , imodel2 in progress_bar : # Update params in progress bar # Transform to dataframe, remove spatial dimensions X1 = self . _get_time_features ( imodel1 [ 1 ][ self . variable ]) X2 = self . _get_time_features ( imodel2 [ 1 ][ self . variable ]) # Normalize inputs min_max_scaler = preprocessing . StandardScaler () X1 = min_max_scaler . fit_transform ( X1 . values ) X2 = min_max_scaler . fit_transform ( X2 . values ) dims = X1 . shape [ 1 ] # ============================= # Calculate Mutual Information # ============================= if self . mi == False : mi_ = None mi_t_ = None else : mi_ , mi_t_ = run_rbig_models ( X1 , X2 , measure = 'mi' , verbose = None ) # Update params in progress bar postfix = dict ( ) # ======================================== # Calculate Entropy and Total Correlation # ======================================== # Model I tc1_ , h1_ , h_t1_ = run_rbig_models ( X1 , measure = 't' , verbose = None ) self . _update_results ( model = model1_id , year = imodel1 [ 0 ], h_time = h_t1_ , tc = tc1_ , h = h1_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Model II tc2_ , h2_ , h_t2_ = run_rbig_models ( X2 , measure = 't' , verbose = None ) self . _update_results ( model = model2_id , year = imodel2 [ 0 ], h_time = h_t2_ , tc = tc2_ , h = h2_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Update params in progress bar postfix = dict ( year = imodel1 [ 0 ], mi = f \" { mi_ : .3f } \" if self . mi is True else None , h1 = f \" { h1_ : .3f } \" , tc1 = f \" { tc1_ : .3f } \" , h2 = f \" { h2_ : .3f } \" , tc2 = f \" { tc2_ : .3f } \" , ) progress_bar . set_postfix ( postfix ) return None def _get_time_features ( self , data_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"This function collapses the spatial dimensions as pivots. This allows us to only consider time as the input feature.\"\"\" return data_df . to_dataframe () . unstack ( level = 0 ) . reset_index () . drop ( columns = [ 'lat' , 'lon' ]) . dropna () def _update_results ( self , model , year , tc , h , h_time , mi , mi_time , dims ): \"\"\"appends new values to the results dataframe.\"\"\" self . results_df = self . results_df . append ({ 'model' : model , 'year' : year , 'tc' : tc , 'h' : h , 'h_time' : h_time , 'mi' : mi , 'mi_time' : mi_time , 'dims' : dims , }, ignore_index = True ) if self . results_path is not None : self . _save_results () return self def _save_results ( self ): \"\"\"Saves the dataframe to the assigned results path.\"\"\" self . results_df . to_csv ( f \" { self . results_path }{ self . variable } _ { self . save_name } .csv\" ) return None # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_ncep' ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid , ncep_data ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 41/41 [1:43:03<00:00, 150.83s/it, year=2019, mi=4.766, h1=-1.313, tc1=9.551, h2=-3.049, tc2=13.532] # extract results results_df = short_decade_exp . results_df ERA5 vs CMIP5 \u00b6 2006 - 01 - 16 , 2025 - 12 - 16 , 1979 - 01 - 01 , 2019 - 07 - 01 # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:08<00:00, 150.61s/it, year=2019, mi=3.290, h1=-1.509, tc1=9.747, h2=-0.807, tc2=8.142] NCEP vs CMIP5 \u00b6 # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'ncep_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( ncep_data . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:04<00:00, 150.30s/it, year=2019, mi=3.552, h1=-1.495, tc1=9.760, h2=-0.807, tc2=8.142]","title":"1.1 compare similar Copy1"},{"location":"notebooks/climate/amip/1.1_compare_similar-Copy1/#comparing-two-climate-models","text":"In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures.","title":"Comparing Two Climate Models"},{"location":"notebooks/climate/amip/1.1_compare_similar-Copy1/#data-climate-models","text":"import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Import RBIG Helper from src.models.train_models import run_rbig_models # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) from esdc.standardize import normalize_temporal from esdc.transform import regrid_data import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\"","title":"Data - Climate Models"},{"location":"notebooks/climate/amip/1.1_compare_similar-Copy1/#era5","text":"era5_data = xr . open_dataset ( f \" { data_path } ERA5.nc\" ) era5_data = era5_data . rename ({ 'msl' : 'mslp' , 'latitude' : 'lat' , 'longitude' : 'lon' }) # era5_data = era5_data.rename({'latitude': 'lat'}) # era5_data.attrs['model_id'] = 'era5' # rescale model from 0.25 to 2.5 degrees # era5_data = era5_data.coarsen(lat=10, lon=10, boundary='pad').mean() era5_data . attrs [ 'model_id' ] = 'era5' era5_data <xarray.Dataset> Dimensions: (lat: 721, lon: 1440, time: 487) Coordinates: * lon (lon) float32 0.0 0.25 0.5 0.75 1.0 ... 359.0 359.25 359.5 359.75 * lat (lat) float32 90.0 89.75 89.5 89.25 ... -89.25 -89.5 -89.75 -90.0 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-07-01 Data variables: mslp (time, lat, lon) float32 ... sp (time, lat, lon) float32 ... Attributes: Conventions: CF-1.6 history: 2019-10-07 09:20:10 GMT by grib_to_netcdf-2.10.0: /opt/ecmw... model_id: era5","title":"ERA5"},{"location":"notebooks/climate/amip/1.1_compare_similar-Copy1/#ncar-ncep-doe-ii","text":"ncep_data = xr . open_mfdataset ( f \" { data_path } *mon.mean.nc\" ) ncep_data = ncep_data . rename ({ 'pres' : 'sp' }) ncep_data . attrs [ 'model_id' ] = 'ncar_ncep_doe_2' ncep_data <xarray.Dataset> Dimensions: (lat: 73, lon: 144, nbnds: 2, time: 489) Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Dimensions without coordinates: nbnds Data variables: mslp (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> time_bnds (time, nbnds) datetime64[ns] dask.array<chunksize=(489, 2), meta=np.ndarray> pr_wtr (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> sp (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> Attributes: Conventions: CF-1.0 title: Monthly NCEP/DOE Reanalysis 2 history: created 2002/03 by Hoop (netCDF2.3) comments: Data is from \\nNCEP/DOE AMIP-II Reanalysis (Reanalysis-2)... platform: Model source: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Model institution: National Centers for Environmental Prediction dataset_title: NCEP-DOE AMIP-II Reanalysis References: https://www.esrl.noaa.gov/psd/data/gridded/data.ncep.rean... source_url: http://www.cpc.ncep.noaa.gov/products/wesley/reanalysis2/ model_id: ncar_ncep_doe_2","title":"NCAR-NCEP-DOE-II"},{"location":"notebooks/climate/amip/1.1_compare_similar-Copy1/#regridding","text":"era5_data_regrid = regrid_data ( ncep_data , era5_data ) Create weight file: nearest_s2d_721x1440_73x144.nc Remove file nearest_s2d_721x1440_73x144.nc era5_data_regrid . attrs = era5_data . attrs era5_data_regrid = xr . Dataset () era5_data_regrid = xr . Dataset () era5_data_regrid [ 'sp' ] = era5_regrid era5_data_regrid . attrs = era5_data . attrs era5_data_regrid <xarray.Dataset> Dimensions: (lat: 73, lon: 144, time: 487) Coordinates: * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-07-01 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 Data variables: sp (time, lat, lon) float64 1.027e+05 1.027e+05 ... 6.859e+04 Attributes: Conventions: CF-1.6 history: 2019-10-07 09:20:10 GMT by grib_to_netcdf-2.10.0: /opt/ecmw... model_id: era5","title":"Regridding"},{"location":"notebooks/climate/amip/1.1_compare_similar-Copy1/#cmip5","text":"cmip5_data = xr . open_dataset ( f \" { data_path } CMIP5.nc\" ) cmip5_data = cmip5_data . rename ({ 'psl' : 'mslp' }) # rescale model from 0.25 to 2.5 degrees # cmip5_data = cmip5_data.coarsen(lat=1, boundary='pad').mean() cmip5_data . attrs [ 'model_id' ] = 'cmip5' cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 90, lon: 144, time: 240) Coordinates: * time (time) object 2006-01-16 12:00:00 ... 2025-12-16 12:00:00 * lat (lat) float64 -89.0 -87.0 -85.0 -83.0 ... 83.0 85.0 87.0 89.0 * lon (lon) float64 1.25 3.75 6.25 8.75 ... 351.2 353.8 356.2 358.8 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object ... lat_bnds (lat, bnds) float64 ... lon_bnds (lon, bnds) float64 ... mslp (time, lat, lon) float32 ... Attributes: institution: NASA/GISS (Goddard Institute for Space Studies) N... institute_id: NASA-GISS experiment_id: rcp85 source: GISS-E2-R-E135RCP85aF40oQ32 Atmosphere: GISS-E2; ... model_id: cmip5 forcing: GHG, LU, Sl, Vl, BC, OC, SA, Oz (also includes or... parent_experiment_id: historical parent_experiment_rip: r1i1p1 branch_time: 2006.0 contact: Kenneth Lo (cdkkl@giss.nasa.gov) references: www.giss.nasa.gov/research/modeling initialization_method: 1 physics_version: 1 tracking_id: 71ff3d6b-02eb-470f-a25d-5e79c1b8c1b5 product: output experiment: RCP8.5 frequency: mon creation_date: 2011-08-30T18:58:55Z history: 2011-08-30T18:58:55Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: GISS-E2-R model output prepared for CMIP5 RCP8.5 parent_experiment: historical modeling_realm: atmos realization: 1 cmor_version: 2.5.7 cmip5_regrid = regrid_data ( ncep_data . mslp , cmip5_data . mslp ) cmip5_data_regrid = xr . Dataset () cmip5_data_regrid [ 'mslp' ] = cmip5_regrid cmip5_data_regrid . attrs = cmip5_data . attrs cmip5_data_regrid Reuse existing file: nearest_s2d_90x144_73x144.nc Remove file nearest_s2d_90x144_73x144.nc <xarray.Dataset> Dimensions: (lat: 73, lon: 144, time: 240) Coordinates: * time (time) object 2006-01-16 12:00:00 ... 2025-12-16 12:00:00 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 Data variables: mslp (time, lat, lon) float64 9.993e+04 9.993e+04 ... 9.985e+04 Attributes: institution: NASA/GISS (Goddard Institute for Space Studies) N... institute_id: NASA-GISS experiment_id: rcp85 source: GISS-E2-R-E135RCP85aF40oQ32 Atmosphere: GISS-E2; ... model_id: cmip5 forcing: GHG, LU, Sl, Vl, BC, OC, SA, Oz (also includes or... parent_experiment_id: historical parent_experiment_rip: r1i1p1 branch_time: 2006.0 contact: Kenneth Lo (cdkkl@giss.nasa.gov) references: www.giss.nasa.gov/research/modeling initialization_method: 1 physics_version: 1 tracking_id: 71ff3d6b-02eb-470f-a25d-5e79c1b8c1b5 product: output experiment: RCP8.5 frequency: mon creation_date: 2011-08-30T18:58:55Z history: 2011-08-30T18:58:55Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: GISS-E2-R model output prepared for CMIP5 RCP8.5 parent_experiment: historical modeling_realm: atmos realization: 1 cmor_version: 2.5.7","title":"CMIP5"},{"location":"notebooks/climate/amip/1.1_compare_similar-Copy1/#experiment-i-comparing-climate-models","text":"","title":"Experiment I - Comparing Climate Models"},{"location":"notebooks/climate/amip/1.1_compare_similar-Copy1/#mean-sea-level-pressure","text":"","title":"Mean Sea Level Pressure"},{"location":"notebooks/climate/amip/1.1_compare_similar-Copy1/#era5-vs-ncep","text":"# Experiment class class ClimateEntropy : def __init__ ( self , save_path : None , variable : str = 'mslp' , save_name = None , mi : bool = True ): self . variable = variable self . results_path = save_path self . results_df = pd . DataFrame () self . save_name = save_name self . mi = mi def run_experiment ( self , climate_model1 : pd . DataFrame , climate_model2 : pd . DataFrame ) -> None : \"\"\"Training loop that goes through each year and calculates the entropy, total correlation and mutual information between the two models.\"\"\" time_length = len ( climate_model1 . groupby ( 'time.year' )) # Normalize BEFORE the individual calculations climate_model1 [ self . variable ] = normalize_temporal ( climate_model1 [ self . variable ]) model1_id = climate_model1 . attrs [ 'model_id' ] model2_id = climate_model2 . attrs [ 'model_id' ] climate_model2 [ self . variable ] = normalize_temporal ( climate_model2 [ self . variable ]) with tqdm ( zip ( climate_model1 . groupby ( 'time.year' ), climate_model2 . groupby ( 'time.year' ) ), total = time_length ) as progress_bar : for imodel1 , imodel2 in progress_bar : # Update params in progress bar # Transform to dataframe, remove spatial dimensions X1 = self . _get_time_features ( imodel1 [ 1 ][ self . variable ]) X2 = self . _get_time_features ( imodel2 [ 1 ][ self . variable ]) # Normalize inputs min_max_scaler = preprocessing . StandardScaler () X1 = min_max_scaler . fit_transform ( X1 . values ) X2 = min_max_scaler . fit_transform ( X2 . values ) dims = X1 . shape [ 1 ] # ============================= # Calculate Mutual Information # ============================= if self . mi == False : mi_ = None mi_t_ = None else : mi_ , mi_t_ = run_rbig_models ( X1 , X2 , measure = 'mi' , verbose = None ) # Update params in progress bar postfix = dict ( ) # ======================================== # Calculate Entropy and Total Correlation # ======================================== # Model I tc1_ , h1_ , h_t1_ = run_rbig_models ( X1 , measure = 't' , verbose = None ) self . _update_results ( model = model1_id , year = imodel1 [ 0 ], h_time = h_t1_ , tc = tc1_ , h = h1_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Model II tc2_ , h2_ , h_t2_ = run_rbig_models ( X2 , measure = 't' , verbose = None ) self . _update_results ( model = model2_id , year = imodel2 [ 0 ], h_time = h_t2_ , tc = tc2_ , h = h2_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Update params in progress bar postfix = dict ( year = imodel1 [ 0 ], mi = f \" { mi_ : .3f } \" if self . mi is True else None , h1 = f \" { h1_ : .3f } \" , tc1 = f \" { tc1_ : .3f } \" , h2 = f \" { h2_ : .3f } \" , tc2 = f \" { tc2_ : .3f } \" , ) progress_bar . set_postfix ( postfix ) return None def _get_time_features ( self , data_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"This function collapses the spatial dimensions as pivots. This allows us to only consider time as the input feature.\"\"\" return data_df . to_dataframe () . unstack ( level = 0 ) . reset_index () . drop ( columns = [ 'lat' , 'lon' ]) . dropna () def _update_results ( self , model , year , tc , h , h_time , mi , mi_time , dims ): \"\"\"appends new values to the results dataframe.\"\"\" self . results_df = self . results_df . append ({ 'model' : model , 'year' : year , 'tc' : tc , 'h' : h , 'h_time' : h_time , 'mi' : mi , 'mi_time' : mi_time , 'dims' : dims , }, ignore_index = True ) if self . results_path is not None : self . _save_results () return self def _save_results ( self ): \"\"\"Saves the dataframe to the assigned results path.\"\"\" self . results_df . to_csv ( f \" { self . results_path }{ self . variable } _ { self . save_name } .csv\" ) return None # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_ncep' ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid , ncep_data ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 41/41 [1:43:03<00:00, 150.83s/it, year=2019, mi=4.766, h1=-1.313, tc1=9.551, h2=-3.049, tc2=13.532] # extract results results_df = short_decade_exp . results_df","title":"ERA5 vs NCEP"},{"location":"notebooks/climate/amip/1.1_compare_similar-Copy1/#era5-vs-cmip5","text":"2006 - 01 - 16 , 2025 - 12 - 16 , 1979 - 01 - 01 , 2019 - 07 - 01 # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:08<00:00, 150.61s/it, year=2019, mi=3.290, h1=-1.509, tc1=9.747, h2=-0.807, tc2=8.142]","title":"ERA5 vs CMIP5"},{"location":"notebooks/climate/amip/1.1_compare_similar-Copy1/#ncep-vs-cmip5","text":"# Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'ncep_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( ncep_data . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:04<00:00, 150.30s/it, year=2019, mi=3.552, h1=-1.495, tc1=9.760, h2=-0.807, tc2=8.142]","title":"NCEP vs CMIP5"},{"location":"notebooks/climate/amip/1_regridding/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Comparing Two Climate Models \u00b6 In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures. Data - Climate Models \u00b6 import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.amip import DataDownloader from src.data.climate.amip import DataLoader # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) from esdc.standardize import normalize_temporal from esdc.transform import regrid_data import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload amip_data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" era5_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/era5/\" ncep_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/ncep/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\" Dataset - GISS \u00b6 loader = DataLoader () dataset = 'ipsl_cm5b_lr' cmip5_data = loader . load_amip_data ( dataset ) cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 96, lon: 96, time: 360) Coordinates: * time (time) object 1979-01-16 12:00:00 ... 2008-12-16 12:00:00 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object dask.array<chunksize=(360, 2), meta=np.ndarray> lat_bnds (lat, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> lon_bnds (lon, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> psl (time, lat, lon) float32 dask.array<chunksize=(360, 96, 96), meta=np.ndarray> Attributes: institution: IPSL (Institut Pierre Simon Laplace, Paris, France) institute_id: IPSL experiment_id: amip source: IPSL-CM5B-LR (2011) : atmos : LMDZ5 (LMDZ5_NPv3.1... model_id: IPSL-CM5B-LR forcing: Nat,Ant,GHG,SA,Oz,LU,SS,Ds,BC,MD,OC,AA parent_experiment_id: N/A parent_experiment_rip: N/A branch_time: 0.0 contact: ipsl-cmip5 _at_ ipsl.jussieu.fr Data manager : Se... comment: This atmospheric only simulation include natural ... references: Model documentation and further reference availab... initialization_method: 1 physics_version: 1 tracking_id: b06de09a-a87a-4f4d-bb64-63ad47eab808 product: output experiment: AMIP frequency: mon creation_date: 2012-06-11T18:49:23Z history: 2012-06-11T18:49:23Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: IPSL-CM5B-LR model output prepared for CMIP5 AMIP parent_experiment: N/A modeling_realm: atmos realization: 1 cmor_version: 2.7.1 Test I - AMIP vs. ERA5 \u00b6 ERA5 \u00b6 def get_era5 (): era5_data = xr . open_mfdataset ( f \" { era5_path } *.nc\" , combine = \"by_coords\" ) era5_data = era5_data . rename ({ 'msl' : 'mslp' , 'latitude' : 'lat' , 'longitude' : 'lon' }) era5_data . attrs [ 'model_id' ] = 'era5' era5_data = era5_data . rename ({ 'mslp' : 'psl' }) return era5_data era5_data = get_era5 () <xarray.Dataset> Dimensions: (lat: 721, lon: 1440, time: 487) Coordinates: * lon (lon) float32 0.0 0.25 0.5 0.75 1.0 ... 359.0 359.25 359.5 359.75 * lat (lat) float32 90.0 89.75 89.5 89.25 ... -89.25 -89.5 -89.75 -90.0 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-07-01 Data variables: psl (time, lat, lon) float32 dask.array<chunksize=(487, 721, 1440), meta=np.ndarray> sp (time, lat, lon) float32 dask.array<chunksize=(487, 721, 1440), meta=np.ndarray> Attributes: Conventions: CF-1.6 history: 2019-10-07 09:20:10 GMT by grib_to_netcdf-2.10.0: /opt/ecmw... model_id: era5 ReGrid \u00b6 cmip5_coords = len ( cmip5_data . lat ) + len ( cmip5_data . lon ) era5_coords = len ( era5_data . lat ) + len ( era5_data . lon ) if cmip5_coords >= era5_coords : cmip5_data = regrid_data ( era5_data , cmip5_data ) else : era5_data = regrid_data ( cmip5_data , era5_data ) Create weight file: nearest_s2d_721x1440_96x96.nc Remove file nearest_s2d_721x1440_96x96.nc era5_data <xarray.Dataset> Dimensions: (lat: 96, lon: 96, time: 487) Coordinates: * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-07-01 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 84.32 86.21 88.11 90.0 Data variables: psl (time, lat, lon) float64 dask.array<chunksize=(487, 96, 96), meta=np.ndarray> sp (time, lat, lon) float64 dask.array<chunksize=(487, 96, 96), meta=np.ndarray> Test II - NCEP Data \u00b6 ncep_data = xr . open_mfdataset ( f \" { ncep_path } *mon.mean.nc\" , combine = \"by_coords\" ) ncep_data = ncep_data . rename ({ 'mslp' : 'psl' }) ncep_data . attrs [ 'model_id' ] = 'ncar_ncep_doe_2' ncep_data <xarray.Dataset> Dimensions: (lat: 73, lon: 144, nbnds: 2, time: 489) Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Dimensions without coordinates: nbnds Data variables: psl (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> time_bnds (time, nbnds) datetime64[ns] dask.array<chunksize=(489, 2), meta=np.ndarray> pr_wtr (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> pres (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> Attributes: Conventions: CF-1.0 title: Monthly NCEP/DOE Reanalysis 2 history: created 2002/03 by Hoop (netCDF2.3) comments: Data is from \\nNCEP/DOE AMIP-II Reanalysis (Reanalysis-2)... platform: Model source: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Model institution: National Centers for Environmental Prediction dataset_title: NCEP-DOE AMIP-II Reanalysis References: https://www.esrl.noaa.gov/psd/data/gridded/data.ncep.rean... source_url: http://www.cpc.ncep.noaa.gov/products/wesley/reanalysis2/ model_id: ncar_ncep_doe_2 ReGrid \u00b6 cmip5_coords = len ( cmip5_data . lat ) + len ( cmip5_data . lon ) ncep_coords = len ( ncep_data . lat ) + len ( ncep_data . lon ) if cmip5_coords >= era5_coords : cmip5_data = regrid_data ( ncep_data , cmip5_data ) else : ncep_data = regrid_data ( cmip5_data , ncep_data ) Create weight file: nearest_s2d_73x144_96x96.nc Remove file nearest_s2d_73x144_96x96.nc ncep_data <xarray.Dataset> Dimensions: (lat: 96, lon: 96, time: 489) Coordinates: * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 84.32 86.21 88.11 90.0 Data variables: psl (time, lat, lon) float64 dask.array<chunksize=(489, 96, 96), meta=np.ndarray> pr_wtr (time, lat, lon) float64 dask.array<chunksize=(489, 96, 96), meta=np.ndarray> pres (time, lat, lon) float64 dask.array<chunksize=(489, 96, 96), meta=np.ndarray>","title":"1 regridding"},{"location":"notebooks/climate/amip/1_regridding/#comparing-two-climate-models","text":"In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures.","title":"Comparing Two Climate Models"},{"location":"notebooks/climate/amip/1_regridding/#data-climate-models","text":"import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.amip import DataDownloader from src.data.climate.amip import DataLoader # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) from esdc.standardize import normalize_temporal from esdc.transform import regrid_data import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload amip_data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" era5_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/era5/\" ncep_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/ncep/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\"","title":"Data - Climate Models"},{"location":"notebooks/climate/amip/1_regridding/#dataset-giss","text":"loader = DataLoader () dataset = 'ipsl_cm5b_lr' cmip5_data = loader . load_amip_data ( dataset ) cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 96, lon: 96, time: 360) Coordinates: * time (time) object 1979-01-16 12:00:00 ... 2008-12-16 12:00:00 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object dask.array<chunksize=(360, 2), meta=np.ndarray> lat_bnds (lat, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> lon_bnds (lon, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> psl (time, lat, lon) float32 dask.array<chunksize=(360, 96, 96), meta=np.ndarray> Attributes: institution: IPSL (Institut Pierre Simon Laplace, Paris, France) institute_id: IPSL experiment_id: amip source: IPSL-CM5B-LR (2011) : atmos : LMDZ5 (LMDZ5_NPv3.1... model_id: IPSL-CM5B-LR forcing: Nat,Ant,GHG,SA,Oz,LU,SS,Ds,BC,MD,OC,AA parent_experiment_id: N/A parent_experiment_rip: N/A branch_time: 0.0 contact: ipsl-cmip5 _at_ ipsl.jussieu.fr Data manager : Se... comment: This atmospheric only simulation include natural ... references: Model documentation and further reference availab... initialization_method: 1 physics_version: 1 tracking_id: b06de09a-a87a-4f4d-bb64-63ad47eab808 product: output experiment: AMIP frequency: mon creation_date: 2012-06-11T18:49:23Z history: 2012-06-11T18:49:23Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: IPSL-CM5B-LR model output prepared for CMIP5 AMIP parent_experiment: N/A modeling_realm: atmos realization: 1 cmor_version: 2.7.1","title":"Dataset - GISS"},{"location":"notebooks/climate/amip/1_regridding/#test-i-amip-vs-era5","text":"","title":"Test I - AMIP vs. ERA5"},{"location":"notebooks/climate/amip/1_regridding/#era5","text":"def get_era5 (): era5_data = xr . open_mfdataset ( f \" { era5_path } *.nc\" , combine = \"by_coords\" ) era5_data = era5_data . rename ({ 'msl' : 'mslp' , 'latitude' : 'lat' , 'longitude' : 'lon' }) era5_data . attrs [ 'model_id' ] = 'era5' era5_data = era5_data . rename ({ 'mslp' : 'psl' }) return era5_data era5_data = get_era5 () <xarray.Dataset> Dimensions: (lat: 721, lon: 1440, time: 487) Coordinates: * lon (lon) float32 0.0 0.25 0.5 0.75 1.0 ... 359.0 359.25 359.5 359.75 * lat (lat) float32 90.0 89.75 89.5 89.25 ... -89.25 -89.5 -89.75 -90.0 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-07-01 Data variables: psl (time, lat, lon) float32 dask.array<chunksize=(487, 721, 1440), meta=np.ndarray> sp (time, lat, lon) float32 dask.array<chunksize=(487, 721, 1440), meta=np.ndarray> Attributes: Conventions: CF-1.6 history: 2019-10-07 09:20:10 GMT by grib_to_netcdf-2.10.0: /opt/ecmw... model_id: era5","title":"ERA5"},{"location":"notebooks/climate/amip/1_regridding/#regrid","text":"cmip5_coords = len ( cmip5_data . lat ) + len ( cmip5_data . lon ) era5_coords = len ( era5_data . lat ) + len ( era5_data . lon ) if cmip5_coords >= era5_coords : cmip5_data = regrid_data ( era5_data , cmip5_data ) else : era5_data = regrid_data ( cmip5_data , era5_data ) Create weight file: nearest_s2d_721x1440_96x96.nc Remove file nearest_s2d_721x1440_96x96.nc era5_data <xarray.Dataset> Dimensions: (lat: 96, lon: 96, time: 487) Coordinates: * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-07-01 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 84.32 86.21 88.11 90.0 Data variables: psl (time, lat, lon) float64 dask.array<chunksize=(487, 96, 96), meta=np.ndarray> sp (time, lat, lon) float64 dask.array<chunksize=(487, 96, 96), meta=np.ndarray>","title":"ReGrid"},{"location":"notebooks/climate/amip/1_regridding/#test-ii-ncep-data","text":"ncep_data = xr . open_mfdataset ( f \" { ncep_path } *mon.mean.nc\" , combine = \"by_coords\" ) ncep_data = ncep_data . rename ({ 'mslp' : 'psl' }) ncep_data . attrs [ 'model_id' ] = 'ncar_ncep_doe_2' ncep_data <xarray.Dataset> Dimensions: (lat: 73, lon: 144, nbnds: 2, time: 489) Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Dimensions without coordinates: nbnds Data variables: psl (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> time_bnds (time, nbnds) datetime64[ns] dask.array<chunksize=(489, 2), meta=np.ndarray> pr_wtr (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> pres (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> Attributes: Conventions: CF-1.0 title: Monthly NCEP/DOE Reanalysis 2 history: created 2002/03 by Hoop (netCDF2.3) comments: Data is from \\nNCEP/DOE AMIP-II Reanalysis (Reanalysis-2)... platform: Model source: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Model institution: National Centers for Environmental Prediction dataset_title: NCEP-DOE AMIP-II Reanalysis References: https://www.esrl.noaa.gov/psd/data/gridded/data.ncep.rean... source_url: http://www.cpc.ncep.noaa.gov/products/wesley/reanalysis2/ model_id: ncar_ncep_doe_2","title":"Test II - NCEP Data"},{"location":"notebooks/climate/amip/1_regridding/#regrid_1","text":"cmip5_coords = len ( cmip5_data . lat ) + len ( cmip5_data . lon ) ncep_coords = len ( ncep_data . lat ) + len ( ncep_data . lon ) if cmip5_coords >= era5_coords : cmip5_data = regrid_data ( ncep_data , cmip5_data ) else : ncep_data = regrid_data ( cmip5_data , ncep_data ) Create weight file: nearest_s2d_73x144_96x96.nc Remove file nearest_s2d_73x144_96x96.nc ncep_data <xarray.Dataset> Dimensions: (lat: 96, lon: 96, time: 489) Coordinates: * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 84.32 86.21 88.11 90.0 Data variables: psl (time, lat, lon) float64 dask.array<chunksize=(489, 96, 96), meta=np.ndarray> pr_wtr (time, lat, lon) float64 dask.array<chunksize=(489, 96, 96), meta=np.ndarray> pres (time, lat, lon) float64 dask.array<chunksize=(489, 96, 96), meta=np.ndarray>","title":"ReGrid"},{"location":"notebooks/climate/amip/2_time_overlap/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Comparing Two Climate Models \u00b6 In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures. Data - Climate Models \u00b6 import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.amip import DataDownloader from src.data.climate.amip import DataLoader from src.features.climate.build_features import get_time_overlap # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) from esdc.standardize import normalize_temporal from esdc.transform import regrid_data from esdc.utils import check_time_coords import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload amip_data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" era5_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/era5/\" ncep_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/ncep/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\" Dataset - CMIP5 \u00b6 loader = DataLoader () dataset = 'ipsl_cm5b_lr' cmip5_data = loader . load_amip_data ( dataset ) cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 96, lon: 96, time: 360) Coordinates: * time (time) object 1979-01-16 12:00:00 ... 2008-12-16 12:00:00 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object dask.array<chunksize=(360, 2), meta=np.ndarray> lat_bnds (lat, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> lon_bnds (lon, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> psl (time, lat, lon) float32 dask.array<chunksize=(360, 96, 96), meta=np.ndarray> Attributes: institution: IPSL (Institut Pierre Simon Laplace, Paris, France) institute_id: IPSL experiment_id: amip source: IPSL-CM5B-LR (2011) : atmos : LMDZ5 (LMDZ5_NPv3.1... model_id: IPSL-CM5B-LR forcing: Nat,Ant,GHG,SA,Oz,LU,SS,Ds,BC,MD,OC,AA parent_experiment_id: N/A parent_experiment_rip: N/A branch_time: 0.0 contact: ipsl-cmip5 _at_ ipsl.jussieu.fr Data manager : Se... comment: This atmospheric only simulation include natural ... references: Model documentation and further reference availab... initialization_method: 1 physics_version: 1 tracking_id: b06de09a-a87a-4f4d-bb64-63ad47eab808 product: output experiment: AMIP frequency: mon creation_date: 2012-06-11T18:49:23Z history: 2012-06-11T18:49:23Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: IPSL-CM5B-LR model output prepared for CMIP5 AMIP parent_experiment: N/A modeling_realm: atmos realization: 1 cmor_version: 2.7.1 ERA5 \u00b6 era5_data = xr . open_mfdataset ( f \" { era5_path } *.nc\" , combine = \"by_coords\" ) era5_data = era5_data . rename ({ 'msl' : 'mslp' , 'latitude' : 'lat' , 'longitude' : 'lon' }) # era5_data = era5_data.rename({'latitude': 'lat'}) # era5_data.attrs['model_id'] = 'era5' # rescale model from 0.25 to 2.5 degrees # era5_data = era5_data.coarsen(lat=10, lon=10, boundary='pad').mean() era5_data . attrs [ 'model_id' ] = 'era5' era5_data = era5_data . rename ({ 'mslp' : 'psl' }) era5_data <xarray.Dataset> Dimensions: (lat: 721, lon: 1440, time: 487) Coordinates: * lon (lon) float32 0.0 0.25 0.5 0.75 1.0 ... 359.0 359.25 359.5 359.75 * lat (lat) float32 90.0 89.75 89.5 89.25 ... -89.25 -89.5 -89.75 -90.0 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-07-01 Data variables: psl (time, lat, lon) float32 dask.array<chunksize=(487, 721, 1440), meta=np.ndarray> sp (time, lat, lon) float32 dask.array<chunksize=(487, 721, 1440), meta=np.ndarray> Attributes: Conventions: CF-1.6 history: 2019-10-07 09:20:10 GMT by grib_to_netcdf-2.10.0: /opt/ecmw... model_id: era5 Time Coords Overlap \u00b6 era5_data , cmip5_data = get_time_overlap ( era5_data , cmip5_data ) era5_data <xarray.Dataset> Dimensions: (lat: 721, lon: 1440, time: 359) Coordinates: * lon (lon) float32 0.0 0.25 0.5 0.75 1.0 ... 359.0 359.25 359.5 359.75 * lat (lat) float32 90.0 89.75 89.5 89.25 ... -89.25 -89.5 -89.75 -90.0 * time (time) datetime64[ns] 1979-02-01 1979-03-01 ... 2008-12-01 Data variables: psl (time, lat, lon) float32 dask.array<chunksize=(359, 721, 1440), meta=np.ndarray> sp (time, lat, lon) float32 dask.array<chunksize=(359, 721, 1440), meta=np.ndarray> Attributes: Conventions: CF-1.6 history: 2019-10-07 09:20:10 GMT by grib_to_netcdf-2.10.0: /opt/ecmw... model_id: era5 cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 96, lon: 96, time: 360) Coordinates: * time (time) datetime64[ns] 1979-01-16T12:00:00 ... 2008-12-16T12:00:00 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object dask.array<chunksize=(360, 2), meta=np.ndarray> lat_bnds (lat, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> lon_bnds (lon, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> psl (time, lat, lon) float32 dask.array<chunksize=(360, 96, 96), meta=np.ndarray> Attributes: institution: IPSL (Institut Pierre Simon Laplace, Paris, France) institute_id: IPSL experiment_id: amip source: IPSL-CM5B-LR (2011) : atmos : LMDZ5 (LMDZ5_NPv3.1... model_id: IPSL-CM5B-LR forcing: Nat,Ant,GHG,SA,Oz,LU,SS,Ds,BC,MD,OC,AA parent_experiment_id: N/A parent_experiment_rip: N/A branch_time: 0.0 contact: ipsl-cmip5 _at_ ipsl.jussieu.fr Data manager : Se... comment: This atmospheric only simulation include natural ... references: Model documentation and further reference availab... initialization_method: 1 physics_version: 1 tracking_id: b06de09a-a87a-4f4d-bb64-63ad47eab808 product: output experiment: AMIP frequency: mon creation_date: 2012-06-11T18:49:23Z history: 2012-06-11T18:49:23Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: IPSL-CM5B-LR model output prepared for CMIP5 AMIP parent_experiment: N/A modeling_realm: atmos realization: 1 cmor_version: 2.7.1 NCEP \u00b6 ncep_data = xr . open_mfdataset ( f \" { ncep_path } *mon.mean.nc\" , combine = \"by_coords\" ) ncep_data = ncep_data . rename ({ 'mslp' : 'psl' }) ncep_data . attrs [ 'model_id' ] = 'ncar_ncep_doe_2' ncep_data <xarray.Dataset> Dimensions: (lat: 73, lon: 144, nbnds: 2, time: 489) Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Dimensions without coordinates: nbnds Data variables: psl (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> time_bnds (time, nbnds) datetime64[ns] dask.array<chunksize=(489, 2), meta=np.ndarray> pr_wtr (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> pres (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> Attributes: Conventions: CF-1.0 title: Monthly NCEP/DOE Reanalysis 2 history: created 2002/03 by Hoop (netCDF2.3) comments: Data is from \\nNCEP/DOE AMIP-II Reanalysis (Reanalysis-2)... platform: Model source: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Model institution: National Centers for Environmental Prediction dataset_title: NCEP-DOE AMIP-II Reanalysis References: https://www.esrl.noaa.gov/psd/data/gridded/data.ncep.rean... source_url: http://www.cpc.ncep.noaa.gov/products/wesley/reanalysis2/ model_id: ncar_ncep_doe_2 CMIP5 \u00b6 loader = DataLoader () dataset = 'ipsl_cm5b_lr' cmip5_data = loader . load_amip_data ( dataset ) cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 96, lon: 96, time: 360) Coordinates: * time (time) object 1979-01-16 12:00:00 ... 2008-12-16 12:00:00 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object dask.array<chunksize=(360, 2), meta=np.ndarray> lat_bnds (lat, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> lon_bnds (lon, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> psl (time, lat, lon) float32 dask.array<chunksize=(360, 96, 96), meta=np.ndarray> Attributes: institution: IPSL (Institut Pierre Simon Laplace, Paris, France) institute_id: IPSL experiment_id: amip source: IPSL-CM5B-LR (2011) : atmos : LMDZ5 (LMDZ5_NPv3.1... model_id: IPSL-CM5B-LR forcing: Nat,Ant,GHG,SA,Oz,LU,SS,Ds,BC,MD,OC,AA parent_experiment_id: N/A parent_experiment_rip: N/A branch_time: 0.0 contact: ipsl-cmip5 _at_ ipsl.jussieu.fr Data manager : Se... comment: This atmospheric only simulation include natural ... references: Model documentation and further reference availab... initialization_method: 1 physics_version: 1 tracking_id: b06de09a-a87a-4f4d-bb64-63ad47eab808 product: output experiment: AMIP frequency: mon creation_date: 2012-06-11T18:49:23Z history: 2012-06-11T18:49:23Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: IPSL-CM5B-LR model output prepared for CMIP5 AMIP parent_experiment: N/A modeling_realm: atmos realization: 1 cmor_version: 2.7.1 ncep_data , cmip5_data = get_time_overlap ( ncep_data , cmip5_data ) ncep_data <xarray.Dataset> Dimensions: (lat: 73, lon: 144, nbnds: 2, time: 359) Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-02-01 1979-03-01 ... 2008-12-01 Dimensions without coordinates: nbnds Data variables: psl (time, lat, lon) float32 dask.array<chunksize=(359, 73, 144), meta=np.ndarray> time_bnds (time, nbnds) datetime64[ns] dask.array<chunksize=(359, 2), meta=np.ndarray> pr_wtr (time, lat, lon) float32 dask.array<chunksize=(359, 73, 144), meta=np.ndarray> pres (time, lat, lon) float32 dask.array<chunksize=(359, 73, 144), meta=np.ndarray> Attributes: Conventions: CF-1.0 title: Monthly NCEP/DOE Reanalysis 2 history: created 2002/03 by Hoop (netCDF2.3) comments: Data is from \\nNCEP/DOE AMIP-II Reanalysis (Reanalysis-2)... platform: Model source: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Model institution: National Centers for Environmental Prediction dataset_title: NCEP-DOE AMIP-II Reanalysis References: https://www.esrl.noaa.gov/psd/data/gridded/data.ncep.rean... source_url: http://www.cpc.ncep.noaa.gov/products/wesley/reanalysis2/ model_id: ncar_ncep_doe_2 cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 96, lon: 96, time: 360) Coordinates: * time (time) datetime64[ns] 1979-01-16T12:00:00 ... 2008-12-16T12:00:00 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object dask.array<chunksize=(360, 2), meta=np.ndarray> lat_bnds (lat, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> lon_bnds (lon, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> psl (time, lat, lon) float32 dask.array<chunksize=(360, 96, 96), meta=np.ndarray> Attributes: institution: IPSL (Institut Pierre Simon Laplace, Paris, France) institute_id: IPSL experiment_id: amip source: IPSL-CM5B-LR (2011) : atmos : LMDZ5 (LMDZ5_NPv3.1... model_id: IPSL-CM5B-LR forcing: Nat,Ant,GHG,SA,Oz,LU,SS,Ds,BC,MD,OC,AA parent_experiment_id: N/A parent_experiment_rip: N/A branch_time: 0.0 contact: ipsl-cmip5 _at_ ipsl.jussieu.fr Data manager : Se... comment: This atmospheric only simulation include natural ... references: Model documentation and further reference availab... initialization_method: 1 physics_version: 1 tracking_id: b06de09a-a87a-4f4d-bb64-63ad47eab808 product: output experiment: AMIP frequency: mon creation_date: 2012-06-11T18:49:23Z history: 2012-06-11T18:49:23Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: IPSL-CM5B-LR model output prepared for CMIP5 AMIP parent_experiment: N/A modeling_realm: atmos realization: 1 cmor_version: 2.7.1","title":"2 time overlap"},{"location":"notebooks/climate/amip/2_time_overlap/#comparing-two-climate-models","text":"In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures.","title":"Comparing Two Climate Models"},{"location":"notebooks/climate/amip/2_time_overlap/#data-climate-models","text":"import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.amip import DataDownloader from src.data.climate.amip import DataLoader from src.features.climate.build_features import get_time_overlap # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) from esdc.standardize import normalize_temporal from esdc.transform import regrid_data from esdc.utils import check_time_coords import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload amip_data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" era5_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/era5/\" ncep_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/ncep/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\"","title":"Data - Climate Models"},{"location":"notebooks/climate/amip/2_time_overlap/#dataset-cmip5","text":"loader = DataLoader () dataset = 'ipsl_cm5b_lr' cmip5_data = loader . load_amip_data ( dataset ) cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 96, lon: 96, time: 360) Coordinates: * time (time) object 1979-01-16 12:00:00 ... 2008-12-16 12:00:00 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object dask.array<chunksize=(360, 2), meta=np.ndarray> lat_bnds (lat, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> lon_bnds (lon, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> psl (time, lat, lon) float32 dask.array<chunksize=(360, 96, 96), meta=np.ndarray> Attributes: institution: IPSL (Institut Pierre Simon Laplace, Paris, France) institute_id: IPSL experiment_id: amip source: IPSL-CM5B-LR (2011) : atmos : LMDZ5 (LMDZ5_NPv3.1... model_id: IPSL-CM5B-LR forcing: Nat,Ant,GHG,SA,Oz,LU,SS,Ds,BC,MD,OC,AA parent_experiment_id: N/A parent_experiment_rip: N/A branch_time: 0.0 contact: ipsl-cmip5 _at_ ipsl.jussieu.fr Data manager : Se... comment: This atmospheric only simulation include natural ... references: Model documentation and further reference availab... initialization_method: 1 physics_version: 1 tracking_id: b06de09a-a87a-4f4d-bb64-63ad47eab808 product: output experiment: AMIP frequency: mon creation_date: 2012-06-11T18:49:23Z history: 2012-06-11T18:49:23Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: IPSL-CM5B-LR model output prepared for CMIP5 AMIP parent_experiment: N/A modeling_realm: atmos realization: 1 cmor_version: 2.7.1","title":"Dataset - CMIP5"},{"location":"notebooks/climate/amip/2_time_overlap/#era5","text":"era5_data = xr . open_mfdataset ( f \" { era5_path } *.nc\" , combine = \"by_coords\" ) era5_data = era5_data . rename ({ 'msl' : 'mslp' , 'latitude' : 'lat' , 'longitude' : 'lon' }) # era5_data = era5_data.rename({'latitude': 'lat'}) # era5_data.attrs['model_id'] = 'era5' # rescale model from 0.25 to 2.5 degrees # era5_data = era5_data.coarsen(lat=10, lon=10, boundary='pad').mean() era5_data . attrs [ 'model_id' ] = 'era5' era5_data = era5_data . rename ({ 'mslp' : 'psl' }) era5_data <xarray.Dataset> Dimensions: (lat: 721, lon: 1440, time: 487) Coordinates: * lon (lon) float32 0.0 0.25 0.5 0.75 1.0 ... 359.0 359.25 359.5 359.75 * lat (lat) float32 90.0 89.75 89.5 89.25 ... -89.25 -89.5 -89.75 -90.0 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-07-01 Data variables: psl (time, lat, lon) float32 dask.array<chunksize=(487, 721, 1440), meta=np.ndarray> sp (time, lat, lon) float32 dask.array<chunksize=(487, 721, 1440), meta=np.ndarray> Attributes: Conventions: CF-1.6 history: 2019-10-07 09:20:10 GMT by grib_to_netcdf-2.10.0: /opt/ecmw... model_id: era5","title":"ERA5"},{"location":"notebooks/climate/amip/2_time_overlap/#time-coords-overlap","text":"era5_data , cmip5_data = get_time_overlap ( era5_data , cmip5_data ) era5_data <xarray.Dataset> Dimensions: (lat: 721, lon: 1440, time: 359) Coordinates: * lon (lon) float32 0.0 0.25 0.5 0.75 1.0 ... 359.0 359.25 359.5 359.75 * lat (lat) float32 90.0 89.75 89.5 89.25 ... -89.25 -89.5 -89.75 -90.0 * time (time) datetime64[ns] 1979-02-01 1979-03-01 ... 2008-12-01 Data variables: psl (time, lat, lon) float32 dask.array<chunksize=(359, 721, 1440), meta=np.ndarray> sp (time, lat, lon) float32 dask.array<chunksize=(359, 721, 1440), meta=np.ndarray> Attributes: Conventions: CF-1.6 history: 2019-10-07 09:20:10 GMT by grib_to_netcdf-2.10.0: /opt/ecmw... model_id: era5 cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 96, lon: 96, time: 360) Coordinates: * time (time) datetime64[ns] 1979-01-16T12:00:00 ... 2008-12-16T12:00:00 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object dask.array<chunksize=(360, 2), meta=np.ndarray> lat_bnds (lat, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> lon_bnds (lon, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> psl (time, lat, lon) float32 dask.array<chunksize=(360, 96, 96), meta=np.ndarray> Attributes: institution: IPSL (Institut Pierre Simon Laplace, Paris, France) institute_id: IPSL experiment_id: amip source: IPSL-CM5B-LR (2011) : atmos : LMDZ5 (LMDZ5_NPv3.1... model_id: IPSL-CM5B-LR forcing: Nat,Ant,GHG,SA,Oz,LU,SS,Ds,BC,MD,OC,AA parent_experiment_id: N/A parent_experiment_rip: N/A branch_time: 0.0 contact: ipsl-cmip5 _at_ ipsl.jussieu.fr Data manager : Se... comment: This atmospheric only simulation include natural ... references: Model documentation and further reference availab... initialization_method: 1 physics_version: 1 tracking_id: b06de09a-a87a-4f4d-bb64-63ad47eab808 product: output experiment: AMIP frequency: mon creation_date: 2012-06-11T18:49:23Z history: 2012-06-11T18:49:23Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: IPSL-CM5B-LR model output prepared for CMIP5 AMIP parent_experiment: N/A modeling_realm: atmos realization: 1 cmor_version: 2.7.1","title":"Time Coords Overlap"},{"location":"notebooks/climate/amip/2_time_overlap/#ncep","text":"ncep_data = xr . open_mfdataset ( f \" { ncep_path } *mon.mean.nc\" , combine = \"by_coords\" ) ncep_data = ncep_data . rename ({ 'mslp' : 'psl' }) ncep_data . attrs [ 'model_id' ] = 'ncar_ncep_doe_2' ncep_data <xarray.Dataset> Dimensions: (lat: 73, lon: 144, nbnds: 2, time: 489) Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Dimensions without coordinates: nbnds Data variables: psl (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> time_bnds (time, nbnds) datetime64[ns] dask.array<chunksize=(489, 2), meta=np.ndarray> pr_wtr (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> pres (time, lat, lon) float32 dask.array<chunksize=(489, 73, 144), meta=np.ndarray> Attributes: Conventions: CF-1.0 title: Monthly NCEP/DOE Reanalysis 2 history: created 2002/03 by Hoop (netCDF2.3) comments: Data is from \\nNCEP/DOE AMIP-II Reanalysis (Reanalysis-2)... platform: Model source: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Model institution: National Centers for Environmental Prediction dataset_title: NCEP-DOE AMIP-II Reanalysis References: https://www.esrl.noaa.gov/psd/data/gridded/data.ncep.rean... source_url: http://www.cpc.ncep.noaa.gov/products/wesley/reanalysis2/ model_id: ncar_ncep_doe_2","title":"NCEP"},{"location":"notebooks/climate/amip/2_time_overlap/#cmip5","text":"loader = DataLoader () dataset = 'ipsl_cm5b_lr' cmip5_data = loader . load_amip_data ( dataset ) cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 96, lon: 96, time: 360) Coordinates: * time (time) object 1979-01-16 12:00:00 ... 2008-12-16 12:00:00 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object dask.array<chunksize=(360, 2), meta=np.ndarray> lat_bnds (lat, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> lon_bnds (lon, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> psl (time, lat, lon) float32 dask.array<chunksize=(360, 96, 96), meta=np.ndarray> Attributes: institution: IPSL (Institut Pierre Simon Laplace, Paris, France) institute_id: IPSL experiment_id: amip source: IPSL-CM5B-LR (2011) : atmos : LMDZ5 (LMDZ5_NPv3.1... model_id: IPSL-CM5B-LR forcing: Nat,Ant,GHG,SA,Oz,LU,SS,Ds,BC,MD,OC,AA parent_experiment_id: N/A parent_experiment_rip: N/A branch_time: 0.0 contact: ipsl-cmip5 _at_ ipsl.jussieu.fr Data manager : Se... comment: This atmospheric only simulation include natural ... references: Model documentation and further reference availab... initialization_method: 1 physics_version: 1 tracking_id: b06de09a-a87a-4f4d-bb64-63ad47eab808 product: output experiment: AMIP frequency: mon creation_date: 2012-06-11T18:49:23Z history: 2012-06-11T18:49:23Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: IPSL-CM5B-LR model output prepared for CMIP5 AMIP parent_experiment: N/A modeling_realm: atmos realization: 1 cmor_version: 2.7.1 ncep_data , cmip5_data = get_time_overlap ( ncep_data , cmip5_data ) ncep_data <xarray.Dataset> Dimensions: (lat: 73, lon: 144, nbnds: 2, time: 359) Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-02-01 1979-03-01 ... 2008-12-01 Dimensions without coordinates: nbnds Data variables: psl (time, lat, lon) float32 dask.array<chunksize=(359, 73, 144), meta=np.ndarray> time_bnds (time, nbnds) datetime64[ns] dask.array<chunksize=(359, 2), meta=np.ndarray> pr_wtr (time, lat, lon) float32 dask.array<chunksize=(359, 73, 144), meta=np.ndarray> pres (time, lat, lon) float32 dask.array<chunksize=(359, 73, 144), meta=np.ndarray> Attributes: Conventions: CF-1.0 title: Monthly NCEP/DOE Reanalysis 2 history: created 2002/03 by Hoop (netCDF2.3) comments: Data is from \\nNCEP/DOE AMIP-II Reanalysis (Reanalysis-2)... platform: Model source: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Model institution: National Centers for Environmental Prediction dataset_title: NCEP-DOE AMIP-II Reanalysis References: https://www.esrl.noaa.gov/psd/data/gridded/data.ncep.rean... source_url: http://www.cpc.ncep.noaa.gov/products/wesley/reanalysis2/ model_id: ncar_ncep_doe_2 cmip5_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 96, lon: 96, time: 360) Coordinates: * time (time) datetime64[ns] 1979-01-16T12:00:00 ... 2008-12-16T12:00:00 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object dask.array<chunksize=(360, 2), meta=np.ndarray> lat_bnds (lat, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> lon_bnds (lon, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> psl (time, lat, lon) float32 dask.array<chunksize=(360, 96, 96), meta=np.ndarray> Attributes: institution: IPSL (Institut Pierre Simon Laplace, Paris, France) institute_id: IPSL experiment_id: amip source: IPSL-CM5B-LR (2011) : atmos : LMDZ5 (LMDZ5_NPv3.1... model_id: IPSL-CM5B-LR forcing: Nat,Ant,GHG,SA,Oz,LU,SS,Ds,BC,MD,OC,AA parent_experiment_id: N/A parent_experiment_rip: N/A branch_time: 0.0 contact: ipsl-cmip5 _at_ ipsl.jussieu.fr Data manager : Se... comment: This atmospheric only simulation include natural ... references: Model documentation and further reference availab... initialization_method: 1 physics_version: 1 tracking_id: b06de09a-a87a-4f4d-bb64-63ad47eab808 product: output experiment: AMIP frequency: mon creation_date: 2012-06-11T18:49:23Z history: 2012-06-11T18:49:23Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: IPSL-CM5B-LR model output prepared for CMIP5 AMIP parent_experiment: N/A modeling_realm: atmos realization: 1 cmor_version: 2.7.1","title":"CMIP5"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Comparing Two Climate Models \u00b6 In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures. Data - Climate Models \u00b6 import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.amip import DataDownloader , DataLoader from src.data.climate.era5 import get_era5_data from src.data.climate.ncep import get_ncep_data from src.features.climate.build_features import ( get_time_overlap , check_time_coords , regrid_2_lower_res , get_spatial_cubes , normalize_data ) from src.experiments.climate.amip_global import ( experiment_loop_comparative , experiment_loop_individual ) # Stat Tools from src.models.train_models import run_rbig_models from scipy import stats import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload amip_data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" era5_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/era5/\" ncep_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/ncep/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\" Demo Experiment \u00b6 Experimental Paams \u00b6 class DataArgs : data_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" results_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip\" class CMIPArgs : # Fixed Params spatial_windows = [ 1 , 2 , # Spatial Window for Density Cubes 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] # Free Params variables = [ 'psl' # Mean Surface Pressure ] cmip_models = [ \"inmcm4\" , \"access1_0\" , \"bcc_csm1_1\" , \"bcc_csm1_1_m\" , \"bnu_esm\" , \"giss_e2_r\" , \"cnrm_cm5\" , \"ipsl_cm5a_lr\" , \"ipsl_cm5a_mr\" , \"ipsl_cm5b_lr\" , \"mpi_esm_lr\" , \"mpi_esm_mr\" , \"noresm1_m\" , ] base_models = [ 'ncep' , \"era5\" ] def run_exp (): for ibase in CMIPArgs . base_models : print ( 'Base Model:' , ibase ) for ivariable in CMIPArgs . variables : print ( 'Variable:' , ivariable ) for icmip in CMIPArgs . cmip_models : print ( \"CMIP Model:\" , icmip ) run_exp () Base Model: ncep Variable: psl CMIP Model: inmcm4 CMIP Model: access1_0 CMIP Model: bcc_csm1_1 CMIP Model: bcc_csm1_1_m CMIP Model: bnu_esm CMIP Model: giss_e2_r CMIP Model: cnrm_cm5 CMIP Model: ipsl_cm5a_lr CMIP Model: ipsl_cm5a_mr CMIP Model: ipsl_cm5b_lr CMIP Model: mpi_esm_lr CMIP Model: mpi_esm_mr CMIP Model: noresm1_m Base Model: era5 Variable: psl CMIP Model: inmcm4 CMIP Model: access1_0 CMIP Model: bcc_csm1_1 CMIP Model: bcc_csm1_1_m CMIP Model: bnu_esm CMIP Model: giss_e2_r CMIP Model: cnrm_cm5 CMIP Model: ipsl_cm5a_lr CMIP Model: ipsl_cm5a_mr CMIP Model: ipsl_cm5b_lr CMIP Model: mpi_esm_lr CMIP Model: mpi_esm_mr CMIP Model: noresm1_m Part I - Grab Data \u00b6 from src.data.climate.amip import get_base_model base_dat = get_base_model ( CMIPArgs . base_models [ 0 ], CMIPArgs . variables [ 0 ]) base_dat <xarray.DataArray 'psl' (time: 489, lat: 73, lon: 144)> dask.array<open_dataset-0ace0936f02ce97f3d79321b6d5f6a55mslp, shape=(489, 73, 144), dtype=float32, chunksize=(489, 73, 144), chunktype=numpy.ndarray> Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Attributes: long_name: Monthly Mean Sea Level Pressure valid_range: [-32766 15234] unpacked_valid_range: [ 77000. 125000.] actual_range: [ 95644. 105703.] units: Pascals precision: 0 GRIB_id: 2 GRIB_name: PRMSL var_desc: Mean Sea Level Pressure dataset: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Monthly... level_desc: Sea Level statistic: Individual Obs parent_stat: Other standard_name: pressure cell_methods: time: mean (monthly from 6-hourly values) model_id: ncep from src.data.climate.cmip5 import get_cmip5_model cmip_dat = get_cmip5_model ( CMIPArgs . cmip_models [ 0 ], CMIPArgs . variables [ 0 ]) cmip_dat <xarray.DataArray 'psl' (time: 360, lat: 120, lon: 180)> dask.array<open_dataset-0a95b28dbc539d100c816bea9e2db9e7psl, shape=(360, 120, 180), dtype=float32, chunksize=(360, 120, 180), chunktype=numpy.ndarray> Coordinates: * time (time) object 1979-01-16 12:00:00 ... 2008-12-16 12:00:00 * lat (lat) float64 -89.25 -87.75 -86.25 -84.75 ... 86.25 87.75 89.25 * lon (lon) float64 0.0 2.0 4.0 6.0 8.0 ... 350.0 352.0 354.0 356.0 358.0 Attributes: standard_name: air_pressure_at_sea_level long_name: Sea Level Pressure comment: not, in general, the same as surface pressure units: Pa original_name: psl cell_methods: time: mean (interval: 1 month) history: 2011-02-07T11:53:19Z altered by CMOR: Reordered dimens... associated_files: baseURL: http://cmip-pcmdi.llnl.gov/CMIP5/dataLocation... model_id: inmcm4 def run_exp (): for ibase in CMIPArgs . base_models : for ivariable in CMIPArgs . variables : for icmip in CMIPArgs . cmip_models : print ( ibase ) print ( ivariable ) print ( icmip ) Part II - Regrid Data \u00b6 base_dat , cmip_dat = regrid_2_lower_res ( base_dat , cmip_dat ) assert ( base_dat . shape [ 1 ] == cmip_dat . shape [ 1 ]) assert ( base_dat . shape [ 2 ] == cmip_dat . shape [ 2 ]) base_dat Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc <xarray.DataArray 'psl' (time: 489, lat: 73, lon: 144)> dask.array<open_dataset-0ace0936f02ce97f3d79321b6d5f6a55mslp, shape=(489, 73, 144), dtype=float32, chunksize=(489, 73, 144), chunktype=numpy.ndarray> Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Attributes: long_name: Monthly Mean Sea Level Pressure valid_range: [-32766 15234] unpacked_valid_range: [ 77000. 125000.] actual_range: [ 95644. 105703.] units: Pascals precision: 0 GRIB_id: 2 GRIB_name: PRMSL var_desc: Mean Sea Level Pressure dataset: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Monthly... level_desc: Sea Level statistic: Individual Obs parent_stat: Other standard_name: pressure cell_methods: time: mean (monthly from 6-hourly values) model_id: ncep Part III - Find Overlapping Times \u00b6 base_dat . shape , cmip_dat . shape ((489, 73, 144), (360, 73, 144)) base_dat , cmip_dat = get_time_overlap ( base_dat , cmip_dat ) Part IV - Get Density Cubes \u00b6 base_df = get_spatial_cubes ( base_dat , CMIPArgs . spatial_windows [ 3 ]) cmip_df = get_spatial_cubes ( cmip_dat , CMIPArgs . spatial_windows [ 3 ]) base_df . shape (4826430, 16) Test Individual Loop \u00b6 test_base_model = 'ncep' test_cmip_model = 'inmcm4' test_variable = 'psl' test_spatial_window = 7 subsamples = 1_000 res = experiment_loop_individual ( test_base_model , test_cmip_model , test_variable , test_spatial_window , subsamples ) Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc res {'h_base': -149.66922579994676, 'tc_base': 184.67797240499547, 'h_cmip': -112.08752658960748, 'tc_cmip': 124.29975263830998, 't_base': 12.304942846298218, 't_cmip': 13.265496253967285} Test Comparative Loop \u00b6 test_base_model = 'ncep' test_cmip_model = 'inmcm4' test_variable = 'psl' test_spatial_window = 7 subsamples = 1_000 res = experiment_loop_comparative ( test_base_model , test_cmip_model , test_variable , test_spatial_window , subsamples ) Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc (3319314, 49) (3328560, 49) res {'mi': 12.93477259679793, 'time_mi': 116.64088153839111, 'pearson': 0.6832584799235359, 'spearman': 0.6455462181832066, 'kendelltau': 0.45468733761829255} Experimental Loop \u00b6 Part IV - Groupby time stamp \u00b6 time_stamps = min ( len ( base_dat . time ), len ( cmip_dat . time )) with tqdm ( range ( time_stamps )) as progress_bar : for itime in progress_bar : ibase_dat = base_dat . isel ( time = itime , drop = False ) icmip_dat = cmip_dat . isel ( time = itime ) print ( ibase_dat ) print ( icmip_dat ) break 0%| | 0/359 [00:00<?, ?it/s] <xarray.DataArray 'psl' (lat: 73, lon: 144)> dask.array<getitem, shape=(73, 144), dtype=float32, chunksize=(73, 144), chunktype=numpy.ndarray> Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 time datetime64[ns] 1979-02-01 Attributes: long_name: Monthly Mean Sea Level Pressure valid_range: [-32766 15234] unpacked_valid_range: [ 77000. 125000.] actual_range: [ 95644. 105703.] units: Pascals precision: 0 GRIB_id: 2 GRIB_name: PRMSL var_desc: Mean Sea Level Pressure dataset: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Monthly... level_desc: Sea Level statistic: Individual Obs parent_stat: Other standard_name: pressure cell_methods: time: mean (monthly from 6-hourly values) model_id: ncep <xarray.DataArray 'psl' (lat: 73, lon: 144)> dask.array<getitem, shape=(73, 144), dtype=float64, chunksize=(73, 144), chunktype=numpy.ndarray> Coordinates: time datetime64[ns] 1979-01-16T12:00:00 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 Attributes: regrid_method: nearest_s2d model_id: inmcm4 Part IV - Get Density Cubes \u00b6 Part V - Normalize \u00b6 base_norm = normalize_data ( base_df ) cmip_norm = normalize_data ( cmip_df ) base_norm [: None ] . shape (4826430, 16) Part VI - RBIG Algorithm \u00b6 Entropy, Total Correlation \u00b6 base_tc , base_h , t1 = run_rbig_models ( base_norm [: 1_000 ], measure = 't' , verbose = None ) cmip_tc , cmip_h , t2 = run_rbig_models ( cmip_norm [: 1_000 ], measure = 't' , verbose = None ) base_tc / 16 , cmip_tc / 16 (2.8645279873342417, 2.6572342136429885) base_h / 16 , cmip_h / 16 (-2.7045988055470094, -2.0434281745463228) type ( t1 ) float Mutual Information \u00b6 mi , t_ = run_rbig_models ( base_norm [: 1_000 ], cmip_norm [: 1_000 ], measure = 'mi' , verbose = None ) mi 0.9339957437091543 from scipy import stats pears = stats . pearsonr ( base_norm [: 1_000 ] . ravel (), cmip_norm [: 1_000 ] . ravel ()) spears = stats . spearmanr ( base_norm [: 1_000 ] . ravel (), cmip_norm [: 1_000 ] . ravel ()) kend = stats . kendalltau ( base_norm [: 1_000 ] . ravel (), cmip_norm [: 1_000 ] . ravel ()) pears [ 0 ], spears [ 0 ], kend [ 0 ] (0.19924053221759874, 0.18606468822908886, 0.1297966352157317) from Experiment I - Comparing Climate Models \u00b6 Mean Sea Level Pressure \u00b6 ERA5 vs NCEP \u00b6 # Experiment class class ClimateEntropy : def __init__ ( self , save_path : None , variable : str = 'mslp' , save_name = None , mi : bool = True ): self . variable = variable self . results_path = save_path self . results_df = pd . DataFrame () self . save_name = save_name self . mi = mi def run_experiment ( self , climate_model1 : pd . DataFrame , climate_model2 : pd . DataFrame ) -> None : \"\"\"Training loop that goes through each year and calculates the entropy, total correlation and mutual information between the two models.\"\"\" time_length = len ( climate_model1 . groupby ( 'time.year' )) # Normalize BEFORE the individual calculations climate_model1 [ self . variable ] = normalize_temporal ( climate_model1 [ self . variable ]) model1_id = climate_model1 . attrs [ 'model_id' ] model2_id = climate_model2 . attrs [ 'model_id' ] climate_model2 [ self . variable ] = normalize_temporal ( climate_model2 [ self . variable ]) with tqdm ( zip ( climate_model1 . groupby ( 'time.year' ), climate_model2 . groupby ( 'time.year' ) ), total = time_length ) as progress_bar : for imodel1 , imodel2 in progress_bar : # Update params in progress bar # Transform to dataframe, remove spatial dimensions X1 = self . _get_time_features ( imodel1 [ 1 ][ self . variable ]) X2 = self . _get_time_features ( imodel2 [ 1 ][ self . variable ]) # Normalize inputs min_max_scaler = preprocessing . StandardScaler () X1 = min_max_scaler . fit_transform ( X1 . values ) X2 = min_max_scaler . fit_transform ( X2 . values ) dims = X1 . shape [ 1 ] # ============================= # Calculate Mutual Information # ============================= if self . mi == False : mi_ = None mi_t_ = None else : mi_ , mi_t_ = run_rbig_models ( X1 , X2 , measure = 'mi' , verbose = None ) # Update params in progress bar postfix = dict ( ) # ======================================== # Calculate Entropy and Total Correlation # ======================================== # Model I tc1_ , h1_ , h_t1_ = run_rbig_models ( X1 , measure = 't' , verbose = None ) self . _update_results ( model = model1_id , year = imodel1 [ 0 ], h_time = h_t1_ , tc = tc1_ , h = h1_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Model II tc2_ , h2_ , h_t2_ = run_rbig_models ( X2 , measure = 't' , verbose = None ) self . _update_results ( model = model2_id , year = imodel2 [ 0 ], h_time = h_t2_ , tc = tc2_ , h = h2_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Update params in progress bar postfix = dict ( year = imodel1 [ 0 ], mi = f \" { mi_ : .3f } \" if self . mi is True else None , h1 = f \" { h1_ : .3f } \" , tc1 = f \" { tc1_ : .3f } \" , h2 = f \" { h2_ : .3f } \" , tc2 = f \" { tc2_ : .3f } \" , ) progress_bar . set_postfix ( postfix ) return None def _get_time_features ( self , data_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"This function collapses the spatial dimensions as pivots. This allows us to only consider time as the input feature.\"\"\" return data_df . to_dataframe () . unstack ( level = 0 ) . reset_index () . drop ( columns = [ 'lat' , 'lon' ]) . dropna () def _update_results ( self , model , year , tc , h , h_time , mi , mi_time , dims ): \"\"\"appends new values to the results dataframe.\"\"\" self . results_df = self . results_df . append ({ 'model' : model , 'year' : year , 'tc' : tc , 'h' : h , 'h_time' : h_time , 'mi' : mi , 'mi_time' : mi_time , 'dims' : dims , }, ignore_index = True ) if self . results_path is not None : self . _save_results () return self def _save_results ( self ): \"\"\"Saves the dataframe to the assigned results path.\"\"\" self . results_df . to_csv ( f \" { self . results_path }{ self . variable } _ { self . save_name } .csv\" ) return None # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_ncep' ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid , ncep_data ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 41/41 [1:43:03<00:00, 150.83s/it, year=2019, mi=4.766, h1=-1.313, tc1=9.551, h2=-3.049, tc2=13.532] # extract results results_df = short_decade_exp . results_df ERA5 vs CMIP5 \u00b6 2006 - 01 - 16 , 2025 - 12 - 16 , 1979 - 01 - 01 , 2019 - 07 - 01 # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:08<00:00, 150.61s/it, year=2019, mi=3.290, h1=-1.509, tc1=9.747, h2=-0.807, tc2=8.142] NCEP vs CMIP5 \u00b6 # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'ncep_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( ncep_data . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:04<00:00, 150.30s/it, year=2019, mi=3.552, h1=-1.495, tc1=9.760, h2=-0.807, tc2=8.142]","title":"3.1 demo exp global"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#comparing-two-climate-models","text":"In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures.","title":"Comparing Two Climate Models"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#data-climate-models","text":"import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.amip import DataDownloader , DataLoader from src.data.climate.era5 import get_era5_data from src.data.climate.ncep import get_ncep_data from src.features.climate.build_features import ( get_time_overlap , check_time_coords , regrid_2_lower_res , get_spatial_cubes , normalize_data ) from src.experiments.climate.amip_global import ( experiment_loop_comparative , experiment_loop_individual ) # Stat Tools from src.models.train_models import run_rbig_models from scipy import stats import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload amip_data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" era5_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/era5/\" ncep_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/ncep/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\"","title":"Data - Climate Models"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#demo-experiment","text":"","title":"Demo Experiment"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#experimental-paams","text":"class DataArgs : data_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" results_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip\" class CMIPArgs : # Fixed Params spatial_windows = [ 1 , 2 , # Spatial Window for Density Cubes 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] # Free Params variables = [ 'psl' # Mean Surface Pressure ] cmip_models = [ \"inmcm4\" , \"access1_0\" , \"bcc_csm1_1\" , \"bcc_csm1_1_m\" , \"bnu_esm\" , \"giss_e2_r\" , \"cnrm_cm5\" , \"ipsl_cm5a_lr\" , \"ipsl_cm5a_mr\" , \"ipsl_cm5b_lr\" , \"mpi_esm_lr\" , \"mpi_esm_mr\" , \"noresm1_m\" , ] base_models = [ 'ncep' , \"era5\" ] def run_exp (): for ibase in CMIPArgs . base_models : print ( 'Base Model:' , ibase ) for ivariable in CMIPArgs . variables : print ( 'Variable:' , ivariable ) for icmip in CMIPArgs . cmip_models : print ( \"CMIP Model:\" , icmip ) run_exp () Base Model: ncep Variable: psl CMIP Model: inmcm4 CMIP Model: access1_0 CMIP Model: bcc_csm1_1 CMIP Model: bcc_csm1_1_m CMIP Model: bnu_esm CMIP Model: giss_e2_r CMIP Model: cnrm_cm5 CMIP Model: ipsl_cm5a_lr CMIP Model: ipsl_cm5a_mr CMIP Model: ipsl_cm5b_lr CMIP Model: mpi_esm_lr CMIP Model: mpi_esm_mr CMIP Model: noresm1_m Base Model: era5 Variable: psl CMIP Model: inmcm4 CMIP Model: access1_0 CMIP Model: bcc_csm1_1 CMIP Model: bcc_csm1_1_m CMIP Model: bnu_esm CMIP Model: giss_e2_r CMIP Model: cnrm_cm5 CMIP Model: ipsl_cm5a_lr CMIP Model: ipsl_cm5a_mr CMIP Model: ipsl_cm5b_lr CMIP Model: mpi_esm_lr CMIP Model: mpi_esm_mr CMIP Model: noresm1_m","title":"Experimental Paams"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#part-i-grab-data","text":"from src.data.climate.amip import get_base_model base_dat = get_base_model ( CMIPArgs . base_models [ 0 ], CMIPArgs . variables [ 0 ]) base_dat <xarray.DataArray 'psl' (time: 489, lat: 73, lon: 144)> dask.array<open_dataset-0ace0936f02ce97f3d79321b6d5f6a55mslp, shape=(489, 73, 144), dtype=float32, chunksize=(489, 73, 144), chunktype=numpy.ndarray> Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Attributes: long_name: Monthly Mean Sea Level Pressure valid_range: [-32766 15234] unpacked_valid_range: [ 77000. 125000.] actual_range: [ 95644. 105703.] units: Pascals precision: 0 GRIB_id: 2 GRIB_name: PRMSL var_desc: Mean Sea Level Pressure dataset: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Monthly... level_desc: Sea Level statistic: Individual Obs parent_stat: Other standard_name: pressure cell_methods: time: mean (monthly from 6-hourly values) model_id: ncep from src.data.climate.cmip5 import get_cmip5_model cmip_dat = get_cmip5_model ( CMIPArgs . cmip_models [ 0 ], CMIPArgs . variables [ 0 ]) cmip_dat <xarray.DataArray 'psl' (time: 360, lat: 120, lon: 180)> dask.array<open_dataset-0a95b28dbc539d100c816bea9e2db9e7psl, shape=(360, 120, 180), dtype=float32, chunksize=(360, 120, 180), chunktype=numpy.ndarray> Coordinates: * time (time) object 1979-01-16 12:00:00 ... 2008-12-16 12:00:00 * lat (lat) float64 -89.25 -87.75 -86.25 -84.75 ... 86.25 87.75 89.25 * lon (lon) float64 0.0 2.0 4.0 6.0 8.0 ... 350.0 352.0 354.0 356.0 358.0 Attributes: standard_name: air_pressure_at_sea_level long_name: Sea Level Pressure comment: not, in general, the same as surface pressure units: Pa original_name: psl cell_methods: time: mean (interval: 1 month) history: 2011-02-07T11:53:19Z altered by CMOR: Reordered dimens... associated_files: baseURL: http://cmip-pcmdi.llnl.gov/CMIP5/dataLocation... model_id: inmcm4 def run_exp (): for ibase in CMIPArgs . base_models : for ivariable in CMIPArgs . variables : for icmip in CMIPArgs . cmip_models : print ( ibase ) print ( ivariable ) print ( icmip )","title":"Part I - Grab Data"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#part-ii-regrid-data","text":"base_dat , cmip_dat = regrid_2_lower_res ( base_dat , cmip_dat ) assert ( base_dat . shape [ 1 ] == cmip_dat . shape [ 1 ]) assert ( base_dat . shape [ 2 ] == cmip_dat . shape [ 2 ]) base_dat Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc <xarray.DataArray 'psl' (time: 489, lat: 73, lon: 144)> dask.array<open_dataset-0ace0936f02ce97f3d79321b6d5f6a55mslp, shape=(489, 73, 144), dtype=float32, chunksize=(489, 73, 144), chunktype=numpy.ndarray> Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Attributes: long_name: Monthly Mean Sea Level Pressure valid_range: [-32766 15234] unpacked_valid_range: [ 77000. 125000.] actual_range: [ 95644. 105703.] units: Pascals precision: 0 GRIB_id: 2 GRIB_name: PRMSL var_desc: Mean Sea Level Pressure dataset: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Monthly... level_desc: Sea Level statistic: Individual Obs parent_stat: Other standard_name: pressure cell_methods: time: mean (monthly from 6-hourly values) model_id: ncep","title":"Part II - Regrid Data"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#part-iii-find-overlapping-times","text":"base_dat . shape , cmip_dat . shape ((489, 73, 144), (360, 73, 144)) base_dat , cmip_dat = get_time_overlap ( base_dat , cmip_dat )","title":"Part III - Find Overlapping Times"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#part-iv-get-density-cubes","text":"base_df = get_spatial_cubes ( base_dat , CMIPArgs . spatial_windows [ 3 ]) cmip_df = get_spatial_cubes ( cmip_dat , CMIPArgs . spatial_windows [ 3 ]) base_df . shape (4826430, 16)","title":"Part IV - Get Density Cubes"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#test-individual-loop","text":"test_base_model = 'ncep' test_cmip_model = 'inmcm4' test_variable = 'psl' test_spatial_window = 7 subsamples = 1_000 res = experiment_loop_individual ( test_base_model , test_cmip_model , test_variable , test_spatial_window , subsamples ) Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc res {'h_base': -149.66922579994676, 'tc_base': 184.67797240499547, 'h_cmip': -112.08752658960748, 'tc_cmip': 124.29975263830998, 't_base': 12.304942846298218, 't_cmip': 13.265496253967285}","title":"Test Individual Loop"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#test-comparative-loop","text":"test_base_model = 'ncep' test_cmip_model = 'inmcm4' test_variable = 'psl' test_spatial_window = 7 subsamples = 1_000 res = experiment_loop_comparative ( test_base_model , test_cmip_model , test_variable , test_spatial_window , subsamples ) Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc (3319314, 49) (3328560, 49) res {'mi': 12.93477259679793, 'time_mi': 116.64088153839111, 'pearson': 0.6832584799235359, 'spearman': 0.6455462181832066, 'kendelltau': 0.45468733761829255}","title":"Test Comparative Loop"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#experimental-loop","text":"","title":"Experimental Loop"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#part-iv-groupby-time-stamp","text":"time_stamps = min ( len ( base_dat . time ), len ( cmip_dat . time )) with tqdm ( range ( time_stamps )) as progress_bar : for itime in progress_bar : ibase_dat = base_dat . isel ( time = itime , drop = False ) icmip_dat = cmip_dat . isel ( time = itime ) print ( ibase_dat ) print ( icmip_dat ) break 0%| | 0/359 [00:00<?, ?it/s] <xarray.DataArray 'psl' (lat: 73, lon: 144)> dask.array<getitem, shape=(73, 144), dtype=float32, chunksize=(73, 144), chunktype=numpy.ndarray> Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 time datetime64[ns] 1979-02-01 Attributes: long_name: Monthly Mean Sea Level Pressure valid_range: [-32766 15234] unpacked_valid_range: [ 77000. 125000.] actual_range: [ 95644. 105703.] units: Pascals precision: 0 GRIB_id: 2 GRIB_name: PRMSL var_desc: Mean Sea Level Pressure dataset: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Monthly... level_desc: Sea Level statistic: Individual Obs parent_stat: Other standard_name: pressure cell_methods: time: mean (monthly from 6-hourly values) model_id: ncep <xarray.DataArray 'psl' (lat: 73, lon: 144)> dask.array<getitem, shape=(73, 144), dtype=float64, chunksize=(73, 144), chunktype=numpy.ndarray> Coordinates: time datetime64[ns] 1979-01-16T12:00:00 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 Attributes: regrid_method: nearest_s2d model_id: inmcm4","title":"Part IV - Groupby time stamp"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#part-iv-get-density-cubes_1","text":"","title":"Part IV - Get Density Cubes"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#part-v-normalize","text":"base_norm = normalize_data ( base_df ) cmip_norm = normalize_data ( cmip_df ) base_norm [: None ] . shape (4826430, 16)","title":"Part V - Normalize"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#part-vi-rbig-algorithm","text":"","title":"Part VI - RBIG Algorithm"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#entropy-total-correlation","text":"base_tc , base_h , t1 = run_rbig_models ( base_norm [: 1_000 ], measure = 't' , verbose = None ) cmip_tc , cmip_h , t2 = run_rbig_models ( cmip_norm [: 1_000 ], measure = 't' , verbose = None ) base_tc / 16 , cmip_tc / 16 (2.8645279873342417, 2.6572342136429885) base_h / 16 , cmip_h / 16 (-2.7045988055470094, -2.0434281745463228) type ( t1 ) float","title":"Entropy, Total Correlation"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#mutual-information","text":"mi , t_ = run_rbig_models ( base_norm [: 1_000 ], cmip_norm [: 1_000 ], measure = 'mi' , verbose = None ) mi 0.9339957437091543 from scipy import stats pears = stats . pearsonr ( base_norm [: 1_000 ] . ravel (), cmip_norm [: 1_000 ] . ravel ()) spears = stats . spearmanr ( base_norm [: 1_000 ] . ravel (), cmip_norm [: 1_000 ] . ravel ()) kend = stats . kendalltau ( base_norm [: 1_000 ] . ravel (), cmip_norm [: 1_000 ] . ravel ()) pears [ 0 ], spears [ 0 ], kend [ 0 ] (0.19924053221759874, 0.18606468822908886, 0.1297966352157317) from","title":"Mutual Information"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#experiment-i-comparing-climate-models","text":"","title":"Experiment I - Comparing Climate Models"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#mean-sea-level-pressure","text":"","title":"Mean Sea Level Pressure"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#era5-vs-ncep","text":"# Experiment class class ClimateEntropy : def __init__ ( self , save_path : None , variable : str = 'mslp' , save_name = None , mi : bool = True ): self . variable = variable self . results_path = save_path self . results_df = pd . DataFrame () self . save_name = save_name self . mi = mi def run_experiment ( self , climate_model1 : pd . DataFrame , climate_model2 : pd . DataFrame ) -> None : \"\"\"Training loop that goes through each year and calculates the entropy, total correlation and mutual information between the two models.\"\"\" time_length = len ( climate_model1 . groupby ( 'time.year' )) # Normalize BEFORE the individual calculations climate_model1 [ self . variable ] = normalize_temporal ( climate_model1 [ self . variable ]) model1_id = climate_model1 . attrs [ 'model_id' ] model2_id = climate_model2 . attrs [ 'model_id' ] climate_model2 [ self . variable ] = normalize_temporal ( climate_model2 [ self . variable ]) with tqdm ( zip ( climate_model1 . groupby ( 'time.year' ), climate_model2 . groupby ( 'time.year' ) ), total = time_length ) as progress_bar : for imodel1 , imodel2 in progress_bar : # Update params in progress bar # Transform to dataframe, remove spatial dimensions X1 = self . _get_time_features ( imodel1 [ 1 ][ self . variable ]) X2 = self . _get_time_features ( imodel2 [ 1 ][ self . variable ]) # Normalize inputs min_max_scaler = preprocessing . StandardScaler () X1 = min_max_scaler . fit_transform ( X1 . values ) X2 = min_max_scaler . fit_transform ( X2 . values ) dims = X1 . shape [ 1 ] # ============================= # Calculate Mutual Information # ============================= if self . mi == False : mi_ = None mi_t_ = None else : mi_ , mi_t_ = run_rbig_models ( X1 , X2 , measure = 'mi' , verbose = None ) # Update params in progress bar postfix = dict ( ) # ======================================== # Calculate Entropy and Total Correlation # ======================================== # Model I tc1_ , h1_ , h_t1_ = run_rbig_models ( X1 , measure = 't' , verbose = None ) self . _update_results ( model = model1_id , year = imodel1 [ 0 ], h_time = h_t1_ , tc = tc1_ , h = h1_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Model II tc2_ , h2_ , h_t2_ = run_rbig_models ( X2 , measure = 't' , verbose = None ) self . _update_results ( model = model2_id , year = imodel2 [ 0 ], h_time = h_t2_ , tc = tc2_ , h = h2_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Update params in progress bar postfix = dict ( year = imodel1 [ 0 ], mi = f \" { mi_ : .3f } \" if self . mi is True else None , h1 = f \" { h1_ : .3f } \" , tc1 = f \" { tc1_ : .3f } \" , h2 = f \" { h2_ : .3f } \" , tc2 = f \" { tc2_ : .3f } \" , ) progress_bar . set_postfix ( postfix ) return None def _get_time_features ( self , data_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"This function collapses the spatial dimensions as pivots. This allows us to only consider time as the input feature.\"\"\" return data_df . to_dataframe () . unstack ( level = 0 ) . reset_index () . drop ( columns = [ 'lat' , 'lon' ]) . dropna () def _update_results ( self , model , year , tc , h , h_time , mi , mi_time , dims ): \"\"\"appends new values to the results dataframe.\"\"\" self . results_df = self . results_df . append ({ 'model' : model , 'year' : year , 'tc' : tc , 'h' : h , 'h_time' : h_time , 'mi' : mi , 'mi_time' : mi_time , 'dims' : dims , }, ignore_index = True ) if self . results_path is not None : self . _save_results () return self def _save_results ( self ): \"\"\"Saves the dataframe to the assigned results path.\"\"\" self . results_df . to_csv ( f \" { self . results_path }{ self . variable } _ { self . save_name } .csv\" ) return None # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_ncep' ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid , ncep_data ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 41/41 [1:43:03<00:00, 150.83s/it, year=2019, mi=4.766, h1=-1.313, tc1=9.551, h2=-3.049, tc2=13.532] # extract results results_df = short_decade_exp . results_df","title":"ERA5 vs NCEP"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#era5-vs-cmip5","text":"2006 - 01 - 16 , 2025 - 12 - 16 , 1979 - 01 - 01 , 2019 - 07 - 01 # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:08<00:00, 150.61s/it, year=2019, mi=3.290, h1=-1.509, tc1=9.747, h2=-0.807, tc2=8.142]","title":"ERA5 vs CMIP5"},{"location":"notebooks/climate/amip/3.1_demo_exp_global/#ncep-vs-cmip5","text":"# Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'ncep_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( ncep_data . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:04<00:00, 150.30s/it, year=2019, mi=3.552, h1=-1.495, tc1=9.760, h2=-0.807, tc2=8.142]","title":"NCEP vs CMIP5"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Comparing Two Climate Models \u00b6 In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures. Data - Climate Models \u00b6 import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.amip import DataDownloader , DataLoader from src.data.climate.era5 import get_era5_data from src.data.climate.ncep import get_ncep_data from src.features.climate.build_features import ( get_time_overlap , check_time_coords , regrid_2_lower_res , get_spatial_cubes , normalize_data ) from src.experiments.climate.amip_global import ( experiment_loop_comparative , experiment_loop_individual ) # Stat Tools from src.models.train_models import run_rbig_models from scipy import stats import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 amip_data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" era5_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/era5/\" ncep_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/ncep/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\" Demo Experiment \u00b6 Experimental Paams \u00b6 class DataArgs : data_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" results_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip\" class CMIPArgs : # Fixed Params spatial_windows = [ 1 , 2 , # Spatial Window for Density Cubes 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] # Free Params variables = [ 'psl' # Mean Surface Pressure ] cmip_models = [ \"inmcm4\" , \"access1_0\" , \"bcc_csm1_1\" , \"bcc_csm1_1_m\" , \"bnu_esm\" , \"giss_e2_r\" , \"cnrm_cm5\" , \"ipsl_cm5a_lr\" , \"ipsl_cm5a_mr\" , \"ipsl_cm5b_lr\" , \"mpi_esm_lr\" , \"mpi_esm_mr\" , \"noresm1_m\" , ] base_models = [ 'ncep' , \"era5\" ] def run_exp (): for ibase in CMIPArgs . base_models : print ( 'Base Model:' , ibase ) for ivariable in CMIPArgs . variables : print ( 'Variable:' , ivariable ) for icmip in CMIPArgs . cmip_models : print ( \"CMIP Model:\" , icmip ) run_exp () Base Model: ncep Variable: psl CMIP Model: inmcm4 CMIP Model: access1_0 CMIP Model: bcc_csm1_1 CMIP Model: bcc_csm1_1_m CMIP Model: bnu_esm CMIP Model: giss_e2_r CMIP Model: cnrm_cm5 CMIP Model: ipsl_cm5a_lr CMIP Model: ipsl_cm5a_mr CMIP Model: ipsl_cm5b_lr CMIP Model: mpi_esm_lr CMIP Model: mpi_esm_mr CMIP Model: noresm1_m Base Model: era5 Variable: psl CMIP Model: inmcm4 CMIP Model: access1_0 CMIP Model: bcc_csm1_1 CMIP Model: bcc_csm1_1_m CMIP Model: bnu_esm CMIP Model: giss_e2_r CMIP Model: cnrm_cm5 CMIP Model: ipsl_cm5a_lr CMIP Model: ipsl_cm5a_mr CMIP Model: ipsl_cm5b_lr CMIP Model: mpi_esm_lr CMIP Model: mpi_esm_mr CMIP Model: noresm1_m Part I - Grab Data \u00b6 from src.data.climate.amip import get_base_model base_dat = get_base_model ( CMIPArgs . base_models [ 0 ], CMIPArgs . variables [ 0 ]) base_dat <xarray.DataArray 'psl' (time: 489, lat: 73, lon: 144)> dask.array<open_dataset-0ace0936f02ce97f3d79321b6d5f6a55mslp, shape=(489, 73, 144), dtype=float32, chunksize=(489, 73, 144), chunktype=numpy.ndarray> Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Attributes: long_name: Monthly Mean Sea Level Pressure valid_range: [-32766 15234] unpacked_valid_range: [ 77000. 125000.] actual_range: [ 95644. 105703.] units: Pascals precision: 0 GRIB_id: 2 GRIB_name: PRMSL var_desc: Mean Sea Level Pressure dataset: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Monthly... level_desc: Sea Level statistic: Individual Obs parent_stat: Other standard_name: pressure cell_methods: time: mean (monthly from 6-hourly values) model_id: ncep from src.data.climate.cmip5 import get_cmip5_model cmip_dat = get_cmip5_model ( CMIPArgs . cmip_models [ 0 ], CMIPArgs . variables [ 0 ]) cmip_dat <xarray.DataArray 'psl' (time: 360, lat: 120, lon: 180)> dask.array<open_dataset-0a95b28dbc539d100c816bea9e2db9e7psl, shape=(360, 120, 180), dtype=float32, chunksize=(360, 120, 180), chunktype=numpy.ndarray> Coordinates: * time (time) object 1979-01-16 12:00:00 ... 2008-12-16 12:00:00 * lat (lat) float64 -89.25 -87.75 -86.25 -84.75 ... 86.25 87.75 89.25 * lon (lon) float64 0.0 2.0 4.0 6.0 8.0 ... 350.0 352.0 354.0 356.0 358.0 Attributes: standard_name: air_pressure_at_sea_level long_name: Sea Level Pressure comment: not, in general, the same as surface pressure units: Pa original_name: psl cell_methods: time: mean (interval: 1 month) history: 2011-02-07T11:53:19Z altered by CMOR: Reordered dimens... associated_files: baseURL: http://cmip-pcmdi.llnl.gov/CMIP5/dataLocation... model_id: inmcm4 def run_exp (): for ibase in CMIPArgs . base_models : for ivariable in CMIPArgs . variables : for icmip in CMIPArgs . cmip_models : print ( ibase ) print ( ivariable ) print ( icmip ) Part II - Regrid Data \u00b6 base_dat , cmip_dat = regrid_2_lower_res ( base_dat , cmip_dat ) assert ( base_dat . shape [ 1 ] == cmip_dat . shape [ 1 ]) assert ( base_dat . shape [ 2 ] == cmip_dat . shape [ 2 ]) base_dat Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc <xarray.DataArray 'psl' (time: 489, lat: 73, lon: 144)> dask.array<open_dataset-0ace0936f02ce97f3d79321b6d5f6a55mslp, shape=(489, 73, 144), dtype=float32, chunksize=(489, 73, 144), chunktype=numpy.ndarray> Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Attributes: long_name: Monthly Mean Sea Level Pressure valid_range: [-32766 15234] unpacked_valid_range: [ 77000. 125000.] actual_range: [ 95644. 105703.] units: Pascals precision: 0 GRIB_id: 2 GRIB_name: PRMSL var_desc: Mean Sea Level Pressure dataset: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Monthly... level_desc: Sea Level statistic: Individual Obs parent_stat: Other standard_name: pressure cell_methods: time: mean (monthly from 6-hourly values) model_id: ncep Get Features Loop \u00b6 from src.experiments.climate.amip_local import get_features_loop base_dat , cmip_dat = get_features_loop ( 'ncep' , 'access1_0' , 'psl' , 'test' ) base_dat . shape , cmip_dat . shape Create weight file: test.nc Remove file test.nc ((359, 73, 144), (360, 73, 144)) Generate Temporal Data Loop \u00b6 from typing import Optional def generate_temporal_data ( base_dat , cmip_dat , time : Optional [ str ] = \"month\" ): if time == \"month\" : time_stamps = min ( len ( base_dat . time ), len ( cmip_dat . time )) for itime in range ( time_stamps ): itime_stamp = base_dat . time . values ibase_dat = base_dat . isel ( time = itime ) icmip_dat = cmip_dat . isel ( time = itime ) ibase_dat = ibase_dat . expand_dims ({ \"time\" : 1 }) icmip_dat = icmip_dat . expand_dims ({ \"time\" : 1 }) yield ibase_dat , icmip_dat elif time == \"year\" : base_dat = base_dat . groupby ( 'time.year' ) cmip_dat = cmip_dat . groupby ( 'time.year' ) for ibase_dat , icmip_dat in zip ( base_dat , cmip_dat ): yield ibase_dat [ 1 ], icmip_dat [ 1 ] for ( ibase_dat , icmip_dat ) in generate_temporal_data ( base_dat , cmip_dat , 'year' ): print ( ibase_dat . shape , icmip_dat . shape ) break (11, 73, 144) (12, 73, 144) Part IV - Get Density Cubes \u00b6 base_df = get_spatial_cubes ( ibase_dat , CMIPArgs . spatial_windows [ 3 ]) cmip_df = get_spatial_cubes ( icmip_dat , CMIPArgs . spatial_windows [ 3 ]) base_df . shape , cmip_df . shape ((108570, 16), (118440, 16)) Test Individual Loop \u00b6 test_base_model = 'ncep' test_cmip_model = 'inmcm4' test_variable = 'psl' test_spatial_window = 7 subsamples = 1_000 res = experiment_loop_individual ( test_base_model , test_cmip_model , test_variable , test_spatial_window , subsamples ) Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc res {'h_base': -149.66922579994676, 'tc_base': 184.67797240499547, 'h_cmip': -112.08752658960748, 'tc_cmip': 124.29975263830998, 't_base': 12.304942846298218, 't_cmip': 13.265496253967285} Test Comparative Loop \u00b6 test_base_model = 'ncep' test_cmip_model = 'inmcm4' test_variable = 'psl' test_spatial_window = 7 subsamples = 1_000 res = experiment_loop_comparative ( test_base_model , test_cmip_model , test_variable , test_spatial_window , subsamples ) Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc (3319314, 49) (3328560, 49) res {'mi': 12.93477259679793, 'time_mi': 116.64088153839111, 'pearson': 0.6832584799235359, 'spearman': 0.6455462181832066, 'kendelltau': 0.45468733761829255} Experimental Loop \u00b6 ibase_dat . time . values numpy.datetime64('1979-02-01T00:00:00.000000000') Part IV - Groupby time stamp \u00b6 time_stamps = min ( len ( base_dat . time ), len ( cmip_dat . time )) with tqdm ( range ( time_stamps )) as progress_bar : for itime in progress_bar : print ( base_dat . isel ( time = itime )) ibase_dat = base_dat . isel ( time = itime , drop = False ) icmip_dat = cmip_dat . isel ( time = itime ) # print(ibase_dat) # print(icmip_dat) break 0%| | 0/359 [00:00<?, ?it/s] <xarray.DataArray 'psl' (lat: 73, lon: 144)> dask.array<getitem, shape=(73, 144), dtype=float32, chunksize=(73, 144), chunktype=numpy.ndarray> Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 time datetime64[ns] 1979-02-01 Attributes: long_name: Monthly Mean Sea Level Pressure valid_range: [-32766 15234] unpacked_valid_range: [ 77000. 125000.] actual_range: [ 95644. 105703.] units: Pascals precision: 0 GRIB_id: 2 GRIB_name: PRMSL var_desc: Mean Sea Level Pressure dataset: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Monthly... level_desc: Sea Level statistic: Individual Obs parent_stat: Other standard_name: pressure cell_methods: time: mean (monthly from 6-hourly values) model_id: ncep Part IV - Get Density Cubes \u00b6 Part V - Normalize \u00b6 base_norm = normalize_data ( base_df ) cmip_norm = normalize_data ( cmip_df ) base_norm [: None ] . shape (4826430, 16) Part VI - RBIG Algorithm \u00b6 Entropy, Total Correlation \u00b6 base_tc , base_h , t1 = run_rbig_models ( base_norm [: 1_000 ], measure = 't' , verbose = None ) cmip_tc , cmip_h , t2 = run_rbig_models ( cmip_norm [: 1_000 ], measure = 't' , verbose = None ) base_tc / 16 , cmip_tc / 16 (2.8645279873342417, 2.6572342136429885) base_h / 16 , cmip_h / 16 (-2.7045988055470094, -2.0434281745463228) type ( t1 ) float Mutual Information \u00b6 mi , t_ = run_rbig_models ( base_norm [: 1_000 ], cmip_norm [: 1_000 ], measure = 'mi' , verbose = None ) mi 0.9339957437091543 from scipy import stats pears = stats . pearsonr ( base_norm [: 1_000 ] . ravel (), cmip_norm [: 1_000 ] . ravel ()) spears = stats . spearmanr ( base_norm [: 1_000 ] . ravel (), cmip_norm [: 1_000 ] . ravel ()) kend = stats . kendalltau ( base_norm [: 1_000 ] . ravel (), cmip_norm [: 1_000 ] . ravel ()) pears [ 0 ], spears [ 0 ], kend [ 0 ] (0.19924053221759874, 0.18606468822908886, 0.1297966352157317) from Experiment I - Comparing Climate Models \u00b6 Mean Sea Level Pressure \u00b6 ERA5 vs NCEP \u00b6 # Experiment class class ClimateEntropy : def __init__ ( self , save_path : None , variable : str = 'mslp' , save_name = None , mi : bool = True ): self . variable = variable self . results_path = save_path self . results_df = pd . DataFrame () self . save_name = save_name self . mi = mi def run_experiment ( self , climate_model1 : pd . DataFrame , climate_model2 : pd . DataFrame ) -> None : \"\"\"Training loop that goes through each year and calculates the entropy, total correlation and mutual information between the two models.\"\"\" time_length = len ( climate_model1 . groupby ( 'time.year' )) # Normalize BEFORE the individual calculations climate_model1 [ self . variable ] = normalize_temporal ( climate_model1 [ self . variable ]) model1_id = climate_model1 . attrs [ 'model_id' ] model2_id = climate_model2 . attrs [ 'model_id' ] climate_model2 [ self . variable ] = normalize_temporal ( climate_model2 [ self . variable ]) with tqdm ( zip ( climate_model1 . groupby ( 'time.year' ), climate_model2 . groupby ( 'time.year' ) ), total = time_length ) as progress_bar : for imodel1 , imodel2 in progress_bar : # Update params in progress bar # Transform to dataframe, remove spatial dimensions X1 = self . _get_time_features ( imodel1 [ 1 ][ self . variable ]) X2 = self . _get_time_features ( imodel2 [ 1 ][ self . variable ]) # Normalize inputs min_max_scaler = preprocessing . StandardScaler () X1 = min_max_scaler . fit_transform ( X1 . values ) X2 = min_max_scaler . fit_transform ( X2 . values ) dims = X1 . shape [ 1 ] # ============================= # Calculate Mutual Information # ============================= if self . mi == False : mi_ = None mi_t_ = None else : mi_ , mi_t_ = run_rbig_models ( X1 , X2 , measure = 'mi' , verbose = None ) # Update params in progress bar postfix = dict ( ) # ======================================== # Calculate Entropy and Total Correlation # ======================================== # Model I tc1_ , h1_ , h_t1_ = run_rbig_models ( X1 , measure = 't' , verbose = None ) self . _update_results ( model = model1_id , year = imodel1 [ 0 ], h_time = h_t1_ , tc = tc1_ , h = h1_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Model II tc2_ , h2_ , h_t2_ = run_rbig_models ( X2 , measure = 't' , verbose = None ) self . _update_results ( model = model2_id , year = imodel2 [ 0 ], h_time = h_t2_ , tc = tc2_ , h = h2_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Update params in progress bar postfix = dict ( year = imodel1 [ 0 ], mi = f \" { mi_ : .3f } \" if self . mi is True else None , h1 = f \" { h1_ : .3f } \" , tc1 = f \" { tc1_ : .3f } \" , h2 = f \" { h2_ : .3f } \" , tc2 = f \" { tc2_ : .3f } \" , ) progress_bar . set_postfix ( postfix ) return None def _get_time_features ( self , data_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"This function collapses the spatial dimensions as pivots. This allows us to only consider time as the input feature.\"\"\" return data_df . to_dataframe () . unstack ( level = 0 ) . reset_index () . drop ( columns = [ 'lat' , 'lon' ]) . dropna () def _update_results ( self , model , year , tc , h , h_time , mi , mi_time , dims ): \"\"\"appends new values to the results dataframe.\"\"\" self . results_df = self . results_df . append ({ 'model' : model , 'year' : year , 'tc' : tc , 'h' : h , 'h_time' : h_time , 'mi' : mi , 'mi_time' : mi_time , 'dims' : dims , }, ignore_index = True ) if self . results_path is not None : self . _save_results () return self def _save_results ( self ): \"\"\"Saves the dataframe to the assigned results path.\"\"\" self . results_df . to_csv ( f \" { self . results_path }{ self . variable } _ { self . save_name } .csv\" ) return None # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_ncep' ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid , ncep_data ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 41/41 [1:43:03<00:00, 150.83s/it, year=2019, mi=4.766, h1=-1.313, tc1=9.551, h2=-3.049, tc2=13.532] # extract results results_df = short_decade_exp . results_df ERA5 vs CMIP5 \u00b6 2006 - 01 - 16 , 2025 - 12 - 16 , 1979 - 01 - 01 , 2019 - 07 - 01 # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:08<00:00, 150.61s/it, year=2019, mi=3.290, h1=-1.509, tc1=9.747, h2=-0.807, tc2=8.142] NCEP vs CMIP5 \u00b6 # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'ncep_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( ncep_data . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:04<00:00, 150.30s/it, year=2019, mi=3.552, h1=-1.495, tc1=9.760, h2=-0.807, tc2=8.142]","title":"3.2 demo exp spatial"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#comparing-two-climate-models","text":"In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures.","title":"Comparing Two Climate Models"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#data-climate-models","text":"import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.amip import DataDownloader , DataLoader from src.data.climate.era5 import get_era5_data from src.data.climate.ncep import get_ncep_data from src.features.climate.build_features import ( get_time_overlap , check_time_coords , regrid_2_lower_res , get_spatial_cubes , normalize_data ) from src.experiments.climate.amip_global import ( experiment_loop_comparative , experiment_loop_individual ) # Stat Tools from src.models.train_models import run_rbig_models from scipy import stats import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 amip_data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" era5_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/era5/\" ncep_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/ncep/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\"","title":"Data - Climate Models"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#demo-experiment","text":"","title":"Demo Experiment"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#experimental-paams","text":"class DataArgs : data_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" results_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip\" class CMIPArgs : # Fixed Params spatial_windows = [ 1 , 2 , # Spatial Window for Density Cubes 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] # Free Params variables = [ 'psl' # Mean Surface Pressure ] cmip_models = [ \"inmcm4\" , \"access1_0\" , \"bcc_csm1_1\" , \"bcc_csm1_1_m\" , \"bnu_esm\" , \"giss_e2_r\" , \"cnrm_cm5\" , \"ipsl_cm5a_lr\" , \"ipsl_cm5a_mr\" , \"ipsl_cm5b_lr\" , \"mpi_esm_lr\" , \"mpi_esm_mr\" , \"noresm1_m\" , ] base_models = [ 'ncep' , \"era5\" ] def run_exp (): for ibase in CMIPArgs . base_models : print ( 'Base Model:' , ibase ) for ivariable in CMIPArgs . variables : print ( 'Variable:' , ivariable ) for icmip in CMIPArgs . cmip_models : print ( \"CMIP Model:\" , icmip ) run_exp () Base Model: ncep Variable: psl CMIP Model: inmcm4 CMIP Model: access1_0 CMIP Model: bcc_csm1_1 CMIP Model: bcc_csm1_1_m CMIP Model: bnu_esm CMIP Model: giss_e2_r CMIP Model: cnrm_cm5 CMIP Model: ipsl_cm5a_lr CMIP Model: ipsl_cm5a_mr CMIP Model: ipsl_cm5b_lr CMIP Model: mpi_esm_lr CMIP Model: mpi_esm_mr CMIP Model: noresm1_m Base Model: era5 Variable: psl CMIP Model: inmcm4 CMIP Model: access1_0 CMIP Model: bcc_csm1_1 CMIP Model: bcc_csm1_1_m CMIP Model: bnu_esm CMIP Model: giss_e2_r CMIP Model: cnrm_cm5 CMIP Model: ipsl_cm5a_lr CMIP Model: ipsl_cm5a_mr CMIP Model: ipsl_cm5b_lr CMIP Model: mpi_esm_lr CMIP Model: mpi_esm_mr CMIP Model: noresm1_m","title":"Experimental Paams"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#part-i-grab-data","text":"from src.data.climate.amip import get_base_model base_dat = get_base_model ( CMIPArgs . base_models [ 0 ], CMIPArgs . variables [ 0 ]) base_dat <xarray.DataArray 'psl' (time: 489, lat: 73, lon: 144)> dask.array<open_dataset-0ace0936f02ce97f3d79321b6d5f6a55mslp, shape=(489, 73, 144), dtype=float32, chunksize=(489, 73, 144), chunktype=numpy.ndarray> Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Attributes: long_name: Monthly Mean Sea Level Pressure valid_range: [-32766 15234] unpacked_valid_range: [ 77000. 125000.] actual_range: [ 95644. 105703.] units: Pascals precision: 0 GRIB_id: 2 GRIB_name: PRMSL var_desc: Mean Sea Level Pressure dataset: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Monthly... level_desc: Sea Level statistic: Individual Obs parent_stat: Other standard_name: pressure cell_methods: time: mean (monthly from 6-hourly values) model_id: ncep from src.data.climate.cmip5 import get_cmip5_model cmip_dat = get_cmip5_model ( CMIPArgs . cmip_models [ 0 ], CMIPArgs . variables [ 0 ]) cmip_dat <xarray.DataArray 'psl' (time: 360, lat: 120, lon: 180)> dask.array<open_dataset-0a95b28dbc539d100c816bea9e2db9e7psl, shape=(360, 120, 180), dtype=float32, chunksize=(360, 120, 180), chunktype=numpy.ndarray> Coordinates: * time (time) object 1979-01-16 12:00:00 ... 2008-12-16 12:00:00 * lat (lat) float64 -89.25 -87.75 -86.25 -84.75 ... 86.25 87.75 89.25 * lon (lon) float64 0.0 2.0 4.0 6.0 8.0 ... 350.0 352.0 354.0 356.0 358.0 Attributes: standard_name: air_pressure_at_sea_level long_name: Sea Level Pressure comment: not, in general, the same as surface pressure units: Pa original_name: psl cell_methods: time: mean (interval: 1 month) history: 2011-02-07T11:53:19Z altered by CMOR: Reordered dimens... associated_files: baseURL: http://cmip-pcmdi.llnl.gov/CMIP5/dataLocation... model_id: inmcm4 def run_exp (): for ibase in CMIPArgs . base_models : for ivariable in CMIPArgs . variables : for icmip in CMIPArgs . cmip_models : print ( ibase ) print ( ivariable ) print ( icmip )","title":"Part I - Grab Data"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#part-ii-regrid-data","text":"base_dat , cmip_dat = regrid_2_lower_res ( base_dat , cmip_dat ) assert ( base_dat . shape [ 1 ] == cmip_dat . shape [ 1 ]) assert ( base_dat . shape [ 2 ] == cmip_dat . shape [ 2 ]) base_dat Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc <xarray.DataArray 'psl' (time: 489, lat: 73, lon: 144)> dask.array<open_dataset-0ace0936f02ce97f3d79321b6d5f6a55mslp, shape=(489, 73, 144), dtype=float32, chunksize=(489, 73, 144), chunktype=numpy.ndarray> Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 * time (time) datetime64[ns] 1979-01-01 1979-02-01 ... 2019-09-01 Attributes: long_name: Monthly Mean Sea Level Pressure valid_range: [-32766 15234] unpacked_valid_range: [ 77000. 125000.] actual_range: [ 95644. 105703.] units: Pascals precision: 0 GRIB_id: 2 GRIB_name: PRMSL var_desc: Mean Sea Level Pressure dataset: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Monthly... level_desc: Sea Level statistic: Individual Obs parent_stat: Other standard_name: pressure cell_methods: time: mean (monthly from 6-hourly values) model_id: ncep","title":"Part II - Regrid Data"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#get-features-loop","text":"from src.experiments.climate.amip_local import get_features_loop base_dat , cmip_dat = get_features_loop ( 'ncep' , 'access1_0' , 'psl' , 'test' ) base_dat . shape , cmip_dat . shape Create weight file: test.nc Remove file test.nc ((359, 73, 144), (360, 73, 144))","title":"Get Features Loop"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#generate-temporal-data-loop","text":"from typing import Optional def generate_temporal_data ( base_dat , cmip_dat , time : Optional [ str ] = \"month\" ): if time == \"month\" : time_stamps = min ( len ( base_dat . time ), len ( cmip_dat . time )) for itime in range ( time_stamps ): itime_stamp = base_dat . time . values ibase_dat = base_dat . isel ( time = itime ) icmip_dat = cmip_dat . isel ( time = itime ) ibase_dat = ibase_dat . expand_dims ({ \"time\" : 1 }) icmip_dat = icmip_dat . expand_dims ({ \"time\" : 1 }) yield ibase_dat , icmip_dat elif time == \"year\" : base_dat = base_dat . groupby ( 'time.year' ) cmip_dat = cmip_dat . groupby ( 'time.year' ) for ibase_dat , icmip_dat in zip ( base_dat , cmip_dat ): yield ibase_dat [ 1 ], icmip_dat [ 1 ] for ( ibase_dat , icmip_dat ) in generate_temporal_data ( base_dat , cmip_dat , 'year' ): print ( ibase_dat . shape , icmip_dat . shape ) break (11, 73, 144) (12, 73, 144)","title":"Generate Temporal Data Loop"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#part-iv-get-density-cubes","text":"base_df = get_spatial_cubes ( ibase_dat , CMIPArgs . spatial_windows [ 3 ]) cmip_df = get_spatial_cubes ( icmip_dat , CMIPArgs . spatial_windows [ 3 ]) base_df . shape , cmip_df . shape ((108570, 16), (118440, 16))","title":"Part IV - Get Density Cubes"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#test-individual-loop","text":"test_base_model = 'ncep' test_cmip_model = 'inmcm4' test_variable = 'psl' test_spatial_window = 7 subsamples = 1_000 res = experiment_loop_individual ( test_base_model , test_cmip_model , test_variable , test_spatial_window , subsamples ) Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc res {'h_base': -149.66922579994676, 'tc_base': 184.67797240499547, 'h_cmip': -112.08752658960748, 'tc_cmip': 124.29975263830998, 't_base': 12.304942846298218, 't_cmip': 13.265496253967285}","title":"Test Individual Loop"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#test-comparative-loop","text":"test_base_model = 'ncep' test_cmip_model = 'inmcm4' test_variable = 'psl' test_spatial_window = 7 subsamples = 1_000 res = experiment_loop_comparative ( test_base_model , test_cmip_model , test_variable , test_spatial_window , subsamples ) Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc (3319314, 49) (3328560, 49) res {'mi': 12.93477259679793, 'time_mi': 116.64088153839111, 'pearson': 0.6832584799235359, 'spearman': 0.6455462181832066, 'kendelltau': 0.45468733761829255}","title":"Test Comparative Loop"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#experimental-loop","text":"ibase_dat . time . values numpy.datetime64('1979-02-01T00:00:00.000000000')","title":"Experimental Loop"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#part-iv-groupby-time-stamp","text":"time_stamps = min ( len ( base_dat . time ), len ( cmip_dat . time )) with tqdm ( range ( time_stamps )) as progress_bar : for itime in progress_bar : print ( base_dat . isel ( time = itime )) ibase_dat = base_dat . isel ( time = itime , drop = False ) icmip_dat = cmip_dat . isel ( time = itime ) # print(ibase_dat) # print(icmip_dat) break 0%| | 0/359 [00:00<?, ?it/s] <xarray.DataArray 'psl' (lat: 73, lon: 144)> dask.array<getitem, shape=(73, 144), dtype=float32, chunksize=(73, 144), chunktype=numpy.ndarray> Coordinates: * lat (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0 * lon (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5 time datetime64[ns] 1979-02-01 Attributes: long_name: Monthly Mean Sea Level Pressure valid_range: [-32766 15234] unpacked_valid_range: [ 77000. 125000.] actual_range: [ 95644. 105703.] units: Pascals precision: 0 GRIB_id: 2 GRIB_name: PRMSL var_desc: Mean Sea Level Pressure dataset: NCEP/DOE AMIP-II Reanalysis (Reanalysis-2) Monthly... level_desc: Sea Level statistic: Individual Obs parent_stat: Other standard_name: pressure cell_methods: time: mean (monthly from 6-hourly values) model_id: ncep","title":"Part IV - Groupby time stamp"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#part-iv-get-density-cubes_1","text":"","title":"Part IV - Get Density Cubes"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#part-v-normalize","text":"base_norm = normalize_data ( base_df ) cmip_norm = normalize_data ( cmip_df ) base_norm [: None ] . shape (4826430, 16)","title":"Part V - Normalize"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#part-vi-rbig-algorithm","text":"","title":"Part VI - RBIG Algorithm"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#entropy-total-correlation","text":"base_tc , base_h , t1 = run_rbig_models ( base_norm [: 1_000 ], measure = 't' , verbose = None ) cmip_tc , cmip_h , t2 = run_rbig_models ( cmip_norm [: 1_000 ], measure = 't' , verbose = None ) base_tc / 16 , cmip_tc / 16 (2.8645279873342417, 2.6572342136429885) base_h / 16 , cmip_h / 16 (-2.7045988055470094, -2.0434281745463228) type ( t1 ) float","title":"Entropy, Total Correlation"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#mutual-information","text":"mi , t_ = run_rbig_models ( base_norm [: 1_000 ], cmip_norm [: 1_000 ], measure = 'mi' , verbose = None ) mi 0.9339957437091543 from scipy import stats pears = stats . pearsonr ( base_norm [: 1_000 ] . ravel (), cmip_norm [: 1_000 ] . ravel ()) spears = stats . spearmanr ( base_norm [: 1_000 ] . ravel (), cmip_norm [: 1_000 ] . ravel ()) kend = stats . kendalltau ( base_norm [: 1_000 ] . ravel (), cmip_norm [: 1_000 ] . ravel ()) pears [ 0 ], spears [ 0 ], kend [ 0 ] (0.19924053221759874, 0.18606468822908886, 0.1297966352157317) from","title":"Mutual Information"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#experiment-i-comparing-climate-models","text":"","title":"Experiment I - Comparing Climate Models"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#mean-sea-level-pressure","text":"","title":"Mean Sea Level Pressure"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#era5-vs-ncep","text":"# Experiment class class ClimateEntropy : def __init__ ( self , save_path : None , variable : str = 'mslp' , save_name = None , mi : bool = True ): self . variable = variable self . results_path = save_path self . results_df = pd . DataFrame () self . save_name = save_name self . mi = mi def run_experiment ( self , climate_model1 : pd . DataFrame , climate_model2 : pd . DataFrame ) -> None : \"\"\"Training loop that goes through each year and calculates the entropy, total correlation and mutual information between the two models.\"\"\" time_length = len ( climate_model1 . groupby ( 'time.year' )) # Normalize BEFORE the individual calculations climate_model1 [ self . variable ] = normalize_temporal ( climate_model1 [ self . variable ]) model1_id = climate_model1 . attrs [ 'model_id' ] model2_id = climate_model2 . attrs [ 'model_id' ] climate_model2 [ self . variable ] = normalize_temporal ( climate_model2 [ self . variable ]) with tqdm ( zip ( climate_model1 . groupby ( 'time.year' ), climate_model2 . groupby ( 'time.year' ) ), total = time_length ) as progress_bar : for imodel1 , imodel2 in progress_bar : # Update params in progress bar # Transform to dataframe, remove spatial dimensions X1 = self . _get_time_features ( imodel1 [ 1 ][ self . variable ]) X2 = self . _get_time_features ( imodel2 [ 1 ][ self . variable ]) # Normalize inputs min_max_scaler = preprocessing . StandardScaler () X1 = min_max_scaler . fit_transform ( X1 . values ) X2 = min_max_scaler . fit_transform ( X2 . values ) dims = X1 . shape [ 1 ] # ============================= # Calculate Mutual Information # ============================= if self . mi == False : mi_ = None mi_t_ = None else : mi_ , mi_t_ = run_rbig_models ( X1 , X2 , measure = 'mi' , verbose = None ) # Update params in progress bar postfix = dict ( ) # ======================================== # Calculate Entropy and Total Correlation # ======================================== # Model I tc1_ , h1_ , h_t1_ = run_rbig_models ( X1 , measure = 't' , verbose = None ) self . _update_results ( model = model1_id , year = imodel1 [ 0 ], h_time = h_t1_ , tc = tc1_ , h = h1_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Model II tc2_ , h2_ , h_t2_ = run_rbig_models ( X2 , measure = 't' , verbose = None ) self . _update_results ( model = model2_id , year = imodel2 [ 0 ], h_time = h_t2_ , tc = tc2_ , h = h2_ , mi = mi_ , mi_time = mi_t_ , dims = dims , ) # Update params in progress bar postfix = dict ( year = imodel1 [ 0 ], mi = f \" { mi_ : .3f } \" if self . mi is True else None , h1 = f \" { h1_ : .3f } \" , tc1 = f \" { tc1_ : .3f } \" , h2 = f \" { h2_ : .3f } \" , tc2 = f \" { tc2_ : .3f } \" , ) progress_bar . set_postfix ( postfix ) return None def _get_time_features ( self , data_df : pd . DataFrame ) -> pd . DataFrame : \"\"\"This function collapses the spatial dimensions as pivots. This allows us to only consider time as the input feature.\"\"\" return data_df . to_dataframe () . unstack ( level = 0 ) . reset_index () . drop ( columns = [ 'lat' , 'lon' ]) . dropna () def _update_results ( self , model , year , tc , h , h_time , mi , mi_time , dims ): \"\"\"appends new values to the results dataframe.\"\"\" self . results_df = self . results_df . append ({ 'model' : model , 'year' : year , 'tc' : tc , 'h' : h , 'h_time' : h_time , 'mi' : mi , 'mi_time' : mi_time , 'dims' : dims , }, ignore_index = True ) if self . results_path is not None : self . _save_results () return self def _save_results ( self ): \"\"\"Saves the dataframe to the assigned results path.\"\"\" self . results_df . to_csv ( f \" { self . results_path }{ self . variable } _ { self . save_name } .csv\" ) return None # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_ncep' ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid , ncep_data ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 41/41 [1:43:03<00:00, 150.83s/it, year=2019, mi=4.766, h1=-1.313, tc1=9.551, h2=-3.049, tc2=13.532] # extract results results_df = short_decade_exp . results_df","title":"ERA5 vs NCEP"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#era5-vs-cmip5","text":"2006 - 01 - 16 , 2025 - 12 - 16 , 1979 - 01 - 01 , 2019 - 07 - 01 # Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'era_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( era5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:08<00:00, 150.61s/it, year=2019, mi=3.290, h1=-1.509, tc1=9.747, h2=-0.807, tc2=8.142]","title":"ERA5 vs CMIP5"},{"location":"notebooks/climate/amip/3.2_demo_exp_spatial/#ncep-vs-cmip5","text":"# Initialize experiment short_decade_exp = ClimateEntropy ( save_path = f \" { results_path } \" , variable = 'mslp' , save_name = 'ncep_cmip' , mi = True ) # run experiment (shorter decade) short_decade_exp . run_experiment ( ncep_data . sel ( time = slice ( '2006-01-16' , '2019-07-01' )), cmip5_data_regrid . sel ( time = slice ( '2006-01-16' , '2019-07-01' )) ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14/14 [35:04<00:00, 150.30s/it, year=2019, mi=3.552, h1=-1.495, tc1=9.760, h2=-0.807, tc2=8.142]","title":"NCEP vs CMIP5"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Information Theory Measures \u00b6 In this notebook, I will be demonstrating some of the aspects of information theory measures. Data - Climate Models \u00b6 import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.amip import DataDownloader , DataLoader from src.data.climate.era5 import get_era5_data from src.data.climate.ncep import get_ncep_data from src.features.climate.build_features import ( get_time_overlap , check_time_coords , regrid_2_lower_res , get_spatial_cubes , normalize_data ) from src.experiments.climate.amip_global import ( experiment_loop_comparative , experiment_loop_individual ) # Stat Tools from src.models.information.entropy import RBIGEstimator as RBIGENTEST from src.models.information.mutual_information import RBIGEstimator as RBIGMIEST from scipy import stats import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 amip_data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" era5_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/era5/\" ncep_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/ncep/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\" Demo Experiment \u00b6 Experimental Paams \u00b6 class DataArgs : data_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" results_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip\" class CMIPArgs : # Fixed Params spatial_windows = [ 1 , 2 , # Spatial Window for Density Cubes 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] # Free Params variables = [ 'psl' # Mean Surface Pressure ] cmip_models = [ \"inmcm4\" , \"access1_0\" , \"bcc_csm1_1\" , \"bcc_csm1_1_m\" , \"bnu_esm\" , \"giss_e2_r\" , \"cnrm_cm5\" , \"ipsl_cm5a_lr\" , \"ipsl_cm5a_mr\" , \"ipsl_cm5b_lr\" , \"mpi_esm_lr\" , \"mpi_esm_mr\" , \"noresm1_m\" , ] base_models = [ 'ncep' , \"era5\" ] Part I - Grab Data \u00b6 from src.data.climate.amip import get_base_model base_dat = get_base_model ( CMIPArgs . base_models [ 0 ], CMIPArgs . variables [ 0 ]) # base_dat from src.data.climate.cmip5 import get_cmip5_model cmip_dat = get_cmip5_model ( CMIPArgs . cmip_models [ 0 ], CMIPArgs . variables [ 0 ]) # cmip_dat Part II - Regrid Data \u00b6 base_dat , cmip_dat = regrid_2_lower_res ( base_dat , cmip_dat ) assert ( base_dat . shape [ 1 ] == cmip_dat . shape [ 1 ]) assert ( base_dat . shape [ 2 ] == cmip_dat . shape [ 2 ]) # base_dat Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc Part III - Find Overlapping Times \u00b6 base_dat . shape , cmip_dat . shape ((489, 73, 144), (360, 73, 144)) base_dat , cmip_dat = get_time_overlap ( base_dat , cmip_dat ) Part IV - Get Density Cubes \u00b6 base_df = get_spatial_cubes ( base_dat , CMIPArgs . spatial_windows [ 3 ]) cmip_df = get_spatial_cubes ( cmip_dat , CMIPArgs . spatial_windows [ 3 ]) base_df . shape (3543330, 16) Normalize \u00b6 base_df = normalize_data ( base_df ) cmip_df = normalize_data ( cmip_df ) Information Theory Measures \u00b6 Entropy, H( X X ) \u00b6 subsample = 10_000 batch_size = None bootstrap = False ent_est = RBIGENTEST ( batch_size = batch_size , bootstrap = bootstrap , ) ent_est . fit ( base_df [: subsample ]) h = ent_est . score ( base_df [: subsample ]) h -33.116467738349236 with Bootstrap \u00b6 batch_size = 10_000 bootstrap = True n_iterations = 100 ent_est = RBIGENTEST ( batch_size = batch_size , bootstrap = bootstrap , n_iterations = n_iterations ) ent_est . fit ( base_df ) h = ent_est . score ( base_df ) h -31.881844520814997 plt . hist ( ent_est . raw_scores ) -28.48503649185888 plt . hist ( ent_est . raw_scores ) (array([ 1., 0., 1., 0., 9., 11., 26., 28., 14., 10.]), array([-30.9162801 , -30.56606644, -30.21585278, -29.86563913, -29.51542547, -29.16521182, -28.81499816, -28.46478451, -28.11457085, -27.76435719, -27.41414354]), <a list of 10 Patch objects>) W. Batches \u00b6 subsample = 40_000 ent_est = RBIGENTEST ( batch_size = 10_000 ) ent_est . fit ( base_df [: subsample ]) h = ent_est . score ( base_df [: subsample ]) h -31.84759524855099 ent_est . raw_scores [-32.17903374504498, -31.753140917432507, -31.67399995592763, -31.784206375798846] Total Correlation, TC( X X ) \u00b6 subsample = 40_000 tc_est = RBIGMIEST ( batch_size = None ) tc_est . fit ( base_df [: subsample ]) tc = tc_est . score ( base_df [: subsample ]) tc 51.735384060195784 w. Batches \u00b6 subsample = 40_000 tc_est = RBIGMIEST ( batch_size = 10_000 ) tc_est . fit ( base_df [: subsample ]) tc = tc_est . score ( base_df [: subsample ]) tc 50.6219155716329 tc_est . raw_scores [50.29844313632438, 51.03391865505402, 50.72933249988033, 50.425967995272856] Mutual Information, MI( X X ) \u00b6 subsample = 100_000 mi_est = RBIGMIEST ( batch_size = None ) mi_est . fit ( base_df [: subsample ], cmip_df [: subsample ] ) mi = mi_est . score ( base_df [: subsample ]) mi 1.2438747143982896 w. Batches \u00b6 subsample = 100_000 mi_est = RBIGMIEST ( batch_size = 50_000 ) mi_est . fit ( base_df [: subsample ], cmip_df [: subsample ] ) mi = mi_est . score ( base_df [: subsample ]) mi 1.215228412628969 mi_est . raw_values --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-84-f0a0474b33b1> in <module> ----> 1 mi_est . raw_values AttributeError : 'RBIGEstimator' object has no attribute 'raw_values' Mutual Information II, H(X) + H(Y) - H(X,Y) \u00b6 subsample = 100_000 batch_size = 25_000 # H(X) print ( 'H(X)' ) x_ent_est = RBIGENTEST ( batch_size = batch_size ) x_ent_est . fit ( base_df . values [: subsample ]) h_x = x_ent_est . score ( base_df . values [: subsample ]) # H(Y) print ( 'H(Y)' ) y_ent_est = RBIGENTEST ( batch_size = batch_size ) y_ent_est . fit ( cmip_df . values [: subsample ]) h_y = y_ent_est . score ( cmip_df . values [: subsample ]) # H(X,Y) print ( 'H(X,Y)' ) xy_ent_est = RBIGENTEST ( batch_size = 50_000 ) xy_ent_est . fit ( np . hstack ( ( base_df . values [: subsample ], cmip_df . values [: subsample ] ) ), ) h_xy = xy_ent_est . score ( base_df . values [: subsample ]) H(X) H(Y) H(X,Y) # H(X,Y) print ( 'H(X,Y)' ) xy_ent_est = RBIGENTEST ( batch_size = 50_000 ) xy_ent_est . fit ( np . hstack ( ( base_df . values [: subsample ], cmip_df . values [: subsample ] ) ), ) h_xy = xy_ent_est . score ( base_df . values [: subsample ]) h_xy H(X,Y) 165.23978712025018 h_x , h_y , h_xy , h_x + h_y - h_xy (79.10616714936484, 87.19046271977632, 165.45410606367204, 0.8425238054691135) 0.4360788203771051 Correlation: Pearson, Spearman, KendallTau \u00b6 pear = stats . pearsonr ( base_df [: subsample ] . ravel (), cmip_df [: subsample ] . ravel (), ) spear = stats . spearmanr ( base_df [: subsample ] . ravel (), cmip_df [: subsample ] . ravel (), ) kend = stats . kendalltau ( base_df [: subsample ] . ravel (), cmip_df [: subsample ] . ravel (), ) pear [ 0 ], spear [ 0 ], kend [ 0 ]","title":"3.2 demo it measures"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#information-theory-measures","text":"In this notebook, I will be demonstrating some of the aspects of information theory measures.","title":"Information Theory Measures"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#data-climate-models","text":"import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.amip import DataDownloader , DataLoader from src.data.climate.era5 import get_era5_data from src.data.climate.ncep import get_ncep_data from src.features.climate.build_features import ( get_time_overlap , check_time_coords , regrid_2_lower_res , get_spatial_cubes , normalize_data ) from src.experiments.climate.amip_global import ( experiment_loop_comparative , experiment_loop_individual ) # Stat Tools from src.models.information.entropy import RBIGEstimator as RBIGENTEST from src.models.information.mutual_information import RBIGEstimator as RBIGMIEST from scipy import stats import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 amip_data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" era5_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/era5/\" ncep_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/ncep/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\"","title":"Data - Climate Models"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#demo-experiment","text":"","title":"Demo Experiment"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#experimental-paams","text":"class DataArgs : data_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" results_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip\" class CMIPArgs : # Fixed Params spatial_windows = [ 1 , 2 , # Spatial Window for Density Cubes 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] # Free Params variables = [ 'psl' # Mean Surface Pressure ] cmip_models = [ \"inmcm4\" , \"access1_0\" , \"bcc_csm1_1\" , \"bcc_csm1_1_m\" , \"bnu_esm\" , \"giss_e2_r\" , \"cnrm_cm5\" , \"ipsl_cm5a_lr\" , \"ipsl_cm5a_mr\" , \"ipsl_cm5b_lr\" , \"mpi_esm_lr\" , \"mpi_esm_mr\" , \"noresm1_m\" , ] base_models = [ 'ncep' , \"era5\" ]","title":"Experimental Paams"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#part-i-grab-data","text":"from src.data.climate.amip import get_base_model base_dat = get_base_model ( CMIPArgs . base_models [ 0 ], CMIPArgs . variables [ 0 ]) # base_dat from src.data.climate.cmip5 import get_cmip5_model cmip_dat = get_cmip5_model ( CMIPArgs . cmip_models [ 0 ], CMIPArgs . variables [ 0 ]) # cmip_dat","title":"Part I - Grab Data"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#part-ii-regrid-data","text":"base_dat , cmip_dat = regrid_2_lower_res ( base_dat , cmip_dat ) assert ( base_dat . shape [ 1 ] == cmip_dat . shape [ 1 ]) assert ( base_dat . shape [ 2 ] == cmip_dat . shape [ 2 ]) # base_dat Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc","title":"Part II - Regrid Data"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#part-iii-find-overlapping-times","text":"base_dat . shape , cmip_dat . shape ((489, 73, 144), (360, 73, 144)) base_dat , cmip_dat = get_time_overlap ( base_dat , cmip_dat )","title":"Part III - Find Overlapping Times"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#part-iv-get-density-cubes","text":"base_df = get_spatial_cubes ( base_dat , CMIPArgs . spatial_windows [ 3 ]) cmip_df = get_spatial_cubes ( cmip_dat , CMIPArgs . spatial_windows [ 3 ]) base_df . shape (3543330, 16)","title":"Part IV - Get Density Cubes"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#normalize","text":"base_df = normalize_data ( base_df ) cmip_df = normalize_data ( cmip_df )","title":"Normalize"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#information-theory-measures_1","text":"","title":"Information Theory Measures"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#entropy-hxx","text":"subsample = 10_000 batch_size = None bootstrap = False ent_est = RBIGENTEST ( batch_size = batch_size , bootstrap = bootstrap , ) ent_est . fit ( base_df [: subsample ]) h = ent_est . score ( base_df [: subsample ]) h -33.116467738349236","title":"Entropy, H(XX)"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#with-bootstrap","text":"batch_size = 10_000 bootstrap = True n_iterations = 100 ent_est = RBIGENTEST ( batch_size = batch_size , bootstrap = bootstrap , n_iterations = n_iterations ) ent_est . fit ( base_df ) h = ent_est . score ( base_df ) h -31.881844520814997 plt . hist ( ent_est . raw_scores ) -28.48503649185888 plt . hist ( ent_est . raw_scores ) (array([ 1., 0., 1., 0., 9., 11., 26., 28., 14., 10.]), array([-30.9162801 , -30.56606644, -30.21585278, -29.86563913, -29.51542547, -29.16521182, -28.81499816, -28.46478451, -28.11457085, -27.76435719, -27.41414354]), <a list of 10 Patch objects>)","title":"with Bootstrap"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#w-batches","text":"subsample = 40_000 ent_est = RBIGENTEST ( batch_size = 10_000 ) ent_est . fit ( base_df [: subsample ]) h = ent_est . score ( base_df [: subsample ]) h -31.84759524855099 ent_est . raw_scores [-32.17903374504498, -31.753140917432507, -31.67399995592763, -31.784206375798846]","title":"W. Batches"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#total-correlation-tcxx","text":"subsample = 40_000 tc_est = RBIGMIEST ( batch_size = None ) tc_est . fit ( base_df [: subsample ]) tc = tc_est . score ( base_df [: subsample ]) tc 51.735384060195784","title":"Total Correlation, TC(XX)"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#w-batches_1","text":"subsample = 40_000 tc_est = RBIGMIEST ( batch_size = 10_000 ) tc_est . fit ( base_df [: subsample ]) tc = tc_est . score ( base_df [: subsample ]) tc 50.6219155716329 tc_est . raw_scores [50.29844313632438, 51.03391865505402, 50.72933249988033, 50.425967995272856]","title":"w. Batches"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#mutual-information-mixx","text":"subsample = 100_000 mi_est = RBIGMIEST ( batch_size = None ) mi_est . fit ( base_df [: subsample ], cmip_df [: subsample ] ) mi = mi_est . score ( base_df [: subsample ]) mi 1.2438747143982896","title":"Mutual Information, MI(XX)"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#w-batches_2","text":"subsample = 100_000 mi_est = RBIGMIEST ( batch_size = 50_000 ) mi_est . fit ( base_df [: subsample ], cmip_df [: subsample ] ) mi = mi_est . score ( base_df [: subsample ]) mi 1.215228412628969 mi_est . raw_values --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-84-f0a0474b33b1> in <module> ----> 1 mi_est . raw_values AttributeError : 'RBIGEstimator' object has no attribute 'raw_values'","title":"w. Batches"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#mutual-information-ii-hx-hy-hxy","text":"subsample = 100_000 batch_size = 25_000 # H(X) print ( 'H(X)' ) x_ent_est = RBIGENTEST ( batch_size = batch_size ) x_ent_est . fit ( base_df . values [: subsample ]) h_x = x_ent_est . score ( base_df . values [: subsample ]) # H(Y) print ( 'H(Y)' ) y_ent_est = RBIGENTEST ( batch_size = batch_size ) y_ent_est . fit ( cmip_df . values [: subsample ]) h_y = y_ent_est . score ( cmip_df . values [: subsample ]) # H(X,Y) print ( 'H(X,Y)' ) xy_ent_est = RBIGENTEST ( batch_size = 50_000 ) xy_ent_est . fit ( np . hstack ( ( base_df . values [: subsample ], cmip_df . values [: subsample ] ) ), ) h_xy = xy_ent_est . score ( base_df . values [: subsample ]) H(X) H(Y) H(X,Y) # H(X,Y) print ( 'H(X,Y)' ) xy_ent_est = RBIGENTEST ( batch_size = 50_000 ) xy_ent_est . fit ( np . hstack ( ( base_df . values [: subsample ], cmip_df . values [: subsample ] ) ), ) h_xy = xy_ent_est . score ( base_df . values [: subsample ]) h_xy H(X,Y) 165.23978712025018 h_x , h_y , h_xy , h_x + h_y - h_xy (79.10616714936484, 87.19046271977632, 165.45410606367204, 0.8425238054691135) 0.4360788203771051","title":"Mutual Information II, H(X) + H(Y) - H(X,Y)"},{"location":"notebooks/climate/amip/3.2_demo_it_measures/#correlation-pearson-spearman-kendalltau","text":"pear = stats . pearsonr ( base_df [: subsample ] . ravel (), cmip_df [: subsample ] . ravel (), ) spear = stats . spearmanr ( base_df [: subsample ] . ravel (), cmip_df [: subsample ] . ravel (), ) kend = stats . kendalltau ( base_df [: subsample ] . ravel (), cmip_df [: subsample ] . ravel (), ) pear [ 0 ], spear [ 0 ], kend [ 0 ]","title":"Correlation: Pearson, Spearman, KendallTau"},{"location":"notebooks/climate/amip/4.1_visualize_global/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Visually Comparing Climate Models \u00b6 Summary \u00b6 In this notebook, I will be comparing three climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 CMIP5 I will be looking at the following variables: Mean Sea Level Pressure (CMIP5, ERA5, NCEP) Surface Pressure (ERA5, NCEP) I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures. If these climate models are that similar, then they should exhibit similar IT measures. Preprocessing Steps \u00b6 Regridded Spatially The ERA5 had the coarsest spatial resolution (2.5 x 2.5). I regridded the NCEP from (0.25 x 0.25) to (2.5 x 2.5). I regridded the CMIP5 from (2 x 2.5) to (2.5 x 2.5). Temporal Resolution ERA5 and NCEP go from 1980-2019 CMIP5 goes from 2006-2018 For comparing ERA5 vs CMIP5 and NCEP vs CMIPF, I found the same time components Measures \u00b6 I'm measuring the following: Entropy - expected uncertainty Total Correlation - amount of redundant information between features Mutual Information - amount of information shared between variables Data \u00b6 Inputs I'm taking each year as is. Each spatial location is a sample and each year is a feature. My inputs are: X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} Outputs All my information theory measures are in nats. They are scalars. Hypothesis \u00b6 Simple: The ERA5 and the NCEP model should be more similar than the CMIP5 model compared to each of them. Data - Climate Models \u00b6 import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) # from esdc.preprocessing import normalize_temporal import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing # Visualization Tools from src.data.climate.loader import ResultsLoader from src.visualization.climate.global_it import plot_global_entropy , plot_global_diff_entropy , plot_global_mutual_info , plot_global_diff_mutual_info # from src.visualization.climate import PlotResults import seaborn as sns import matplotlib.pyplot as plt # plt.style.use('ggplot') plt . style . use ([ 'seaborn-notebook' , 'fivethirtyeight' ]) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\" Experiment I - Global Entropy \u00b6 from pathlib import Path data_path = f \"/home/emmanuel/projects/projects/2020_rbig_rs/data/climate/results/amip/local/compare/\" def get_results_files ( base_model : str , trials : bool ) -> pd . DataFrame : path = Path ( data_path ) if base_model == 'ncep' : base_pattern = 'ncep' elif base_model == 'era5' : base_pattern = 'era5' else : raise ValueError ( 'Unrecognized base model:' , base_model ) trials_ext = 'tr100_v1' # if trials == True: # trials_ext = 'v1' # elif trials == False: # trials_ext = 'v2' # else: # raise ValueError(\"Unrecognized trials extentions:\", trials) filename_pattern = base_pattern + '*' + trials_ext + '.csv' df_from_each_file = [ pd . read_csv ( f , index_col = 0 ) for f in path . rglob ( filename_pattern )] results_df = pd . concat ( df_from_each_file , ignore_index = True ) return results_df def post_processing_compare ( df : pd . DataFrame ) -> pd . DataFrame : # divide by the spatial resolution df [ 'mi' ] = df [ 'mi' ] / ( df [ 'spatial' ] ** 2 ) return df ! ls / home / emmanuel / projects / 2020 _rbig_rs / data / climate / results / amip / compare_trial_v1.csv spatial_compare_v3.csv global spatial_compare_v4.csv global_compare_v2.csv spatial_compare_v5.csv global_compare_v3.csv spatial_individual_trial_v1.csv global_individual_v2.csv spatial_individual_v2.csv global_individual_v3.csv spatial_individual_v3.csv global_individual_v4.csv spatial_individual_v4.csv individual_trial_v1.csv spatial_individual_v5.csv local spatial_individual_v6.csv spatial_compare_trial_v1.csv v1.csv spatial_compare_v2.csv v2.csv Entropy \u00b6 df = pd . read_csv ( '/home/emmanuel/projects/2020_rbig_rs/data/climate/interim/amip/global/individual/ncep_bnu_esm_tr100_v1.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 base cmip h_base h_cmip spatial subsample t_base t_cmip tc_base tc_cmip trial variable 0 0 ncep bnu_esm 1.230567 1.290014 1.0 50000.0 1.301512 1.309189 0.0 0.0 0.0 psl 1 1 ncep bnu_esm 1.220025 1.284430 1.0 50000.0 1.288438 1.305716 0.0 0.0 1.0 psl 2 2 ncep bnu_esm 1.221952 1.291945 1.0 50000.0 1.292614 1.298469 0.0 0.0 2.0 psl 3 3 ncep bnu_esm 1.222894 1.289754 1.0 50000.0 1.295369 1.305580 0.0 0.0 3.0 psl 4 4 ncep bnu_esm 1.228588 1.290732 1.0 50000.0 1.302234 1.308249 0.0 0.0 4.0 psl data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/interim/amip/global/individual/\" results_df = get_results_files ( 'era5' , False ) results_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base cmip h_base h_cmip spatial subsample t_base t_cmip tc_base tc_cmip trial variable 0 era5 bcc_csm1_1 1.218948 1.314152 1.0 50000.0 1.496089 1.480152 0.0 0.0 0.0 psl 1 era5 bcc_csm1_1 1.208673 1.308177 1.0 50000.0 1.498710 1.504606 0.0 0.0 1.0 psl 2 era5 bcc_csm1_1 1.214323 1.305756 1.0 50000.0 1.498986 1.505039 0.0 0.0 2.0 psl 3 era5 bcc_csm1_1 1.215485 1.309741 1.0 50000.0 1.543764 1.508892 0.0 0.0 3.0 psl 4 era5 bcc_csm1_1 1.217624 1.315335 1.0 50000.0 1.544796 1.531270 0.0 0.0 4.0 psl results_df . cmip . unique () . tolist () ['bcc_csm1_1', 'access1_0', 'mpi_esm_lr', 'ipsl_cm5a_lr', 'noresm1_m', 'giss_e2_r', 'cnrm_cm5', 'bnu_esm'] SAVEPATH = '/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/amip/global/entropy/' def plot_global_entropy ( results_df : pd . DataFrame , base : str , cmip : str , normalized = True , log_mi = True , save = True ) -> None : if normalized == True : results_df [ 'h_cmip' ] = results_df [ 'h_cmip' ] / results_df [ 'spatial' ] ** 2 results_df [ 'h_base' ] = results_df [ 'h_base' ] / results_df [ 'spatial' ] ** 2 results_df = results_df [ results_df [ 'base' ] == base ] results_df = results_df [ results_df [ 'cmip' ] == cmip ] fig , ax = plt . subplots () sns . scatterplot ( data = results_df , x = 'spatial' , y = 'h_base' , label = f \" { base } \" , ax = ax ) sns . scatterplot ( data = results_df , x = 'spatial' , y = 'h_cmip' , label = f \" { cmip } \" , ax = ax ) ax . set_title ( f ' { base . upper () } vs CMIP: { cmip . upper () } ' ) ax . set_xlabel ( 'Spatial Features' ) # ax.set_xlim([2, 6]) ax . set_ylabel ( 'Entropy, H' ) ax . legend () if save : savename = f \"global_h_ { base } _ { cmip } .png\" fig . savefig ( SAVEPATH + savename , ) else : plt . show () return None Example \u00b6 results_df . cmip . unique () . tolist () ['bcc_csm1_1', 'access1_0', 'mpi_esm_lr', 'ipsl_cm5a_lr', 'noresm1_m', 'giss_e2_r', 'cnrm_cm5', 'bnu_esm'] results_df = get_results_files ( 'era5' , False ) plot_global_entropy ( results_df , 'era5' , 'ipsl_cm5a_lr' , normalized = True , save = True ) for imodel in results_df . cmip . unique () . tolist (): results_df = get_results_files ( 'era5' , False ) plot_global_entropy ( results_df , 'era5' , imodel , normalized = True , save = True ) # plot_global_entropy(results, 'ncep', 'inmcm4', normalized=False) for imodel in results_df . cmip . unique () . tolist (): results_df = get_results_files ( 'ncep' , False ) plot_global_entropy ( results_df , 'ncep' , imodel , normalized = True ,) Difference in Entropy \u00b6 import numpy as np def plot_global_diff_entropy ( results_df : pd . DataFrame , base : str , normalized = True , log_mi = False ) -> None : results_copy = results_df [ results_df [ 'base' ] == base ] if normalized == True : results_copy [ 'h_base' ] = results_copy [ 'h_base' ] / results_copy [ 'spatial' ] ** 2 results_copy [ 'h_cmip' ] = results_copy [ 'h_cmip' ] / results_copy [ 'spatial' ] ** 2 results_copy = results_df [ results_df [ 'base' ] == base ] # calculate difference results_copy [ 'h_diff' ] = np . abs ( results_copy [ 'h_cmip' ] - results_copy [ 'h_base' ]) if log_mi == True : results_copy [ 'h_diff' ] = np . log ( results_copy [ 'h_diff' ]) fig , ax = plt . subplots () # sns.scatterplot(ax=ax, data=results_copy, x='spatial', y='h_diff', hue='base', color='black') sns . lineplot ( ax = ax , data = results_copy , x = 'spatial' , y = 'h_diff' , hue = 'cmip' ) plt . title ( f '' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( 'Entropy, H' ) plt . legend ( ncol = 2 , bbox_to_anchor = ( 2.05 , 1 ), fontsize = 16 ) # plt.tight_layout() plt . show () results_df = get_results_files ( 'era5' , False ) plot_global_diff_entropy ( results_df , 'era5' , normalized = True , save = True ) results_df = get_results_files ( 'ncep' , False ) plot_global_diff_entropy ( results_df , 'ncep' , normalized = True , save = True ) Mutual Information \u00b6 from pathlib import Path data_path = f \"/home/emmanuel/projects/projects/2020_rbig_rs/data/climate/results/amip/global/compare/\" def get_results_files ( base_model : str , trials : bool ) -> pd . DataFrame : path = Path ( data_path ) if base_model == 'ncep' : base_pattern = 'ncep' elif base_model == 'era5' : base_pattern = 'era5' else : raise ValueError ( 'Unrecognized base model:' , base_model ) if trials == True : trials_ext = 'v1' elif trials == False : trials_ext = 'v2' else : raise ValueError ( \"Unrecognized trials extentions:\" , trials ) filename_pattern = base_pattern + '*' + trials_ext + '.csv' # [print(f) for f in path.rglob(filename_pattern)] # [print(pd.read_csv(f, index_col=0).columns.shape) for f in path.rglob(filename_pattern)] df_from_each_file = [ pd . read_csv ( f , index_col = 0 ) for f in path . rglob ( filename_pattern )] results_df = pd . concat ( df_from_each_file , ignore_index = True ) return results_df def post_processing_compare ( df : pd . DataFrame ) -> pd . DataFrame : # divide by the spatial resolution df [ 'mi' ] = df [ 'mi' ] / ( df [ 'spatial' ] ** 2 ) return df data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip/global/compare/\" results_df = get_results_files ( 'ncep' , True ) results_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base cmip kendelltau mi pearson spatial spearman subsample time_mi trial variable 4310 ncep bcc_csm1_1 0.003454 0.023890 0.000746 3.0 0.005176 50000.0 86.576983 5.0 psl 4311 ncep bcc_csm1_1 -0.003573 0.028074 -0.007091 3.0 -0.005361 50000.0 76.669153 6.0 psl 4312 ncep bcc_csm1_1 0.003528 0.027675 0.008871 3.0 0.005295 50000.0 85.649547 7.0 psl 4313 ncep bcc_csm1_1 0.004418 0.066879 0.012918 3.0 0.006640 50000.0 149.052748 8.0 psl 4314 ncep bcc_csm1_1 0.002212 0.025614 0.002713 3.0 0.003324 50000.0 100.513506 9.0 psl results_df . cmip . unique () . tolist () ['access1_0', 'ipsl_cm5a_lr', 'giss_e2_r', 'cnrm_cm5', 'mpi_esm_lr', 'inmcm4', 'bnu_esm', 'bcc_csm1_1_m', 'noresm1_m', 'bcc_csm1_1'] results_df = get_results_files ( 'ncep' , True ) plot_global_mutual_info ( results_df , base = 'ncep' , cmip = 'giss_e2_r' , measure = 'mi' , normalized = True , log_mi = True ) results_df = get_results_files ( 'ncep' , True ) plot_global_mutual_info ( results_df , base = 'ncep' , cmip = None , measure = 'mi' , normalized = True , ) results_df = get_results_files ( 'era5' , True ) plot_global_mutual_info ( results_df , base = 'era5' , cmip = None , measure = 'mi' , normalized = True , log_mi = True ) for imodel in results_df . cmip . unique () . tolist (): results_df = get_results_files ( 'ncep' , True ) plot_global_mutual_info ( results_df , base = 'ncep' , cmip = imodel , measure = 'mi' , normalized = True , log_mi = True ) for imodel in results_df . cmip . unique () . tolist (): results_df = get_results_files ( 'era5' , True ) plot_global_mutual_info ( results_df , base = 'era5' , cmip = imodel , measure = 'mi' , normalized = True , log_mi = False ) def plot_global_mi ( results_df : pd . DataFrame , base : str , cmip : str , measure = 'mi' , normalized = True ) -> None : results_df = results_df [ results_df [ 'base' ] == base ] results_df = results_df [ results_df [ 'cmip' ] == cmip ] if normalized and measure == 'mi' : results_df [ measure ] = results_df [ measure ] / results_df [ 'spatial' ] sns . scatterplot ( data = results_df , x = 'spatial' , y = measure , label = f \" { base } \" ) sns . scatterplot ( data = results_df , x = 'spatial' , y = measure , label = f \" { cmip } \" ) plt . title ( f ' { base . upper () } , CMIP: { cmip . upper () } ' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( f ' { measure . upper () } ' ) plt . legend () plt . show () plot_global_mi ( results_df , 'ncep' , 'access1_0' , 'mi' ) # plot_global_mi(results_df, 'era5', 'noresm1_m', 'pearson') # plot_global_mi(results_df, 'era5', 'noresm1_m', 'spearman') # plot_global_mi(results_df, 'era5', 'noresm1_m', 'kendelltau') plot_global_mi ( results_df , 'ncep' , 'giss_e2_r' , 'mi' ) plot_global_mi ( results_df , 'ncep' , 'giss_e2_r' , 'pearson' ) plot_global_mi ( results_df , 'ncep' , 'giss_e2_r' , 'spearman' ) plot_global_mi ( results_df , 'ncep' , 'giss_e2_r' , 'kendelltau' ) def plot_global_diff_mi ( results_df : pd . DataFrame , base : str , measure = 'mi' , normalized = True , log_mi = True ) -> None : results_copy = results_df [ results_df [ 'base' ] == base ] if normalized == True : results_copy [ measure ] = results_copy [ measure ] / results_copy [ 'spatial' ] if log_mi == True and measure == 'mi' : results_copy [ measure ] = np . log ( results_copy [ measure ]) fig , ax = plt . subplots () # sns.scatterplot(ax=ax, data=results_copy, x='spatial', y='h_diff', hue='base', color='black') sns . scatterplot ( ax = ax , data = results_copy , x = 'spatial' , y = measure , hue = 'cmip' ) plt . title ( f '' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( f ' { measure . upper () } ' ) plt . legend ( ncol = 2 , bbox_to_anchor = ( 2.05 , 1 ), fontsize = 16 ) # plt.tight_layout() plt . show () giss_e2_r plot_global_diff_mi ( results_df , 'ncep' , 'mi' , True , True ) plot_global_diff_mi ( results_df , 'ncep' , 'pearson' , True , True ) plot_global_diff_mi ( results_df , 'ncep' , 'spearman' , True , True ) plot_global_diff_mi ( results_df , 'ncep' , 'kendelltau' , True , True )","title":"4.1 visualize global"},{"location":"notebooks/climate/amip/4.1_visualize_global/#visually-comparing-climate-models","text":"","title":"Visually Comparing Climate Models"},{"location":"notebooks/climate/amip/4.1_visualize_global/#summary","text":"In this notebook, I will be comparing three climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 CMIP5 I will be looking at the following variables: Mean Sea Level Pressure (CMIP5, ERA5, NCEP) Surface Pressure (ERA5, NCEP) I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures. If these climate models are that similar, then they should exhibit similar IT measures.","title":"Summary"},{"location":"notebooks/climate/amip/4.1_visualize_global/#preprocessing-steps","text":"Regridded Spatially The ERA5 had the coarsest spatial resolution (2.5 x 2.5). I regridded the NCEP from (0.25 x 0.25) to (2.5 x 2.5). I regridded the CMIP5 from (2 x 2.5) to (2.5 x 2.5). Temporal Resolution ERA5 and NCEP go from 1980-2019 CMIP5 goes from 2006-2018 For comparing ERA5 vs CMIP5 and NCEP vs CMIPF, I found the same time components","title":"Preprocessing Steps"},{"location":"notebooks/climate/amip/4.1_visualize_global/#measures","text":"I'm measuring the following: Entropy - expected uncertainty Total Correlation - amount of redundant information between features Mutual Information - amount of information shared between variables","title":"Measures"},{"location":"notebooks/climate/amip/4.1_visualize_global/#data","text":"Inputs I'm taking each year as is. Each spatial location is a sample and each year is a feature. My inputs are: X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} Outputs All my information theory measures are in nats. They are scalars.","title":"Data"},{"location":"notebooks/climate/amip/4.1_visualize_global/#hypothesis","text":"Simple: The ERA5 and the NCEP model should be more similar than the CMIP5 model compared to each of them.","title":"Hypothesis"},{"location":"notebooks/climate/amip/4.1_visualize_global/#data-climate-models","text":"import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) # from esdc.preprocessing import normalize_temporal import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing # Visualization Tools from src.data.climate.loader import ResultsLoader from src.visualization.climate.global_it import plot_global_entropy , plot_global_diff_entropy , plot_global_mutual_info , plot_global_diff_mutual_info # from src.visualization.climate import PlotResults import seaborn as sns import matplotlib.pyplot as plt # plt.style.use('ggplot') plt . style . use ([ 'seaborn-notebook' , 'fivethirtyeight' ]) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\"","title":"Data - Climate Models"},{"location":"notebooks/climate/amip/4.1_visualize_global/#experiment-i-global-entropy","text":"from pathlib import Path data_path = f \"/home/emmanuel/projects/projects/2020_rbig_rs/data/climate/results/amip/local/compare/\" def get_results_files ( base_model : str , trials : bool ) -> pd . DataFrame : path = Path ( data_path ) if base_model == 'ncep' : base_pattern = 'ncep' elif base_model == 'era5' : base_pattern = 'era5' else : raise ValueError ( 'Unrecognized base model:' , base_model ) trials_ext = 'tr100_v1' # if trials == True: # trials_ext = 'v1' # elif trials == False: # trials_ext = 'v2' # else: # raise ValueError(\"Unrecognized trials extentions:\", trials) filename_pattern = base_pattern + '*' + trials_ext + '.csv' df_from_each_file = [ pd . read_csv ( f , index_col = 0 ) for f in path . rglob ( filename_pattern )] results_df = pd . concat ( df_from_each_file , ignore_index = True ) return results_df def post_processing_compare ( df : pd . DataFrame ) -> pd . DataFrame : # divide by the spatial resolution df [ 'mi' ] = df [ 'mi' ] / ( df [ 'spatial' ] ** 2 ) return df ! ls / home / emmanuel / projects / 2020 _rbig_rs / data / climate / results / amip / compare_trial_v1.csv spatial_compare_v3.csv global spatial_compare_v4.csv global_compare_v2.csv spatial_compare_v5.csv global_compare_v3.csv spatial_individual_trial_v1.csv global_individual_v2.csv spatial_individual_v2.csv global_individual_v3.csv spatial_individual_v3.csv global_individual_v4.csv spatial_individual_v4.csv individual_trial_v1.csv spatial_individual_v5.csv local spatial_individual_v6.csv spatial_compare_trial_v1.csv v1.csv spatial_compare_v2.csv v2.csv","title":"Experiment I - Global Entropy"},{"location":"notebooks/climate/amip/4.1_visualize_global/#entropy","text":"df = pd . read_csv ( '/home/emmanuel/projects/2020_rbig_rs/data/climate/interim/amip/global/individual/ncep_bnu_esm_tr100_v1.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 base cmip h_base h_cmip spatial subsample t_base t_cmip tc_base tc_cmip trial variable 0 0 ncep bnu_esm 1.230567 1.290014 1.0 50000.0 1.301512 1.309189 0.0 0.0 0.0 psl 1 1 ncep bnu_esm 1.220025 1.284430 1.0 50000.0 1.288438 1.305716 0.0 0.0 1.0 psl 2 2 ncep bnu_esm 1.221952 1.291945 1.0 50000.0 1.292614 1.298469 0.0 0.0 2.0 psl 3 3 ncep bnu_esm 1.222894 1.289754 1.0 50000.0 1.295369 1.305580 0.0 0.0 3.0 psl 4 4 ncep bnu_esm 1.228588 1.290732 1.0 50000.0 1.302234 1.308249 0.0 0.0 4.0 psl data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/interim/amip/global/individual/\" results_df = get_results_files ( 'era5' , False ) results_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base cmip h_base h_cmip spatial subsample t_base t_cmip tc_base tc_cmip trial variable 0 era5 bcc_csm1_1 1.218948 1.314152 1.0 50000.0 1.496089 1.480152 0.0 0.0 0.0 psl 1 era5 bcc_csm1_1 1.208673 1.308177 1.0 50000.0 1.498710 1.504606 0.0 0.0 1.0 psl 2 era5 bcc_csm1_1 1.214323 1.305756 1.0 50000.0 1.498986 1.505039 0.0 0.0 2.0 psl 3 era5 bcc_csm1_1 1.215485 1.309741 1.0 50000.0 1.543764 1.508892 0.0 0.0 3.0 psl 4 era5 bcc_csm1_1 1.217624 1.315335 1.0 50000.0 1.544796 1.531270 0.0 0.0 4.0 psl results_df . cmip . unique () . tolist () ['bcc_csm1_1', 'access1_0', 'mpi_esm_lr', 'ipsl_cm5a_lr', 'noresm1_m', 'giss_e2_r', 'cnrm_cm5', 'bnu_esm'] SAVEPATH = '/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/amip/global/entropy/' def plot_global_entropy ( results_df : pd . DataFrame , base : str , cmip : str , normalized = True , log_mi = True , save = True ) -> None : if normalized == True : results_df [ 'h_cmip' ] = results_df [ 'h_cmip' ] / results_df [ 'spatial' ] ** 2 results_df [ 'h_base' ] = results_df [ 'h_base' ] / results_df [ 'spatial' ] ** 2 results_df = results_df [ results_df [ 'base' ] == base ] results_df = results_df [ results_df [ 'cmip' ] == cmip ] fig , ax = plt . subplots () sns . scatterplot ( data = results_df , x = 'spatial' , y = 'h_base' , label = f \" { base } \" , ax = ax ) sns . scatterplot ( data = results_df , x = 'spatial' , y = 'h_cmip' , label = f \" { cmip } \" , ax = ax ) ax . set_title ( f ' { base . upper () } vs CMIP: { cmip . upper () } ' ) ax . set_xlabel ( 'Spatial Features' ) # ax.set_xlim([2, 6]) ax . set_ylabel ( 'Entropy, H' ) ax . legend () if save : savename = f \"global_h_ { base } _ { cmip } .png\" fig . savefig ( SAVEPATH + savename , ) else : plt . show () return None","title":"Entropy"},{"location":"notebooks/climate/amip/4.1_visualize_global/#example","text":"results_df . cmip . unique () . tolist () ['bcc_csm1_1', 'access1_0', 'mpi_esm_lr', 'ipsl_cm5a_lr', 'noresm1_m', 'giss_e2_r', 'cnrm_cm5', 'bnu_esm'] results_df = get_results_files ( 'era5' , False ) plot_global_entropy ( results_df , 'era5' , 'ipsl_cm5a_lr' , normalized = True , save = True ) for imodel in results_df . cmip . unique () . tolist (): results_df = get_results_files ( 'era5' , False ) plot_global_entropy ( results_df , 'era5' , imodel , normalized = True , save = True ) # plot_global_entropy(results, 'ncep', 'inmcm4', normalized=False) for imodel in results_df . cmip . unique () . tolist (): results_df = get_results_files ( 'ncep' , False ) plot_global_entropy ( results_df , 'ncep' , imodel , normalized = True ,)","title":"Example"},{"location":"notebooks/climate/amip/4.1_visualize_global/#difference-in-entropy","text":"import numpy as np def plot_global_diff_entropy ( results_df : pd . DataFrame , base : str , normalized = True , log_mi = False ) -> None : results_copy = results_df [ results_df [ 'base' ] == base ] if normalized == True : results_copy [ 'h_base' ] = results_copy [ 'h_base' ] / results_copy [ 'spatial' ] ** 2 results_copy [ 'h_cmip' ] = results_copy [ 'h_cmip' ] / results_copy [ 'spatial' ] ** 2 results_copy = results_df [ results_df [ 'base' ] == base ] # calculate difference results_copy [ 'h_diff' ] = np . abs ( results_copy [ 'h_cmip' ] - results_copy [ 'h_base' ]) if log_mi == True : results_copy [ 'h_diff' ] = np . log ( results_copy [ 'h_diff' ]) fig , ax = plt . subplots () # sns.scatterplot(ax=ax, data=results_copy, x='spatial', y='h_diff', hue='base', color='black') sns . lineplot ( ax = ax , data = results_copy , x = 'spatial' , y = 'h_diff' , hue = 'cmip' ) plt . title ( f '' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( 'Entropy, H' ) plt . legend ( ncol = 2 , bbox_to_anchor = ( 2.05 , 1 ), fontsize = 16 ) # plt.tight_layout() plt . show () results_df = get_results_files ( 'era5' , False ) plot_global_diff_entropy ( results_df , 'era5' , normalized = True , save = True ) results_df = get_results_files ( 'ncep' , False ) plot_global_diff_entropy ( results_df , 'ncep' , normalized = True , save = True )","title":"Difference in Entropy"},{"location":"notebooks/climate/amip/4.1_visualize_global/#mutual-information","text":"from pathlib import Path data_path = f \"/home/emmanuel/projects/projects/2020_rbig_rs/data/climate/results/amip/global/compare/\" def get_results_files ( base_model : str , trials : bool ) -> pd . DataFrame : path = Path ( data_path ) if base_model == 'ncep' : base_pattern = 'ncep' elif base_model == 'era5' : base_pattern = 'era5' else : raise ValueError ( 'Unrecognized base model:' , base_model ) if trials == True : trials_ext = 'v1' elif trials == False : trials_ext = 'v2' else : raise ValueError ( \"Unrecognized trials extentions:\" , trials ) filename_pattern = base_pattern + '*' + trials_ext + '.csv' # [print(f) for f in path.rglob(filename_pattern)] # [print(pd.read_csv(f, index_col=0).columns.shape) for f in path.rglob(filename_pattern)] df_from_each_file = [ pd . read_csv ( f , index_col = 0 ) for f in path . rglob ( filename_pattern )] results_df = pd . concat ( df_from_each_file , ignore_index = True ) return results_df def post_processing_compare ( df : pd . DataFrame ) -> pd . DataFrame : # divide by the spatial resolution df [ 'mi' ] = df [ 'mi' ] / ( df [ 'spatial' ] ** 2 ) return df data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip/global/compare/\" results_df = get_results_files ( 'ncep' , True ) results_df . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base cmip kendelltau mi pearson spatial spearman subsample time_mi trial variable 4310 ncep bcc_csm1_1 0.003454 0.023890 0.000746 3.0 0.005176 50000.0 86.576983 5.0 psl 4311 ncep bcc_csm1_1 -0.003573 0.028074 -0.007091 3.0 -0.005361 50000.0 76.669153 6.0 psl 4312 ncep bcc_csm1_1 0.003528 0.027675 0.008871 3.0 0.005295 50000.0 85.649547 7.0 psl 4313 ncep bcc_csm1_1 0.004418 0.066879 0.012918 3.0 0.006640 50000.0 149.052748 8.0 psl 4314 ncep bcc_csm1_1 0.002212 0.025614 0.002713 3.0 0.003324 50000.0 100.513506 9.0 psl results_df . cmip . unique () . tolist () ['access1_0', 'ipsl_cm5a_lr', 'giss_e2_r', 'cnrm_cm5', 'mpi_esm_lr', 'inmcm4', 'bnu_esm', 'bcc_csm1_1_m', 'noresm1_m', 'bcc_csm1_1'] results_df = get_results_files ( 'ncep' , True ) plot_global_mutual_info ( results_df , base = 'ncep' , cmip = 'giss_e2_r' , measure = 'mi' , normalized = True , log_mi = True ) results_df = get_results_files ( 'ncep' , True ) plot_global_mutual_info ( results_df , base = 'ncep' , cmip = None , measure = 'mi' , normalized = True , ) results_df = get_results_files ( 'era5' , True ) plot_global_mutual_info ( results_df , base = 'era5' , cmip = None , measure = 'mi' , normalized = True , log_mi = True ) for imodel in results_df . cmip . unique () . tolist (): results_df = get_results_files ( 'ncep' , True ) plot_global_mutual_info ( results_df , base = 'ncep' , cmip = imodel , measure = 'mi' , normalized = True , log_mi = True ) for imodel in results_df . cmip . unique () . tolist (): results_df = get_results_files ( 'era5' , True ) plot_global_mutual_info ( results_df , base = 'era5' , cmip = imodel , measure = 'mi' , normalized = True , log_mi = False ) def plot_global_mi ( results_df : pd . DataFrame , base : str , cmip : str , measure = 'mi' , normalized = True ) -> None : results_df = results_df [ results_df [ 'base' ] == base ] results_df = results_df [ results_df [ 'cmip' ] == cmip ] if normalized and measure == 'mi' : results_df [ measure ] = results_df [ measure ] / results_df [ 'spatial' ] sns . scatterplot ( data = results_df , x = 'spatial' , y = measure , label = f \" { base } \" ) sns . scatterplot ( data = results_df , x = 'spatial' , y = measure , label = f \" { cmip } \" ) plt . title ( f ' { base . upper () } , CMIP: { cmip . upper () } ' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( f ' { measure . upper () } ' ) plt . legend () plt . show () plot_global_mi ( results_df , 'ncep' , 'access1_0' , 'mi' ) # plot_global_mi(results_df, 'era5', 'noresm1_m', 'pearson') # plot_global_mi(results_df, 'era5', 'noresm1_m', 'spearman') # plot_global_mi(results_df, 'era5', 'noresm1_m', 'kendelltau') plot_global_mi ( results_df , 'ncep' , 'giss_e2_r' , 'mi' ) plot_global_mi ( results_df , 'ncep' , 'giss_e2_r' , 'pearson' ) plot_global_mi ( results_df , 'ncep' , 'giss_e2_r' , 'spearman' ) plot_global_mi ( results_df , 'ncep' , 'giss_e2_r' , 'kendelltau' ) def plot_global_diff_mi ( results_df : pd . DataFrame , base : str , measure = 'mi' , normalized = True , log_mi = True ) -> None : results_copy = results_df [ results_df [ 'base' ] == base ] if normalized == True : results_copy [ measure ] = results_copy [ measure ] / results_copy [ 'spatial' ] if log_mi == True and measure == 'mi' : results_copy [ measure ] = np . log ( results_copy [ measure ]) fig , ax = plt . subplots () # sns.scatterplot(ax=ax, data=results_copy, x='spatial', y='h_diff', hue='base', color='black') sns . scatterplot ( ax = ax , data = results_copy , x = 'spatial' , y = measure , hue = 'cmip' ) plt . title ( f '' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( f ' { measure . upper () } ' ) plt . legend ( ncol = 2 , bbox_to_anchor = ( 2.05 , 1 ), fontsize = 16 ) # plt.tight_layout() plt . show () giss_e2_r plot_global_diff_mi ( results_df , 'ncep' , 'mi' , True , True ) plot_global_diff_mi ( results_df , 'ncep' , 'pearson' , True , True ) plot_global_diff_mi ( results_df , 'ncep' , 'spearman' , True , True ) plot_global_diff_mi ( results_df , 'ncep' , 'kendelltau' , True , True )","title":"Mutual Information"},{"location":"notebooks/climate/amip/4.2_visualize_spatial/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Visually Comparing Climate Models \u00b6 Summary \u00b6 In this notebook, I will be comparing three climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 CMIP5 I will be looking at the following variables: Mean Sea Level Pressure (CMIP5, ERA5, NCEP) Surface Pressure (ERA5, NCEP) I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures. If these climate models are that similar, then they should exhibit similar IT measures. Preprocessing Steps \u00b6 Regridded Spatially The ERA5 had the coarsest spatial resolution (2.5 x 2.5). I regridded the NCEP from (0.25 x 0.25) to (2.5 x 2.5). I regridded the CMIP5 from (2 x 2.5) to (2.5 x 2.5). Temporal Resolution ERA5 and NCEP go from 1980-2019 CMIP5 goes from 2006-2018 For comparing ERA5 vs CMIP5 and NCEP vs CMIPF, I found the same time components Measures \u00b6 I'm measuring the following: Entropy - expected uncertainty Total Correlation - amount of redundant information between features Mutual Information - amount of information shared between variables Data \u00b6 Inputs I'm taking each year as is. Each spatial location is a sample and each year is a feature. My inputs are: X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} Outputs All my information theory measures are in nats. They are scalars. Hypothesis \u00b6 Simple: The ERA5 and the NCEP model should be more similar than the CMIP5 model compared to each of them. Data - Climate Models \u00b6 import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) # from esdc.preprocessing import normalize_temporal import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing # Visualization Tools from src.data.climate.loader import ResultsLoader from src.visualization.climate import PlotResults import seaborn as sns import matplotlib.pyplot as plt # plt.style.use('ggplot') plt . style . use ([ 'seaborn-notebook' , 'fivethirtyeight' ]) % matplotlib inline % load_ext autoreload % autoreload 2 data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\" Results \u00b6 CMIP5 vs ERA5 vs NCEP \u00b6 ! ls \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip/\" compare_trial_v1.csv spatial_compare_v2.csv global_compare_v2.csv spatial_compare_v3.csv global_compare_v3.csv spatial_compare_v4.csv global_individual_v2.csv spatial_individual_trial_v1.csv global_individual_v3.csv spatial_individual_v2.csv global_individual_v4.csv spatial_individual_v3.csv individual_trial_v1.csv spatial_individual_v4.csv spatial_compare_trial_v1.csv results = pd . read_csv ( results_path + 'spatial_individual_v4' + '.csv' , index_col = [ 0 ]) columns = results . columns . tolist () results . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base base_time cmip cmip_time h_base h_cmip spatial subsample t_base t_cmip tc_base tc_cmip trial variable 20349 ncep 1999-07-01 inmcm4 1999-06-16 00:00:00 -77.516609 -60.457848 6.0 5000.0 21.345011 29.123130 120.900891 107.421496 6.0 psl 20350 ncep 1999-08-01 inmcm4 1999-07-16 12:00:00 -79.558294 -60.430310 6.0 5000.0 23.388674 31.234617 119.664003 108.125878 6.0 psl 20351 ncep 1999-09-01 inmcm4 1999-08-16 12:00:00 -77.975434 -62.943946 6.0 5000.0 20.472294 29.212621 117.276838 110.032793 6.0 psl 20352 ncep 1999-10-01 inmcm4 1999-09-16 00:00:00 -86.973429 -62.825051 6.0 5000.0 23.205492 39.465716 117.996714 107.935966 6.0 psl 20353 ncep 1999-11-01 inmcm4 1999-10-16 12:00:00 -82.277103 -66.988666 6.0 5000.0 26.059670 38.457328 120.735649 108.947965 6.0 psl # results['dates'] = pd.to_datetime(results.base_time) # results = results.groupby(results.dates.dt.year).mean().drop_duplicates() # results.head() def plot_spatial_entropy ( results_df : pd . DataFrame , base : str , cmip : str , spatial : int , normalize = True ) -> None : # choose spatial window size results_df = results_df [ results [ 'spatial' ] == spatial ] results_df = results_df [ results_df [ 'base' ] == base ] results_df = results_df [ results_df [ 'cmip' ] == cmip ] results_df [ 'dates' ] = pd . to_datetime ( results_df . base_time ) results_df = results_df . groupby ( results_df . dates . dt . year ) . transform ( 'mean' ) . drop_duplicates () # normalize if normalize : results_df [ 'h_base' ] = results_df [ 'h_base' ] / results_df [ 'spatial' ] results_df [ 'h_cmip' ] = results_df [ 'h_cmip' ] / results_df [ 'spatial' ] sns . scatterplot ( data = results_df , x = 'dates' , y = 'h_base' , label = f \" { base } \" ) sns . scatterplot ( data = results_df , x = 'dates' , y = 'h_cmip' , label = f \" { cmip } \" ) plt . title ( f ' { base . upper () } , CMIP: { cmip . upper () } , Spatial: { int ( spatial ) } ' ) plt . xlabel ( 'Years' ) plt . ylabel ( 'Entropy, H' ) plt . legend () plt . show () Example \u00b6 # plot_spatial_entropy(results, 'era5', 'inmcm4') plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 1.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 2.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 3.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 4.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 5.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 6.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 7.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 8.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 9.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 10.0 ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-9-2a6fd108413e> in <module> 6 plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 5.0 ) 7 plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 6.0 ) ----> 8 plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 7.0 ) 9 plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 8.0 ) 10 plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 9.0 ) <ipython-input-7-6a32b1ebac42> in plot_spatial_entropy (results_df, base, cmip, spatial, normalize) 7 8 results_df [ 'dates' ] = pd . to_datetime ( results_df . base_time ) ----> 9 results_df = results_df . groupby ( results_df . dates . dt . year ) . transform ( 'mean' ) . drop_duplicates ( ) 10 11 # normalize ~/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/pandas/core/groupby/generic.py in transform (self, func, *args, **kwargs) 592 # nuiscance columns 593 if not result . columns . equals ( obj . columns ) : --> 594 return self . _transform_general ( func , * args , ** kwargs ) 595 596 return self . _transform_fast ( result , obj , func ) ~/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/pandas/core/groupby/generic.py in _transform_general (self, func, *args, **kwargs) 564 concat_index = obj . columns if self . axis == 0 else obj . index 565 other_axis = 1 if self . axis == 0 else 0 # switches between 0 & 1 --> 566 concatenated = concat ( applied , axis = self . axis , verify_integrity = False ) 567 concatenated = concatenated . reindex ( concat_index , axis = other_axis , copy = False ) 568 return self . _set_result_index_ordered ( concatenated ) ~/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/pandas/core/reshape/concat.py in concat (objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy) 253 verify_integrity = verify_integrity , 254 copy = copy , --> 255 sort = sort , 256 ) 257 ~/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/pandas/core/reshape/concat.py in __init__ (self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort) 302 303 if len ( objs ) == 0 : --> 304 raise ValueError ( \"No objects to concatenate\" ) 305 306 if keys is None : ValueError : No objects to concatenate With Trials \u00b6 results = pd . read_csv ( results_path + 'spatial_individual_v2' + '.csv' , index_col = [ 0 ]) columns = results . columns . tolist () results . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base base_time cmip cmip_time h_base h_cmip spatial subsample t_base t_cmip tc_base tc_cmip trial variable 3604 ncep 1980-04-01 inmcm4 1980-03-16 12:00:00 -305.487809 -306.000709 2.0 100000.0 3131.432539 3195.500214 310.320863 310.573945 0.0 psl 3605 ncep 1980-05-01 inmcm4 1980-04-16 00:00:00 -305.418017 -306.468608 2.0 100000.0 3121.518930 3184.075421 310.309945 310.960205 0.0 psl 3606 ncep 1980-06-01 inmcm4 1980-05-16 12:00:00 -306.280263 -305.456243 2.0 100000.0 3131.040802 3193.071733 311.079259 309.870990 0.0 psl 3607 ncep 1980-07-01 inmcm4 1980-06-16 00:00:00 -304.751022 -306.033549 2.0 100000.0 3123.749256 3183.194890 309.773832 310.637945 0.0 psl 3608 ncep 1980-08-01 inmcm4 1980-07-16 12:00:00 -305.726799 -306.364675 2.0 100000.0 3131.382549 3194.702803 310.502920 311.400503 0.0 psl # plot_spatial_entropy(results, 'era5', 'inmcm4') plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 1.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 2.0 ) plot_global_entropy ( results , 'era5' , 'access1_0' ) plot_global_entropy ( results , 'ncep' , 'access1_0' ) plot_global_entropy ( results , 'era5' , 'bcc_csm1_1' ) plot_global_entropy ( results , 'ncep' , 'bcc_csm1_1' ) Difference in Entropy \u00b6 import numpy as np def plot_global_diff_entropy ( results_df : pd . DataFrame , base : str , normalized = True ) -> None : results_copy = results_df [ results_df [ 'base' ] == base ] # calculate difference results_copy [ 'h_diff' ] = np . abs ( results_copy [ 'h_cmip' ] - results_copy [ 'h_base' ]) if normalized == True : results_copy [ 'h_diff' ] = results_copy [ 'h_diff' ] / results_copy [ 'spatial' ] fig , ax = plt . subplots () # sns.scatterplot(ax=ax, data=results_copy, x='spatial', y='h_diff', hue='base', color='black') sns . scatterplot ( ax = ax , data = results_copy , x = 'spatial' , y = 'h_diff' , hue = 'cmip' ) plt . title ( f '' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( 'Entropy, H' ) plt . legend ( ncol = 2 , bbox_to_anchor = ( 2.05 , 1 ), fontsize = 16 ) # plt.tight_layout() plt . show () plot_global_diff_entropy ( results , 'era5' ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy if __name__ == '__main__': /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy if sys.path[0] == '': Total Correlation \u00b6 def plot_global_tc ( results_df : pd . DataFrame , base : str , cmip : str ) -> None : results_df = results_df [ results_df [ 'base' ] == base ] results_df = results_df [ results_df [ 'cmip' ] == cmip ] sns . scatterplot ( data = results_df , x = 'spatial' , y = 'tc_base' , label = f \" { base } \" ) sns . scatterplot ( data = results_df , x = 'spatial' , y = 'tc_cmip' , label = f \" { cmip } \" ) plt . title ( f ' { base . upper () } , CMIP: { cmip . upper () } ' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( 'Total Correlation, TC' ) plt . legend () plt . show () plot_global_tc ( results , 'era5' , 'bcc_csm1_1' ) plot_global_tc ( results , 'ncep' , 'bcc_csm1_1' ) def plot_global_diff_tc ( results_df : pd . DataFrame , base : str , normalized = True ) -> None : results_copy = results_df [ results_df [ 'base' ] == base ] # calculate difference results_copy [ 'tc_diff' ] = np . abs ( results_copy [ 'tc_cmip' ] - results_copy [ 'tc_base' ]) if normalized == True : results_copy [ 'tc_diff' ] = results_copy [ 'tc_diff' ] / results_copy [ 'spatial' ] fig , ax = plt . subplots () # sns.scatterplot(ax=ax, data=results_copy, x='spatial', y='h_diff', hue='base', color='black') sns . scatterplot ( ax = ax , data = results_copy , x = 'spatial' , y = 'tc_diff' , hue = 'cmip' ) plt . title ( f '' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( 'Total Correlation, TC' ) plt . legend ( ncol = 2 , bbox_to_anchor = ( 2.05 , 1 ), fontsize = 16 ) # plt.tight_layout() plt . show () plot_global_diff_tc ( results , 'era5' , normalized = True ) plot_global_diff_tc ( results , 'ncep' , normalized = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy import sys /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy # Remove the CWD from sys.path while we load stuff. Mutual Information \u00b6 results = pd . read_csv ( results_path + 'compare_trial_v1' + '.csv' , index_col = [ 0 ]) results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base cmip kendelltau mi pearson spatial spearman subsample time_mi variable 0 ncep inmcm4 0.552030 9.962479 0.833456 1.0 0.711300 10000.0 39.619016 psl 1 ncep inmcm4 0.568794 1.489607 0.842241 2.0 0.730925 10000.0 10.354173 psl 2 ncep inmcm4 0.581166 1.555495 0.844528 3.0 0.745527 10000.0 65.894330 psl 3 ncep inmcm4 0.562111 1.463338 0.803643 4.0 0.715848 10000.0 96.675056 psl 4 ncep inmcm4 0.529905 1.766060 0.751060 5.0 0.670227 10000.0 73.076941 psl def plot_global_mi ( results_df : pd . DataFrame , base : str , cmip : str , measure = 'mi' , normalized = True ) -> None : results_df = results_df [ results_df [ 'base' ] == base ] results_df = results_df [ results_df [ 'cmip' ] == cmip ] if normalized and measure == 'mi' : results_df [ measure ] = results_df [ measure ] / results_df [ 'spatial' ] sns . scatterplot ( data = results_df , x = 'spatial' , y = measure , label = f \" { base } \" ) sns . scatterplot ( data = results_df , x = 'spatial' , y = measure , label = f \" { cmip } \" ) plt . title ( f ' { base . upper () } , CMIP: { cmip . upper () } ' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( 'Mutual Information, MI' ) plt . legend () plt . show () plot_global_mi ( results , 'era5' , 'inmcm4' , 'mi' ) plot_global_mi ( results , 'era5' , 'inmcm4' , 'pearson' ) plot_global_mi ( results , 'era5' , 'inmcm4' , 'spearman' ) plot_global_mi ( results , 'era5' , 'inmcm4' , 'kendelltau' ) plot_global_mi ( results , 'era5' , 'noresm1_m' , 'mi' ) plot_global_mi ( results , 'era5' , 'noresm1_m' , 'pearson' ) plot_global_mi ( results , 'era5' , 'noresm1_m' , 'spearman' ) plot_global_mi ( results , 'era5' , 'noresm1_m' , 'kendelltau' ) def plot_global_diff_mi ( results_df : pd . DataFrame , base : str , measure = 'mi' , normalized = True , log_mi = True ) -> None : results_copy = results_df [ results_df [ 'base' ] == base ] if normalized == True : results_copy [ measure ] = results_copy [ measure ] / results_copy [ 'spatial' ] if log_mi == True and measure == 'mi' : results_copy [ measure ] = np . log ( results_copy [ measure ]) fig , ax = plt . subplots () # sns.scatterplot(ax=ax, data=results_copy, x='spatial', y='h_diff', hue='base', color='black') sns . scatterplot ( ax = ax , data = results_copy , x = 'spatial' , y = measure , hue = 'cmip' ) plt . title ( f '' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( f ' { measure . upper () } ' ) plt . legend ( ncol = 2 , bbox_to_anchor = ( 2.05 , 1 ), fontsize = 16 ) # plt.tight_layout() plt . show () plot_global_diff_mi ( results , 'era5' , 'mi' , True ) plot_global_diff_mi ( results , 'era5' , 'pearson' , True , False ) plot_global_diff_mi ( results , 'era5' , 'spearman' , True , False ) plot_global_diff_mi ( results , 'era5' , 'kendelltau' , True , False ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy # This is added back by InteractiveShellApp.init_path()","title":"4.2 visualize spatial"},{"location":"notebooks/climate/amip/4.2_visualize_spatial/#visually-comparing-climate-models","text":"","title":"Visually Comparing Climate Models"},{"location":"notebooks/climate/amip/4.2_visualize_spatial/#summary","text":"In this notebook, I will be comparing three climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 CMIP5 I will be looking at the following variables: Mean Sea Level Pressure (CMIP5, ERA5, NCEP) Surface Pressure (ERA5, NCEP) I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures. If these climate models are that similar, then they should exhibit similar IT measures.","title":"Summary"},{"location":"notebooks/climate/amip/4.2_visualize_spatial/#preprocessing-steps","text":"Regridded Spatially The ERA5 had the coarsest spatial resolution (2.5 x 2.5). I regridded the NCEP from (0.25 x 0.25) to (2.5 x 2.5). I regridded the CMIP5 from (2 x 2.5) to (2.5 x 2.5). Temporal Resolution ERA5 and NCEP go from 1980-2019 CMIP5 goes from 2006-2018 For comparing ERA5 vs CMIP5 and NCEP vs CMIPF, I found the same time components","title":"Preprocessing Steps"},{"location":"notebooks/climate/amip/4.2_visualize_spatial/#measures","text":"I'm measuring the following: Entropy - expected uncertainty Total Correlation - amount of redundant information between features Mutual Information - amount of information shared between variables","title":"Measures"},{"location":"notebooks/climate/amip/4.2_visualize_spatial/#data","text":"Inputs I'm taking each year as is. Each spatial location is a sample and each year is a feature. My inputs are: X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} Outputs All my information theory measures are in nats. They are scalars.","title":"Data"},{"location":"notebooks/climate/amip/4.2_visualize_spatial/#hypothesis","text":"Simple: The ERA5 and the NCEP model should be more similar than the CMIP5 model compared to each of them.","title":"Hypothesis"},{"location":"notebooks/climate/amip/4.2_visualize_spatial/#data-climate-models","text":"import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) # from esdc.preprocessing import normalize_temporal import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing # Visualization Tools from src.data.climate.loader import ResultsLoader from src.visualization.climate import PlotResults import seaborn as sns import matplotlib.pyplot as plt # plt.style.use('ggplot') plt . style . use ([ 'seaborn-notebook' , 'fivethirtyeight' ]) % matplotlib inline % load_ext autoreload % autoreload 2 data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\"","title":"Data - Climate Models"},{"location":"notebooks/climate/amip/4.2_visualize_spatial/#results","text":"","title":"Results"},{"location":"notebooks/climate/amip/4.2_visualize_spatial/#cmip5-vs-era5-vs-ncep","text":"! ls \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip/\" compare_trial_v1.csv spatial_compare_v2.csv global_compare_v2.csv spatial_compare_v3.csv global_compare_v3.csv spatial_compare_v4.csv global_individual_v2.csv spatial_individual_trial_v1.csv global_individual_v3.csv spatial_individual_v2.csv global_individual_v4.csv spatial_individual_v3.csv individual_trial_v1.csv spatial_individual_v4.csv spatial_compare_trial_v1.csv results = pd . read_csv ( results_path + 'spatial_individual_v4' + '.csv' , index_col = [ 0 ]) columns = results . columns . tolist () results . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base base_time cmip cmip_time h_base h_cmip spatial subsample t_base t_cmip tc_base tc_cmip trial variable 20349 ncep 1999-07-01 inmcm4 1999-06-16 00:00:00 -77.516609 -60.457848 6.0 5000.0 21.345011 29.123130 120.900891 107.421496 6.0 psl 20350 ncep 1999-08-01 inmcm4 1999-07-16 12:00:00 -79.558294 -60.430310 6.0 5000.0 23.388674 31.234617 119.664003 108.125878 6.0 psl 20351 ncep 1999-09-01 inmcm4 1999-08-16 12:00:00 -77.975434 -62.943946 6.0 5000.0 20.472294 29.212621 117.276838 110.032793 6.0 psl 20352 ncep 1999-10-01 inmcm4 1999-09-16 00:00:00 -86.973429 -62.825051 6.0 5000.0 23.205492 39.465716 117.996714 107.935966 6.0 psl 20353 ncep 1999-11-01 inmcm4 1999-10-16 12:00:00 -82.277103 -66.988666 6.0 5000.0 26.059670 38.457328 120.735649 108.947965 6.0 psl # results['dates'] = pd.to_datetime(results.base_time) # results = results.groupby(results.dates.dt.year).mean().drop_duplicates() # results.head() def plot_spatial_entropy ( results_df : pd . DataFrame , base : str , cmip : str , spatial : int , normalize = True ) -> None : # choose spatial window size results_df = results_df [ results [ 'spatial' ] == spatial ] results_df = results_df [ results_df [ 'base' ] == base ] results_df = results_df [ results_df [ 'cmip' ] == cmip ] results_df [ 'dates' ] = pd . to_datetime ( results_df . base_time ) results_df = results_df . groupby ( results_df . dates . dt . year ) . transform ( 'mean' ) . drop_duplicates () # normalize if normalize : results_df [ 'h_base' ] = results_df [ 'h_base' ] / results_df [ 'spatial' ] results_df [ 'h_cmip' ] = results_df [ 'h_cmip' ] / results_df [ 'spatial' ] sns . scatterplot ( data = results_df , x = 'dates' , y = 'h_base' , label = f \" { base } \" ) sns . scatterplot ( data = results_df , x = 'dates' , y = 'h_cmip' , label = f \" { cmip } \" ) plt . title ( f ' { base . upper () } , CMIP: { cmip . upper () } , Spatial: { int ( spatial ) } ' ) plt . xlabel ( 'Years' ) plt . ylabel ( 'Entropy, H' ) plt . legend () plt . show ()","title":"CMIP5 vs ERA5 vs NCEP"},{"location":"notebooks/climate/amip/4.2_visualize_spatial/#example","text":"# plot_spatial_entropy(results, 'era5', 'inmcm4') plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 1.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 2.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 3.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 4.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 5.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 6.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 7.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 8.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 9.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 10.0 ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-9-2a6fd108413e> in <module> 6 plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 5.0 ) 7 plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 6.0 ) ----> 8 plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 7.0 ) 9 plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 8.0 ) 10 plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 9.0 ) <ipython-input-7-6a32b1ebac42> in plot_spatial_entropy (results_df, base, cmip, spatial, normalize) 7 8 results_df [ 'dates' ] = pd . to_datetime ( results_df . base_time ) ----> 9 results_df = results_df . groupby ( results_df . dates . dt . year ) . transform ( 'mean' ) . drop_duplicates ( ) 10 11 # normalize ~/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/pandas/core/groupby/generic.py in transform (self, func, *args, **kwargs) 592 # nuiscance columns 593 if not result . columns . equals ( obj . columns ) : --> 594 return self . _transform_general ( func , * args , ** kwargs ) 595 596 return self . _transform_fast ( result , obj , func ) ~/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/pandas/core/groupby/generic.py in _transform_general (self, func, *args, **kwargs) 564 concat_index = obj . columns if self . axis == 0 else obj . index 565 other_axis = 1 if self . axis == 0 else 0 # switches between 0 & 1 --> 566 concatenated = concat ( applied , axis = self . axis , verify_integrity = False ) 567 concatenated = concatenated . reindex ( concat_index , axis = other_axis , copy = False ) 568 return self . _set_result_index_ordered ( concatenated ) ~/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/pandas/core/reshape/concat.py in concat (objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy) 253 verify_integrity = verify_integrity , 254 copy = copy , --> 255 sort = sort , 256 ) 257 ~/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/pandas/core/reshape/concat.py in __init__ (self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort) 302 303 if len ( objs ) == 0 : --> 304 raise ValueError ( \"No objects to concatenate\" ) 305 306 if keys is None : ValueError : No objects to concatenate","title":"Example"},{"location":"notebooks/climate/amip/4.2_visualize_spatial/#with-trials","text":"results = pd . read_csv ( results_path + 'spatial_individual_v2' + '.csv' , index_col = [ 0 ]) columns = results . columns . tolist () results . tail () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base base_time cmip cmip_time h_base h_cmip spatial subsample t_base t_cmip tc_base tc_cmip trial variable 3604 ncep 1980-04-01 inmcm4 1980-03-16 12:00:00 -305.487809 -306.000709 2.0 100000.0 3131.432539 3195.500214 310.320863 310.573945 0.0 psl 3605 ncep 1980-05-01 inmcm4 1980-04-16 00:00:00 -305.418017 -306.468608 2.0 100000.0 3121.518930 3184.075421 310.309945 310.960205 0.0 psl 3606 ncep 1980-06-01 inmcm4 1980-05-16 12:00:00 -306.280263 -305.456243 2.0 100000.0 3131.040802 3193.071733 311.079259 309.870990 0.0 psl 3607 ncep 1980-07-01 inmcm4 1980-06-16 00:00:00 -304.751022 -306.033549 2.0 100000.0 3123.749256 3183.194890 309.773832 310.637945 0.0 psl 3608 ncep 1980-08-01 inmcm4 1980-07-16 12:00:00 -305.726799 -306.364675 2.0 100000.0 3131.382549 3194.702803 310.502920 311.400503 0.0 psl # plot_spatial_entropy(results, 'era5', 'inmcm4') plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 1.0 ) plot_spatial_entropy ( results , 'ncep' , 'inmcm4' , 2.0 ) plot_global_entropy ( results , 'era5' , 'access1_0' ) plot_global_entropy ( results , 'ncep' , 'access1_0' ) plot_global_entropy ( results , 'era5' , 'bcc_csm1_1' ) plot_global_entropy ( results , 'ncep' , 'bcc_csm1_1' )","title":"With Trials"},{"location":"notebooks/climate/amip/4.2_visualize_spatial/#difference-in-entropy","text":"import numpy as np def plot_global_diff_entropy ( results_df : pd . DataFrame , base : str , normalized = True ) -> None : results_copy = results_df [ results_df [ 'base' ] == base ] # calculate difference results_copy [ 'h_diff' ] = np . abs ( results_copy [ 'h_cmip' ] - results_copy [ 'h_base' ]) if normalized == True : results_copy [ 'h_diff' ] = results_copy [ 'h_diff' ] / results_copy [ 'spatial' ] fig , ax = plt . subplots () # sns.scatterplot(ax=ax, data=results_copy, x='spatial', y='h_diff', hue='base', color='black') sns . scatterplot ( ax = ax , data = results_copy , x = 'spatial' , y = 'h_diff' , hue = 'cmip' ) plt . title ( f '' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( 'Entropy, H' ) plt . legend ( ncol = 2 , bbox_to_anchor = ( 2.05 , 1 ), fontsize = 16 ) # plt.tight_layout() plt . show () plot_global_diff_entropy ( results , 'era5' ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy if __name__ == '__main__': /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy if sys.path[0] == '':","title":"Difference in Entropy"},{"location":"notebooks/climate/amip/4.2_visualize_spatial/#total-correlation","text":"def plot_global_tc ( results_df : pd . DataFrame , base : str , cmip : str ) -> None : results_df = results_df [ results_df [ 'base' ] == base ] results_df = results_df [ results_df [ 'cmip' ] == cmip ] sns . scatterplot ( data = results_df , x = 'spatial' , y = 'tc_base' , label = f \" { base } \" ) sns . scatterplot ( data = results_df , x = 'spatial' , y = 'tc_cmip' , label = f \" { cmip } \" ) plt . title ( f ' { base . upper () } , CMIP: { cmip . upper () } ' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( 'Total Correlation, TC' ) plt . legend () plt . show () plot_global_tc ( results , 'era5' , 'bcc_csm1_1' ) plot_global_tc ( results , 'ncep' , 'bcc_csm1_1' ) def plot_global_diff_tc ( results_df : pd . DataFrame , base : str , normalized = True ) -> None : results_copy = results_df [ results_df [ 'base' ] == base ] # calculate difference results_copy [ 'tc_diff' ] = np . abs ( results_copy [ 'tc_cmip' ] - results_copy [ 'tc_base' ]) if normalized == True : results_copy [ 'tc_diff' ] = results_copy [ 'tc_diff' ] / results_copy [ 'spatial' ] fig , ax = plt . subplots () # sns.scatterplot(ax=ax, data=results_copy, x='spatial', y='h_diff', hue='base', color='black') sns . scatterplot ( ax = ax , data = results_copy , x = 'spatial' , y = 'tc_diff' , hue = 'cmip' ) plt . title ( f '' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( 'Total Correlation, TC' ) plt . legend ( ncol = 2 , bbox_to_anchor = ( 2.05 , 1 ), fontsize = 16 ) # plt.tight_layout() plt . show () plot_global_diff_tc ( results , 'era5' , normalized = True ) plot_global_diff_tc ( results , 'ncep' , normalized = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy import sys /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy # Remove the CWD from sys.path while we load stuff.","title":"Total Correlation"},{"location":"notebooks/climate/amip/4.2_visualize_spatial/#mutual-information","text":"results = pd . read_csv ( results_path + 'compare_trial_v1' + '.csv' , index_col = [ 0 ]) results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base cmip kendelltau mi pearson spatial spearman subsample time_mi variable 0 ncep inmcm4 0.552030 9.962479 0.833456 1.0 0.711300 10000.0 39.619016 psl 1 ncep inmcm4 0.568794 1.489607 0.842241 2.0 0.730925 10000.0 10.354173 psl 2 ncep inmcm4 0.581166 1.555495 0.844528 3.0 0.745527 10000.0 65.894330 psl 3 ncep inmcm4 0.562111 1.463338 0.803643 4.0 0.715848 10000.0 96.675056 psl 4 ncep inmcm4 0.529905 1.766060 0.751060 5.0 0.670227 10000.0 73.076941 psl def plot_global_mi ( results_df : pd . DataFrame , base : str , cmip : str , measure = 'mi' , normalized = True ) -> None : results_df = results_df [ results_df [ 'base' ] == base ] results_df = results_df [ results_df [ 'cmip' ] == cmip ] if normalized and measure == 'mi' : results_df [ measure ] = results_df [ measure ] / results_df [ 'spatial' ] sns . scatterplot ( data = results_df , x = 'spatial' , y = measure , label = f \" { base } \" ) sns . scatterplot ( data = results_df , x = 'spatial' , y = measure , label = f \" { cmip } \" ) plt . title ( f ' { base . upper () } , CMIP: { cmip . upper () } ' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( 'Mutual Information, MI' ) plt . legend () plt . show () plot_global_mi ( results , 'era5' , 'inmcm4' , 'mi' ) plot_global_mi ( results , 'era5' , 'inmcm4' , 'pearson' ) plot_global_mi ( results , 'era5' , 'inmcm4' , 'spearman' ) plot_global_mi ( results , 'era5' , 'inmcm4' , 'kendelltau' ) plot_global_mi ( results , 'era5' , 'noresm1_m' , 'mi' ) plot_global_mi ( results , 'era5' , 'noresm1_m' , 'pearson' ) plot_global_mi ( results , 'era5' , 'noresm1_m' , 'spearman' ) plot_global_mi ( results , 'era5' , 'noresm1_m' , 'kendelltau' ) def plot_global_diff_mi ( results_df : pd . DataFrame , base : str , measure = 'mi' , normalized = True , log_mi = True ) -> None : results_copy = results_df [ results_df [ 'base' ] == base ] if normalized == True : results_copy [ measure ] = results_copy [ measure ] / results_copy [ 'spatial' ] if log_mi == True and measure == 'mi' : results_copy [ measure ] = np . log ( results_copy [ measure ]) fig , ax = plt . subplots () # sns.scatterplot(ax=ax, data=results_copy, x='spatial', y='h_diff', hue='base', color='black') sns . scatterplot ( ax = ax , data = results_copy , x = 'spatial' , y = measure , hue = 'cmip' ) plt . title ( f '' ) plt . xlabel ( 'Spatial Features' ) plt . ylabel ( f ' { measure . upper () } ' ) plt . legend ( ncol = 2 , bbox_to_anchor = ( 2.05 , 1 ), fontsize = 16 ) # plt.tight_layout() plt . show () plot_global_diff_mi ( results , 'era5' , 'mi' , True ) plot_global_diff_mi ( results , 'era5' , 'pearson' , True , False ) plot_global_diff_mi ( results , 'era5' , 'spearman' , True , False ) plot_global_diff_mi ( results , 'era5' , 'kendelltau' , True , False ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy # This is added back by InteractiveShellApp.init_path()","title":"Mutual Information"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Visually Comparing Climate Models \u00b6 Summary \u00b6 In this notebook, I will be comparing three climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 CMIP5 I will be looking at the following variables: Mean Sea Level Pressure (CMIP5, ERA5, NCEP) Surface Pressure (ERA5, NCEP) I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures. If these climate models are that similar, then they should exhibit similar IT measures. Preprocessing Steps \u00b6 Regridded Spatially The ERA5 had the coarsest spatial resolution (2.5 x 2.5). I regridded the NCEP from (0.25 x 0.25) to (2.5 x 2.5). I regridded the CMIP5 from (2 x 2.5) to (2.5 x 2.5). Temporal Resolution ERA5 and NCEP go from 1980-2019 CMIP5 goes from 2006-2018 For comparing ERA5 vs CMIP5 and NCEP vs CMIPF, I found the same time components Measures \u00b6 I'm measuring the following: Entropy - expected uncertainty Total Correlation - amount of redundant information between features Mutual Information - amount of information shared between variables Data \u00b6 Inputs I'm taking each year as is. Each spatial location is a sample and each year is a feature. My inputs are: X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} Outputs All my information theory measures are in nats. They are scalars. Hypothesis \u00b6 Simple: The ERA5 and the NCEP model should be more similar than the CMIP5 model compared to each of them. Data - Climate Models \u00b6 ! ls / home / emmanuel / projects / 2020 _rbig_rs / notebooks / climate /../../ LICENSE data environment_dev.yml notebooks reports src README.md environment.yml environment_gpu.yml references setup.py import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) # Import RBIG Helper # from src.models.train_models import run_rbig_models # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) # from esdc.preprocessing import normalize_temporal from pathlib import Path import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing # Visualization Tools from src.data.climate.loader import ResultsLoader # from src.visualization.climate import PlotResults from src.visualization.climate.local import plot_diff , plot_individual , plot_individual_diff , plot_individual_all import seaborn as sns import matplotlib.pyplot as plt # plt.style.use('ggplot') plt . style . use ([ 'seaborn-poster' , 'fivethirtyeight' ]) % matplotlib inline % load_ext autoreload % autoreload 2 Entropy \u00b6 def get_entropy_results ( base_model : str , results : str = 'interim' , model : str = 'rcp' ) -> pd . DataFrame : base_path = '/home/emmanuel/projects/2020_rbig_rs/data/climate/' # CHECK: RCP or AMIP if model == 'rcp' : data_path = f \"/rcp/local/individual/\" ext = 'v3' elif model == 'amip' : data_path = f \"/amip/local/individual/\" ext = 'v4' else : raise ValueError ( 'Unrecognized model: ' , model ) # print(base_path + results + data_path) path = Path ( base_path + results + data_path ) if base_model == 'ncep' : filename_pattern = 'ncep_*_' elif base_model == 'era5' : filename_pattern = 'era5_*_' else : raise ValueError ( 'Unrecognized base model:' , base_model ) df_from_each_file = [ pd . read_csv ( f , index_col = 0 ) for f in path . rglob ( filename_pattern + ext + '.csv' )] results_df = pd . concat ( df_from_each_file , ignore_index = True ) return results_df from typing import List def post_processing_entropy ( df : pd . DataFrame , model : str = 'amip' , exclude : List [ str ] = [ 'inmcm4' ], normalized : bool = False ) -> pd . DataFrame : # exclude models df = df [ ~ df [ 'cmip' ] . isin ( exclude )] # divide by the spatial resolution if model == 'amip' : banned_cmip_dates = [ '1979-01-16 12:00:00' , '2009-01-16 12:00:00' , '2010-01-16 12:00:00' ] banned_base_dates = [ '1979-01-01' , '2010-01-01' ] banned_out_dates = [ '2006-02-01' , '2011-01-01' , '2012-01-01' , '2013-01-01' , '2014-01-01' , '2015-01-01' , '2016-01-01' , '2017-01-01' , '2018-01-01' , '2019-01-01' ] elif model == 'rcp' : banned_cmip_dates = [] banned_base_dates = [] banned_out_dates = [] elif model == 'none' : banned_out_dates = [] banned_cmip_dates = [] banned_base_dates = [] else : raise ValueError ( 'Unrecognized dataset' ) # divide by the spatial resolution if normalized : df [ 'h_base' ] = df [ 'h_base' ] / ( df [ 'spatial' ] ** 2 ) df [ 'h_cmip' ] = df [ 'h_cmip' ] / ( df [ 'spatial' ] ** 2 ) df = df [ ~ df . cmip_time . isin ( banned_cmip_dates )] df = df [ ~ df . base_time . isin ( banned_base_dates )] df = df [ ~ df . base_time . isin ( banned_out_dates )] return df NCEP - Individual IT Measures \u00b6 data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/rcp/local/individual/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/rcp/local/individual/\" # extract results results_df = get_entropy_results ( 'ncep' , 'interim' , 'amip' ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' ] results_df = post_processing_entropy ( results_df , exclude = exclude ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy results_df . cmip . unique () . tolist () ['mpi_esm_lr', 'noresm1_m', 'mpi_esm_mr', 'access1_0', 'ipsl_cm5a_mr'] results_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base base_time cmip cmip_time h_base h_cmip spatial subsample t_base t_cmip tc_base tc_cmip trial variable 0 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.173002 1.263623 1.0 50000.0 1.319296 1.451097 0.0 0.0 0.0 psl 1 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.177404 1.269881 1.0 50000.0 1.351144 1.327829 0.0 0.0 1.0 psl 2 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.186812 1.277439 1.0 50000.0 1.282283 1.448870 0.0 0.0 2.0 psl 3 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.183485 1.273517 1.0 50000.0 1.317563 1.319149 0.0 0.0 3.0 psl 4 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.193985 1.280803 1.0 50000.0 1.315317 1.527269 0.0 0.0 4.0 psl results_df [ results_df [ 'cmip' ] == 'inmcm4' ] . unique () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base base_time cmip cmip_time h_base h_cmip spatial subsample t_base t_cmip tc_base tc_cmip trial variable 0 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.173002 1.263623 1.0 50000.0 1.319296 1.451097 0.0 0.0 0.0 psl 1 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.177404 1.269881 1.0 50000.0 1.351144 1.327829 0.0 0.0 1.0 psl 2 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.186812 1.277439 1.0 50000.0 1.282283 1.448870 0.0 0.0 2.0 psl 3 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.183485 1.273517 1.0 50000.0 1.317563 1.319149 0.0 0.0 3.0 psl 4 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.193985 1.280803 1.0 50000.0 1.315317 1.527269 0.0 0.0 4.0 psl AMIP \u00b6 # extract results base_model = 'ncep' exp = 'amip' results_df = get_entropy_results ( base_model , results = 'results' , model = exp ) # post processing results_df = post_processing ( results_df , 'amip' ) results_df [ results_df [ 'cmip' ] == 'inmcm4' ] . base_time . unique () . tolist () ['2006-02-01', '2007-01-01', '2008-01-01', '2011-01-01', '2012-01-01', '2013-01-01', '2014-01-01', '2015-01-01', '2016-01-01', '2017-01-01', '2018-01-01', '2019-01-01'] results_df . base_time . unique () . tolist () ['1980-01-01', '1981-01-01', '1982-01-01', '1983-01-01', '1984-01-01', '1985-01-01', '1986-01-01', '1987-01-01', '1988-01-01', '1989-01-01', '1990-01-01', '1991-01-01', '1992-01-01', '1993-01-01', '1994-01-01', '1995-01-01', '1996-01-01', '1997-01-01', '1998-01-01', '1999-01-01', '2000-01-01', '2001-01-01', '2002-01-01', '2003-01-01', '2004-01-01', '2005-01-01', '2006-01-01', '2007-01-01', '2008-01-01'] # extract results base_model = 'ncep' exp = 'amip' results_df = get_entropy_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' ] normalized = True results_df = post_processing_entropy ( results_df , model = exp , exclude = exclude , normalized = normalized ) for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , ispatial , model = exp , save = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy RCP \u00b6 We want to extract the different CMIP models. The other parameters are constant for now. # extract results base_model = 'ncep' exp = 'rcp' results_df = get_entropy_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' , 'access1_3' ] normalized = True results_df = post_processing_entropy ( results_df , model = exp , exclude = exclude , normalized = normalized ) for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , ispatial , model = exp , save = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy ERA5 - Individual IT Measures \u00b6 AMIP \u00b6 # extract results base_model = 'era5' exp = 'amip' results_df = get_entropy_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' ] results_df = post_processing_entropy ( results_df , model = exp , exclude = exclude ) for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , ispatial , model = exp , save = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy RCP \u00b6 # extract results base_model = 'era5' exp = 'rcp' results_df = get_entropy_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' , 'access1_3' ] results_df = post_processing_entropy ( results_df , model = exp , exclude = exclude ) for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , ispatial , model = exp , save = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy Mutual Information \u00b6 def get_mutual_info_results ( base_model : str , results : str = 'interim' , model : str = 'rcp' ) -> pd . DataFrame : base_path = '/home/emmanuel/projects/2020_rbig_rs/data/climate/' # CHECK: RCP or AMIP if model == 'rcp' : data_path = f \"/rcp/local/compare/\" elif model == 'amip' : data_path = f \"/amip/local/compare/\" else : raise ValueError ( 'Unrecognized model: ' , model ) # print(base_path + results + data_path) path = Path ( base_path + results + data_path ) if base_model == 'ncep' : filename_pattern = 'ncep_*_v3.csv' elif base_model == 'era5' : filename_pattern = 'era5_*_v3.csv' else : raise ValueError ( 'Unrecognized base model:' , base_model ) df_from_each_file = [ pd . read_csv ( f , index_col = 0 ) for f in path . rglob ( filename_pattern )] results_df = pd . concat ( df_from_each_file , ignore_index = True ) return results_df # def get_results_files_mi(base_model: str)-> pd.DataFrame: # path = Path(data_path) # if base_model == 'ncep': # filename_pattern = 'ncep_*_v3.csv' # elif base_model == 'era5': # filename_pattern = 'era5_*_v3.csv' # else: # raise ValueError('Unrecognized base model:', base_model) # df_from_each_file = [pd.read_csv(f, index_col=0) for f in path.rglob(filename_pattern)] # results_df = pd.concat(df_from_each_file, ignore_index=True) # return results_df from typing import List def post_processing_mi ( df : pd . DataFrame , info : str = 'h' , exp : str = 'amip' , exclude : List [ str ] = [ 'inmcm4' ], normalized : bool = True ) -> pd . DataFrame : # # subset models # cmip_models = [ # 'mpi_esm_lr', # 'noresm1_m', # # 'inmcm4', # 'mpi_esm_mr', # 'access1_0', # 'ipsl_cm5a_mr' # ] df = df [ ~ df [ 'cmip' ] . isin ( exclude )] # divide by the spatial resolution if exp == 'amip' : banned_cmip_dates = [ '1979-01-16 12:00:00' , '2009-01-16 12:00:00' , '2010-01-16 12:00:00' ] banned_base_dates = [ '1979-01-01' , '2010-01-01' ] elif exp == 'rcp' : banned_cmip_dates = [ '2019-01-16 12:00:00' ] banned_base_dates = [] else : raise ValueError ( 'Unrecognized exp:' , exp ) if info == 'h' : if normalized : df [ 'h_base' ] = df [ 'h_base' ] / ( df [ 'spatial' ] ** 2 ) df [ 'h_cmip' ] = df [ 'h_cmip' ] / ( df [ 'spatial' ] ** 2 ) else : pass elif info == 'mi' : if normalized : df [ 'mi' ] = df [ 'mi' ] / ( df [ 'spatial' ] ** 2 ) else : pass df = df [ ~ df . cmip_time . isin ( banned_cmip_dates )] df = df [ ~ df . base_time . isin ( banned_base_dates )] return df data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/interim/amip/local/compare/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/amip/local/compare/\" results_df . cmip . unique () . tolist () ['mpi_esm_lr', 'access1_0'] NCEP - Comparative IT Measures \u00b6 AMIP \u00b6 results_df . cmip . unique () . tolist () ['mpi_esm_lr', 'ipsl_cm5a_lr', 'noresm1_m', 'mpi_esm_mr', 'access1_0'] results_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base base_time cmip cmip_time kendelltau mi pearson spatial spearman subsample time_mi trial variable 3220 ncep 1980-01-01 mpi_esm_lr 1980-01-16 12:00:00 0.599945 0.746869 0.791773 1 0.748195 10000 1.75668 0 psl 3221 ncep 1980-01-01 mpi_esm_lr 1980-01-16 12:00:00 0.591253 0.694736 0.787028 1 0.740457 10000 1.83795 1 psl 3222 ncep 1980-01-01 mpi_esm_lr 1980-01-16 12:00:00 0.584682 0.71889 0.780007 1 0.729812 10000 2.21417 2 psl 3223 ncep 1980-01-01 mpi_esm_lr 1980-01-16 12:00:00 0.598413 0.716736 0.783515 1 0.745998 10000 1.94358 3 psl 3224 ncep 1980-01-01 mpi_esm_lr 1980-01-16 12:00:00 0.59382 0.706751 0.787425 1 0.743049 10000 1.97115 4 psl # extract results base_model = 'ncep' exp = 'amip' results_df = get_mutual_info_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' ] normalized = False results_df = post_processing_mi ( results_df , info = 'mi' , exclude = exclude , normalized = normalized ) results_df . cmip [ results_df [ 'cmip' ] == 'cnrm_cm5' ] = 'mpi_esm_mr' for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , spatial_res = ispatial , info = 'mi' , model = exp , save = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy RCP \u00b6 # extract results base_model = 'ncep' exp = 'rcp' results_df = get_mutual_info_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' , 'access1_3' ] normalized = False results_df = post_processing_mi ( results_df , info = 'mi' , exclude = exclude , exp = exp , normalized = normalized ) results_df . cmip [ results_df [ 'cmip' ] == 'cnrm_cm5' ] = 'mpi_esm_mr' for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , spatial_res = ispatial , info = 'mi' , model = exp , save = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy ERA5 - Comparative IT Measures \u00b6 AMIP \u00b6 # extract results base_model = 'era5' exp = 'amip' results_df = get_mutual_info_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' ] normalized = False results_df = post_processing_mi ( results_df , info = 'mi' , exclude = exclude , normalized = normalized ) for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , spatial_res = ispatial , info = 'mi' , model = exp , save = True ) RCP \u00b6 # extract results base_model = 'era5' exp = 'rcp' results_df = get_mutual_info_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' , 'access1_3' ] normalized = False results_df = post_processing_mi ( results_df , info = 'mi' , exclude = exclude , normalized = normalized , exp = exp ) for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , spatial_res = ispatial , info = 'mi' , model = exp , save = True )","title":"4.3 visualize local indi"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#visually-comparing-climate-models","text":"","title":"Visually Comparing Climate Models"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#summary","text":"In this notebook, I will be comparing three climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 CMIP5 I will be looking at the following variables: Mean Sea Level Pressure (CMIP5, ERA5, NCEP) Surface Pressure (ERA5, NCEP) I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures. If these climate models are that similar, then they should exhibit similar IT measures.","title":"Summary"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#preprocessing-steps","text":"Regridded Spatially The ERA5 had the coarsest spatial resolution (2.5 x 2.5). I regridded the NCEP from (0.25 x 0.25) to (2.5 x 2.5). I regridded the CMIP5 from (2 x 2.5) to (2.5 x 2.5). Temporal Resolution ERA5 and NCEP go from 1980-2019 CMIP5 goes from 2006-2018 For comparing ERA5 vs CMIP5 and NCEP vs CMIPF, I found the same time components","title":"Preprocessing Steps"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#measures","text":"I'm measuring the following: Entropy - expected uncertainty Total Correlation - amount of redundant information between features Mutual Information - amount of information shared between variables","title":"Measures"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#data","text":"Inputs I'm taking each year as is. Each spatial location is a sample and each year is a feature. My inputs are: X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} Outputs All my information theory measures are in nats. They are scalars.","title":"Data"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#hypothesis","text":"Simple: The ERA5 and the NCEP model should be more similar than the CMIP5 model compared to each of them.","title":"Hypothesis"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#data-climate-models","text":"! ls / home / emmanuel / projects / 2020 _rbig_rs / notebooks / climate /../../ LICENSE data environment_dev.yml notebooks reports src README.md environment.yml environment_gpu.yml references setup.py import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) # Import RBIG Helper # from src.models.train_models import run_rbig_models # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) # from esdc.preprocessing import normalize_temporal from pathlib import Path import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing # Visualization Tools from src.data.climate.loader import ResultsLoader # from src.visualization.climate import PlotResults from src.visualization.climate.local import plot_diff , plot_individual , plot_individual_diff , plot_individual_all import seaborn as sns import matplotlib.pyplot as plt # plt.style.use('ggplot') plt . style . use ([ 'seaborn-poster' , 'fivethirtyeight' ]) % matplotlib inline % load_ext autoreload % autoreload 2","title":"Data - Climate Models"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#entropy","text":"def get_entropy_results ( base_model : str , results : str = 'interim' , model : str = 'rcp' ) -> pd . DataFrame : base_path = '/home/emmanuel/projects/2020_rbig_rs/data/climate/' # CHECK: RCP or AMIP if model == 'rcp' : data_path = f \"/rcp/local/individual/\" ext = 'v3' elif model == 'amip' : data_path = f \"/amip/local/individual/\" ext = 'v4' else : raise ValueError ( 'Unrecognized model: ' , model ) # print(base_path + results + data_path) path = Path ( base_path + results + data_path ) if base_model == 'ncep' : filename_pattern = 'ncep_*_' elif base_model == 'era5' : filename_pattern = 'era5_*_' else : raise ValueError ( 'Unrecognized base model:' , base_model ) df_from_each_file = [ pd . read_csv ( f , index_col = 0 ) for f in path . rglob ( filename_pattern + ext + '.csv' )] results_df = pd . concat ( df_from_each_file , ignore_index = True ) return results_df from typing import List def post_processing_entropy ( df : pd . DataFrame , model : str = 'amip' , exclude : List [ str ] = [ 'inmcm4' ], normalized : bool = False ) -> pd . DataFrame : # exclude models df = df [ ~ df [ 'cmip' ] . isin ( exclude )] # divide by the spatial resolution if model == 'amip' : banned_cmip_dates = [ '1979-01-16 12:00:00' , '2009-01-16 12:00:00' , '2010-01-16 12:00:00' ] banned_base_dates = [ '1979-01-01' , '2010-01-01' ] banned_out_dates = [ '2006-02-01' , '2011-01-01' , '2012-01-01' , '2013-01-01' , '2014-01-01' , '2015-01-01' , '2016-01-01' , '2017-01-01' , '2018-01-01' , '2019-01-01' ] elif model == 'rcp' : banned_cmip_dates = [] banned_base_dates = [] banned_out_dates = [] elif model == 'none' : banned_out_dates = [] banned_cmip_dates = [] banned_base_dates = [] else : raise ValueError ( 'Unrecognized dataset' ) # divide by the spatial resolution if normalized : df [ 'h_base' ] = df [ 'h_base' ] / ( df [ 'spatial' ] ** 2 ) df [ 'h_cmip' ] = df [ 'h_cmip' ] / ( df [ 'spatial' ] ** 2 ) df = df [ ~ df . cmip_time . isin ( banned_cmip_dates )] df = df [ ~ df . base_time . isin ( banned_base_dates )] df = df [ ~ df . base_time . isin ( banned_out_dates )] return df","title":"Entropy"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#ncep-individual-it-measures","text":"data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/rcp/local/individual/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/rcp/local/individual/\" # extract results results_df = get_entropy_results ( 'ncep' , 'interim' , 'amip' ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' ] results_df = post_processing_entropy ( results_df , exclude = exclude ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy results_df . cmip . unique () . tolist () ['mpi_esm_lr', 'noresm1_m', 'mpi_esm_mr', 'access1_0', 'ipsl_cm5a_mr'] results_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base base_time cmip cmip_time h_base h_cmip spatial subsample t_base t_cmip tc_base tc_cmip trial variable 0 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.173002 1.263623 1.0 50000.0 1.319296 1.451097 0.0 0.0 0.0 psl 1 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.177404 1.269881 1.0 50000.0 1.351144 1.327829 0.0 0.0 1.0 psl 2 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.186812 1.277439 1.0 50000.0 1.282283 1.448870 0.0 0.0 2.0 psl 3 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.183485 1.273517 1.0 50000.0 1.317563 1.319149 0.0 0.0 3.0 psl 4 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.193985 1.280803 1.0 50000.0 1.315317 1.527269 0.0 0.0 4.0 psl results_df [ results_df [ 'cmip' ] == 'inmcm4' ] . unique () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base base_time cmip cmip_time h_base h_cmip spatial subsample t_base t_cmip tc_base tc_cmip trial variable 0 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.173002 1.263623 1.0 50000.0 1.319296 1.451097 0.0 0.0 0.0 psl 1 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.177404 1.269881 1.0 50000.0 1.351144 1.327829 0.0 0.0 1.0 psl 2 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.186812 1.277439 1.0 50000.0 1.282283 1.448870 0.0 0.0 2.0 psl 3 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.183485 1.273517 1.0 50000.0 1.317563 1.319149 0.0 0.0 3.0 psl 4 ncep 1979-01-01 inmcm4 1979-01-16 12:00:00 1.193985 1.280803 1.0 50000.0 1.315317 1.527269 0.0 0.0 4.0 psl","title":"NCEP - Individual IT Measures"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#amip","text":"# extract results base_model = 'ncep' exp = 'amip' results_df = get_entropy_results ( base_model , results = 'results' , model = exp ) # post processing results_df = post_processing ( results_df , 'amip' ) results_df [ results_df [ 'cmip' ] == 'inmcm4' ] . base_time . unique () . tolist () ['2006-02-01', '2007-01-01', '2008-01-01', '2011-01-01', '2012-01-01', '2013-01-01', '2014-01-01', '2015-01-01', '2016-01-01', '2017-01-01', '2018-01-01', '2019-01-01'] results_df . base_time . unique () . tolist () ['1980-01-01', '1981-01-01', '1982-01-01', '1983-01-01', '1984-01-01', '1985-01-01', '1986-01-01', '1987-01-01', '1988-01-01', '1989-01-01', '1990-01-01', '1991-01-01', '1992-01-01', '1993-01-01', '1994-01-01', '1995-01-01', '1996-01-01', '1997-01-01', '1998-01-01', '1999-01-01', '2000-01-01', '2001-01-01', '2002-01-01', '2003-01-01', '2004-01-01', '2005-01-01', '2006-01-01', '2007-01-01', '2008-01-01'] # extract results base_model = 'ncep' exp = 'amip' results_df = get_entropy_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' ] normalized = True results_df = post_processing_entropy ( results_df , model = exp , exclude = exclude , normalized = normalized ) for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , ispatial , model = exp , save = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy","title":"AMIP"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#rcp","text":"We want to extract the different CMIP models. The other parameters are constant for now. # extract results base_model = 'ncep' exp = 'rcp' results_df = get_entropy_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' , 'access1_3' ] normalized = True results_df = post_processing_entropy ( results_df , model = exp , exclude = exclude , normalized = normalized ) for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , ispatial , model = exp , save = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy","title":"RCP"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#era5-individual-it-measures","text":"","title":"ERA5 - Individual IT Measures"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#amip_1","text":"# extract results base_model = 'era5' exp = 'amip' results_df = get_entropy_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' ] results_df = post_processing_entropy ( results_df , model = exp , exclude = exclude ) for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , ispatial , model = exp , save = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy","title":"AMIP"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#rcp_1","text":"# extract results base_model = 'era5' exp = 'rcp' results_df = get_entropy_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' , 'access1_3' ] results_df = post_processing_entropy ( results_df , model = exp , exclude = exclude ) for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , ispatial , model = exp , save = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy","title":"RCP"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#mutual-information","text":"def get_mutual_info_results ( base_model : str , results : str = 'interim' , model : str = 'rcp' ) -> pd . DataFrame : base_path = '/home/emmanuel/projects/2020_rbig_rs/data/climate/' # CHECK: RCP or AMIP if model == 'rcp' : data_path = f \"/rcp/local/compare/\" elif model == 'amip' : data_path = f \"/amip/local/compare/\" else : raise ValueError ( 'Unrecognized model: ' , model ) # print(base_path + results + data_path) path = Path ( base_path + results + data_path ) if base_model == 'ncep' : filename_pattern = 'ncep_*_v3.csv' elif base_model == 'era5' : filename_pattern = 'era5_*_v3.csv' else : raise ValueError ( 'Unrecognized base model:' , base_model ) df_from_each_file = [ pd . read_csv ( f , index_col = 0 ) for f in path . rglob ( filename_pattern )] results_df = pd . concat ( df_from_each_file , ignore_index = True ) return results_df # def get_results_files_mi(base_model: str)-> pd.DataFrame: # path = Path(data_path) # if base_model == 'ncep': # filename_pattern = 'ncep_*_v3.csv' # elif base_model == 'era5': # filename_pattern = 'era5_*_v3.csv' # else: # raise ValueError('Unrecognized base model:', base_model) # df_from_each_file = [pd.read_csv(f, index_col=0) for f in path.rglob(filename_pattern)] # results_df = pd.concat(df_from_each_file, ignore_index=True) # return results_df from typing import List def post_processing_mi ( df : pd . DataFrame , info : str = 'h' , exp : str = 'amip' , exclude : List [ str ] = [ 'inmcm4' ], normalized : bool = True ) -> pd . DataFrame : # # subset models # cmip_models = [ # 'mpi_esm_lr', # 'noresm1_m', # # 'inmcm4', # 'mpi_esm_mr', # 'access1_0', # 'ipsl_cm5a_mr' # ] df = df [ ~ df [ 'cmip' ] . isin ( exclude )] # divide by the spatial resolution if exp == 'amip' : banned_cmip_dates = [ '1979-01-16 12:00:00' , '2009-01-16 12:00:00' , '2010-01-16 12:00:00' ] banned_base_dates = [ '1979-01-01' , '2010-01-01' ] elif exp == 'rcp' : banned_cmip_dates = [ '2019-01-16 12:00:00' ] banned_base_dates = [] else : raise ValueError ( 'Unrecognized exp:' , exp ) if info == 'h' : if normalized : df [ 'h_base' ] = df [ 'h_base' ] / ( df [ 'spatial' ] ** 2 ) df [ 'h_cmip' ] = df [ 'h_cmip' ] / ( df [ 'spatial' ] ** 2 ) else : pass elif info == 'mi' : if normalized : df [ 'mi' ] = df [ 'mi' ] / ( df [ 'spatial' ] ** 2 ) else : pass df = df [ ~ df . cmip_time . isin ( banned_cmip_dates )] df = df [ ~ df . base_time . isin ( banned_base_dates )] return df data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/interim/amip/local/compare/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/amip/local/compare/\" results_df . cmip . unique () . tolist () ['mpi_esm_lr', 'access1_0']","title":"Mutual Information"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#ncep-comparative-it-measures","text":"","title":"NCEP - Comparative IT Measures"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#amip_2","text":"results_df . cmip . unique () . tolist () ['mpi_esm_lr', 'ipsl_cm5a_lr', 'noresm1_m', 'mpi_esm_mr', 'access1_0'] results_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base base_time cmip cmip_time kendelltau mi pearson spatial spearman subsample time_mi trial variable 3220 ncep 1980-01-01 mpi_esm_lr 1980-01-16 12:00:00 0.599945 0.746869 0.791773 1 0.748195 10000 1.75668 0 psl 3221 ncep 1980-01-01 mpi_esm_lr 1980-01-16 12:00:00 0.591253 0.694736 0.787028 1 0.740457 10000 1.83795 1 psl 3222 ncep 1980-01-01 mpi_esm_lr 1980-01-16 12:00:00 0.584682 0.71889 0.780007 1 0.729812 10000 2.21417 2 psl 3223 ncep 1980-01-01 mpi_esm_lr 1980-01-16 12:00:00 0.598413 0.716736 0.783515 1 0.745998 10000 1.94358 3 psl 3224 ncep 1980-01-01 mpi_esm_lr 1980-01-16 12:00:00 0.59382 0.706751 0.787425 1 0.743049 10000 1.97115 4 psl # extract results base_model = 'ncep' exp = 'amip' results_df = get_mutual_info_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' ] normalized = False results_df = post_processing_mi ( results_df , info = 'mi' , exclude = exclude , normalized = normalized ) results_df . cmip [ results_df [ 'cmip' ] == 'cnrm_cm5' ] = 'mpi_esm_mr' for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , spatial_res = ispatial , info = 'mi' , model = exp , save = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy","title":"AMIP"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#rcp_2","text":"# extract results base_model = 'ncep' exp = 'rcp' results_df = get_mutual_info_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' , 'access1_3' ] normalized = False results_df = post_processing_mi ( results_df , info = 'mi' , exclude = exclude , exp = exp , normalized = normalized ) results_df . cmip [ results_df [ 'cmip' ] == 'cnrm_cm5' ] = 'mpi_esm_mr' for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , spatial_res = ispatial , info = 'mi' , model = exp , save = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy","title":"RCP"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#era5-comparative-it-measures","text":"","title":"ERA5 - Comparative IT Measures"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#amip_3","text":"# extract results base_model = 'era5' exp = 'amip' results_df = get_mutual_info_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' ] normalized = False results_df = post_processing_mi ( results_df , info = 'mi' , exclude = exclude , normalized = normalized ) for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , spatial_res = ispatial , info = 'mi' , model = exp , save = True )","title":"AMIP"},{"location":"notebooks/climate/amip/4.3_visualize_local_indi/#rcp_3","text":"# extract results base_model = 'era5' exp = 'rcp' results_df = get_mutual_info_results ( base_model , results = 'results' , model = exp ) # post processing exclude = [ 'inmcm4' , 'giss_e2_r' , 'bnu_esm' , 'bcc_csm1_1' , 'access1_3' ] normalized = False results_df = post_processing_mi ( results_df , info = 'mi' , exclude = exclude , normalized = normalized , exp = exp ) for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_individual_all ( results_df , base_model , spatial_res = ispatial , info = 'mi' , model = exp , save = True )","title":"RCP"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Visually Comparing Climate Models \u00b6 Summary \u00b6 In this notebook, I will be comparing three climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 CMIP5 I will be looking at the following variables: Mean Sea Level Pressure (CMIP5, ERA5, NCEP) Surface Pressure (ERA5, NCEP) I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures. If these climate models are that similar, then they should exhibit similar IT measures. Preprocessing Steps \u00b6 Regridded Spatially The ERA5 had the coarsest spatial resolution (2.5 x 2.5). I regridded the NCEP from (0.25 x 0.25) to (2.5 x 2.5). I regridded the CMIP5 from (2 x 2.5) to (2.5 x 2.5). Temporal Resolution ERA5 and NCEP go from 1980-2019 CMIP5 goes from 2006-2018 For comparing ERA5 vs CMIP5 and NCEP vs CMIPF, I found the same time components Measures \u00b6 I'm measuring the following: Entropy - expected uncertainty Total Correlation - amount of redundant information between features Mutual Information - amount of information shared between variables Data \u00b6 Inputs I'm taking each year as is. Each spatial location is a sample and each year is a feature. My inputs are: X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} Outputs All my information theory measures are in nats. They are scalars. Hypothesis \u00b6 Simple: The ERA5 and the NCEP model should be more similar than the CMIP5 model compared to each of them. Data - Climate Models \u00b6 ! ls / home / emmanuel / projects / 2020 _rbig_rs / data / climate / results / amip / local / compare / era5_access1_0_v1.csv era5_ipsl_cm5a_lr_v2.csv ncep_cnrm_cm5_v1.csv era5_access1_0_v2.csv era5_mpi_esm_lr_v1.csv ncep_cnrm_cm5_v2.csv era5_bcc_csm1_1_v1.csv era5_mpi_esm_lr_v2.csv ncep_giss_e2_r_v1.csv era5_bcc_csm1_1_v2.csv era5_noresm1_m_v1.csv ncep_giss_e2_r_v2.csv era5_bnu_esm_v1.csv era5_noresm1_m_v2.csv ncep_ipsl_cm5a_lr_v1.csv era5_bnu_esm_v2.csv ncep_access1_0_v1.csv ncep_ipsl_cm5a_lr_v2.csv era5_cnrm_cm5_v1.csv ncep_access1_0_v2.csv ncep_mpi_esm_lr_v1.csv era5_cnrm_cm5_v2.csv ncep_bcc_csm1_1_v1.csv ncep_mpi_esm_lr_v2.csv era5_giss_e2_r_v1.csv ncep_bcc_csm1_1_v2.csv ncep_noresm1_m_v1.csv era5_giss_e2_r_v2.csv ncep_bnu_esm_v1.csv ncep_noresm1_m_v2.csv era5_ipsl_cm5a_lr_v1.csv ncep_bnu_esm_v2.csv import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) # from esdc.preprocessing import normalize_temporal from pathlib import Path import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing # Visualization Tools # from src.data.climate.loader import ResultsLoader # from src.visualization.climate import PlotResults from src.visualization.climate.compare import plot_individual , plot_all import seaborn as sns import matplotlib.pyplot as plt # plt.style.use('ggplot') plt . style . use ([ 'seaborn-poster' , 'fivethirtyeight' ]) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Experiment I - Local \u00b6 data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip/local/compare/\" def get_results_files ( base_model : str , trials : bool ) -> pd . DataFrame : path = Path ( data_path ) if base_model == 'ncep' : base_pattern = 'ncep' elif base_model == 'era5' : base_pattern = 'era5' else : raise ValueError ( 'Unrecognized base model:' , base_model ) if trials == True : trials_ext = 'v1' elif trials == False : trials_ext = 'v2' else : raise ValueError ( \"Unrecognized trials extentions:\" , trials ) filename_pattern = base_pattern + '*' + trials_ext + '.csv' df_from_each_file = [ pd . read_csv ( f , index_col = 0 ) for f in path . rglob ( filename_pattern )] results_df = pd . concat ( df_from_each_file , ignore_index = True ) return results_df def post_processing_compare ( df : pd . DataFrame ) -> pd . DataFrame : # divide by the spatial resolution df [ 'mi' ] = df [ 'mi' ] / ( df [ 'spatial' ] ** 2 ) return df NCEP - Individual IT Measures \u00b6 data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip/local/compare/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/amip/local/compare/\" # extract results results_df = get_results_files ( 'ncep' , False ) # results_df.head() # # post processing results_df = post_processing_compare ( results_df , ) results_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base base_time cmip cmip_time kendelltau mi pearson spatial spearman subsample time_mi trial variable 0 ncep 1979-02-01 cnrm_cm5 1979-01-16 12:00:00 0.404668 93.762735 0.472259 1.0 0.560047 50000.0 462.748412 0.0 psl 1 ncep 1979-03-01 cnrm_cm5 1979-02-15 00:00:00 0.581296 93.383035 0.706488 1.0 0.774481 50000.0 461.874625 0.0 psl 2 ncep 1979-04-01 cnrm_cm5 1979-03-16 12:00:00 0.633132 94.526655 0.852317 1.0 0.814823 50000.0 462.157740 0.0 psl 3 ncep 1979-05-01 cnrm_cm5 1979-04-16 00:00:00 0.642649 93.431233 0.848718 1.0 0.809581 50000.0 462.089180 0.0 psl 4 ncep 1979-06-01 cnrm_cm5 1979-05-16 12:00:00 0.582758 93.599717 0.847242 1.0 0.744458 50000.0 462.955628 0.0 psl We want to extract the different CMIP models. The other parameters are constant for now. sample_spatial = 1.0 for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_all ( results_df , ispatial , 'pearson' ) ERA5 - Individual IT Measures \u00b6 # extract results trials = False results_df = get_results_files ( 'era5' , trials = True ) # post processing results_df = post_processing_compare ( results_df ) sample_spatial = 1.0 for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_all ( results_df , ispatial , 'pearson' ) Trials \u00b6 NCEP - Comparative IT Measures \u00b6 # extract results trials = False results_df = get_results_files ( 'ncep' , trials = True ) # post processing results_df = post_processing_compare ( results_df ) sample_spatial = 1.0 for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_all ( results_df , ispatial , 'pearson' ) ERA5 - Comparative IT Measures \u00b6 # extract results trials = False results_df = get_results_files ( 'era5' , trials = True ) # post processing results_df = post_processing_compare ( results_df ) sample_spatial = 1.0 for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_all ( results_df , ispatial , 'pearson' ) Results \u00b6 Mean Sea Level Pressure \u00b6 CMIP5 vs ERA5 vs NCEP \u00b6 variables = [ 'mslp_era_cmip' , 'mslp_ncep_cmip' ] filenames = [ f \" { results_path }{ variable } .csv\" for variable in variables ] results = ResultsLoader ( filenames ) . load_dataframes () # initializer plotter plotter = PlotResults ( results ) plotter . results = plotter . results [ plotter . results [ 'year' ] < 2019 ] Entropy \u00b6 ent1_fig , ent1_ax = plotter . plot_entropy () Total Correlation \u00b6 tc1_fig , tc1_ax = plotter . plot_total_correlation () Mutual Information \u00b6 This is the MI between CMIP5 and the two models (ERA5 and NCEP) mi1_fig , mi1_ax = plotter . plot_mutual_information (( 'model' , [ 'cmip5' ])) NCEP vs ERA5 \u00b6 Mean Sea Level Pressure \u00b6 variables = [ 'mslp_ncep_era' ] filenames = [ f \" { results_path }{ variable } .csv\" for variable in variables ] results = ResultsLoader ( filenames ) . load_dataframes () # initializer plotter plotter = PlotResults ( results ) plotter . results = plotter . results [ plotter . results [ 'year' ] < 2017 ] Entropy \u00b6 plotter . plot_entropy (); Total Correlation \u00b6 plotter . plot_total_correlation (); Mutual Information \u00b6 The MI between ERA5 and the NCAR_NCEP_DOE_2 model. plotter . plot_mutual_information ( omit_models = ( 'model' , [ 'ncar_ncep_doe_2' ])); Surface Pressure \u00b6 variables = [ 'sp_ncep_era' ] filenames = [ f \" { results_path }{ variable } .csv\" for variable in variables ] results = ResultsLoader ( filenames ) . load_dataframes () # initializer plotter plotter = PlotResults ( results ) plotter . results = plotter . results [ plotter . results [ 'year' ] < 2018 ] Entropy \u00b6 plotter . plot_entropy (); Total Correlation \u00b6 plotter . plot_total_correlation (); Mutual Information \u00b6 The MI between ERA5 and NCEP. plotter . plot_mutual_information ( omit_models = ( 'model' , [ 'ncar_ncep_doe_2' ]));","title":"4.4 visualize local compare"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#visually-comparing-climate-models","text":"","title":"Visually Comparing Climate Models"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#summary","text":"In this notebook, I will be comparing three climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 CMIP5 I will be looking at the following variables: Mean Sea Level Pressure (CMIP5, ERA5, NCEP) Surface Pressure (ERA5, NCEP) I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures. If these climate models are that similar, then they should exhibit similar IT measures.","title":"Summary"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#preprocessing-steps","text":"Regridded Spatially The ERA5 had the coarsest spatial resolution (2.5 x 2.5). I regridded the NCEP from (0.25 x 0.25) to (2.5 x 2.5). I regridded the CMIP5 from (2 x 2.5) to (2.5 x 2.5). Temporal Resolution ERA5 and NCEP go from 1980-2019 CMIP5 goes from 2006-2018 For comparing ERA5 vs CMIP5 and NCEP vs CMIPF, I found the same time components","title":"Preprocessing Steps"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#measures","text":"I'm measuring the following: Entropy - expected uncertainty Total Correlation - amount of redundant information between features Mutual Information - amount of information shared between variables","title":"Measures"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#data","text":"Inputs I'm taking each year as is. Each spatial location is a sample and each year is a feature. My inputs are: X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} X \\in \\mathbb{R}^{\\text{spatial } \\times \\text{ month}} Outputs All my information theory measures are in nats. They are scalars.","title":"Data"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#hypothesis","text":"Simple: The ERA5 and the NCEP model should be more similar than the CMIP5 model compared to each of them.","title":"Hypothesis"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#data-climate-models","text":"! ls / home / emmanuel / projects / 2020 _rbig_rs / data / climate / results / amip / local / compare / era5_access1_0_v1.csv era5_ipsl_cm5a_lr_v2.csv ncep_cnrm_cm5_v1.csv era5_access1_0_v2.csv era5_mpi_esm_lr_v1.csv ncep_cnrm_cm5_v2.csv era5_bcc_csm1_1_v1.csv era5_mpi_esm_lr_v2.csv ncep_giss_e2_r_v1.csv era5_bcc_csm1_1_v2.csv era5_noresm1_m_v1.csv ncep_giss_e2_r_v2.csv era5_bnu_esm_v1.csv era5_noresm1_m_v2.csv ncep_ipsl_cm5a_lr_v1.csv era5_bnu_esm_v2.csv ncep_access1_0_v1.csv ncep_ipsl_cm5a_lr_v2.csv era5_cnrm_cm5_v1.csv ncep_access1_0_v2.csv ncep_mpi_esm_lr_v1.csv era5_cnrm_cm5_v2.csv ncep_bcc_csm1_1_v1.csv ncep_mpi_esm_lr_v2.csv era5_giss_e2_r_v1.csv ncep_bcc_csm1_1_v2.csv ncep_noresm1_m_v1.csv era5_giss_e2_r_v2.csv ncep_bnu_esm_v1.csv ncep_noresm1_m_v2.csv era5_ipsl_cm5a_lr_v1.csv ncep_bnu_esm_v2.csv import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) # from esdc.preprocessing import normalize_temporal from pathlib import Path import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing # Visualization Tools # from src.data.climate.loader import ResultsLoader # from src.visualization.climate import PlotResults from src.visualization.climate.compare import plot_individual , plot_all import seaborn as sns import matplotlib.pyplot as plt # plt.style.use('ggplot') plt . style . use ([ 'seaborn-poster' , 'fivethirtyeight' ]) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"Data - Climate Models"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#experiment-i-local","text":"data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip/local/compare/\" def get_results_files ( base_model : str , trials : bool ) -> pd . DataFrame : path = Path ( data_path ) if base_model == 'ncep' : base_pattern = 'ncep' elif base_model == 'era5' : base_pattern = 'era5' else : raise ValueError ( 'Unrecognized base model:' , base_model ) if trials == True : trials_ext = 'v1' elif trials == False : trials_ext = 'v2' else : raise ValueError ( \"Unrecognized trials extentions:\" , trials ) filename_pattern = base_pattern + '*' + trials_ext + '.csv' df_from_each_file = [ pd . read_csv ( f , index_col = 0 ) for f in path . rglob ( filename_pattern )] results_df = pd . concat ( df_from_each_file , ignore_index = True ) return results_df def post_processing_compare ( df : pd . DataFrame ) -> pd . DataFrame : # divide by the spatial resolution df [ 'mi' ] = df [ 'mi' ] / ( df [ 'spatial' ] ** 2 ) return df","title":"Experiment I - Local"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#ncep-individual-it-measures","text":"data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip/local/compare/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/amip/local/compare/\" # extract results results_df = get_results_files ( 'ncep' , False ) # results_df.head() # # post processing results_df = post_processing_compare ( results_df , ) results_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } base base_time cmip cmip_time kendelltau mi pearson spatial spearman subsample time_mi trial variable 0 ncep 1979-02-01 cnrm_cm5 1979-01-16 12:00:00 0.404668 93.762735 0.472259 1.0 0.560047 50000.0 462.748412 0.0 psl 1 ncep 1979-03-01 cnrm_cm5 1979-02-15 00:00:00 0.581296 93.383035 0.706488 1.0 0.774481 50000.0 461.874625 0.0 psl 2 ncep 1979-04-01 cnrm_cm5 1979-03-16 12:00:00 0.633132 94.526655 0.852317 1.0 0.814823 50000.0 462.157740 0.0 psl 3 ncep 1979-05-01 cnrm_cm5 1979-04-16 00:00:00 0.642649 93.431233 0.848718 1.0 0.809581 50000.0 462.089180 0.0 psl 4 ncep 1979-06-01 cnrm_cm5 1979-05-16 12:00:00 0.582758 93.599717 0.847242 1.0 0.744458 50000.0 462.955628 0.0 psl We want to extract the different CMIP models. The other parameters are constant for now. sample_spatial = 1.0 for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_all ( results_df , ispatial , 'pearson' )","title":"NCEP - Individual IT Measures"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#era5-individual-it-measures","text":"# extract results trials = False results_df = get_results_files ( 'era5' , trials = True ) # post processing results_df = post_processing_compare ( results_df ) sample_spatial = 1.0 for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_all ( results_df , ispatial , 'pearson' )","title":"ERA5 - Individual IT Measures"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#trials","text":"","title":"Trials"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#ncep-comparative-it-measures","text":"# extract results trials = False results_df = get_results_files ( 'ncep' , trials = True ) # post processing results_df = post_processing_compare ( results_df ) sample_spatial = 1.0 for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_all ( results_df , ispatial , 'pearson' )","title":"NCEP - Comparative IT Measures"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#era5-comparative-it-measures","text":"# extract results trials = False results_df = get_results_files ( 'era5' , trials = True ) # post processing results_df = post_processing_compare ( results_df ) sample_spatial = 1.0 for ispatial in [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]: plot_all ( results_df , ispatial , 'pearson' )","title":"ERA5 - Comparative IT Measures"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#results","text":"","title":"Results"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#mean-sea-level-pressure","text":"","title":"Mean Sea Level Pressure"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#cmip5-vs-era5-vs-ncep","text":"variables = [ 'mslp_era_cmip' , 'mslp_ncep_cmip' ] filenames = [ f \" { results_path }{ variable } .csv\" for variable in variables ] results = ResultsLoader ( filenames ) . load_dataframes () # initializer plotter plotter = PlotResults ( results ) plotter . results = plotter . results [ plotter . results [ 'year' ] < 2019 ]","title":"CMIP5 vs ERA5 vs NCEP"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#entropy","text":"ent1_fig , ent1_ax = plotter . plot_entropy ()","title":"Entropy"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#total-correlation","text":"tc1_fig , tc1_ax = plotter . plot_total_correlation ()","title":"Total Correlation"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#mutual-information","text":"This is the MI between CMIP5 and the two models (ERA5 and NCEP) mi1_fig , mi1_ax = plotter . plot_mutual_information (( 'model' , [ 'cmip5' ]))","title":"Mutual Information"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#ncep-vs-era5","text":"","title":"NCEP vs ERA5"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#mean-sea-level-pressure_1","text":"variables = [ 'mslp_ncep_era' ] filenames = [ f \" { results_path }{ variable } .csv\" for variable in variables ] results = ResultsLoader ( filenames ) . load_dataframes () # initializer plotter plotter = PlotResults ( results ) plotter . results = plotter . results [ plotter . results [ 'year' ] < 2017 ]","title":"Mean Sea Level Pressure"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#entropy_1","text":"plotter . plot_entropy ();","title":"Entropy"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#total-correlation_1","text":"plotter . plot_total_correlation ();","title":"Total Correlation"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#mutual-information_1","text":"The MI between ERA5 and the NCAR_NCEP_DOE_2 model. plotter . plot_mutual_information ( omit_models = ( 'model' , [ 'ncar_ncep_doe_2' ]));","title":"Mutual Information"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#surface-pressure","text":"variables = [ 'sp_ncep_era' ] filenames = [ f \" { results_path }{ variable } .csv\" for variable in variables ] results = ResultsLoader ( filenames ) . load_dataframes () # initializer plotter plotter = PlotResults ( results ) plotter . results = plotter . results [ plotter . results [ 'year' ] < 2018 ]","title":"Surface Pressure"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#entropy_2","text":"plotter . plot_entropy ();","title":"Entropy"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#total-correlation_2","text":"plotter . plot_total_correlation ();","title":"Total Correlation"},{"location":"notebooks/climate/amip/4.4_visualize_local_compare/#mutual-information_2","text":"The MI between ERA5 and NCEP. plotter . plot_mutual_information ( omit_models = ( 'model' , [ 'ncar_ncep_doe_2' ]));","title":"Mutual Information"},{"location":"notebooks/climate/rcp/0_data_download/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Comparing Two Climate Models \u00b6 In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures. Data - Climate Models \u00b6 import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.amip import DataDownloader from src.data.climate.amip import DataLoader # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) from esdc.standardize import normalize_temporal from esdc.grid import regrid_data import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\" Download Datasets \u00b6 downloader = DataDownloader () downloader . download_all () 2019-11-06 16:03:59,814 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels 2019-11-06 16:04:00,498 INFO Request is completed 2019-11-06 16:04:00,499 INFO Downloading http://136.156.133.46/cache-compute-0015/cache/data4/70c39ac0-958f-4b70-8ac1-0991f58ba384-psl_Amon_IPSL-CM5B-LR_amip_r1i1p1_197901-200812.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/ipsl_cm5b_lr/amip_ipsl_cm5b_lr.zip (12.7M) 2019-11-06 16:04:01,760 INFO Download rate 10.1M/s /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/ipsl_cm5b_lr/amip_ipsl_cm5b_lr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/ipsl_cm5b_lr/amip_ipsl_cm5b_lr.nc Load Datasets \u00b6 loader = DataLoader() \u00b6 dataset = 'ipsl_cm5b_lr' xr_data = loader.load_amip_data(dataset) xr_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 96, lon: 96, time: 360) Coordinates: * time (time) object 1979-01-16 12:00:00 ... 2008-12-16 12:00:00 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object dask.array<chunksize=(360, 2), meta=np.ndarray> lat_bnds (lat, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> lon_bnds (lon, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> psl (time, lat, lon) float32 dask.array<chunksize=(360, 96, 96), meta=np.ndarray> Attributes: institution: IPSL (Institut Pierre Simon Laplace, Paris, France) institute_id: IPSL experiment_id: amip source: IPSL-CM5B-LR (2011) : atmos : LMDZ5 (LMDZ5_NPv3.1... model_id: IPSL-CM5B-LR forcing: Nat,Ant,GHG,SA,Oz,LU,SS,Ds,BC,MD,OC,AA parent_experiment_id: N/A parent_experiment_rip: N/A branch_time: 0.0 contact: ipsl-cmip5 _at_ ipsl.jussieu.fr Data manager : Se... comment: This atmospheric only simulation include natural ... references: Model documentation and further reference availab... initialization_method: 1 physics_version: 1 tracking_id: b06de09a-a87a-4f4d-bb64-63ad47eab808 product: output experiment: AMIP frequency: mon creation_date: 2012-06-11T18:49:23Z history: 2012-06-11T18:49:23Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: IPSL-CM5B-LR model output prepared for CMIP5 AMIP parent_experiment: N/A modeling_realm: atmos realization: 1 cmor_version: 2.7.1 Testing \u00b6 datasets = [ \"inmcm4\" , \"access1_0\" , \"bcc_csm1_1\" , \"bcc_csm1_1_m\" , \"bnu_esm\" , \"giss_e2_r\" , \"cnrm_cm5\" , \"ipsl_cm5a_lr\" , \"ipsl_cm5a_mr\" , \"ipsl_cm5b_lr\" , \"mpi_esm_lr\" , \"mpi_esm_mr\" , \"noresm1_m\" , ] for idataset in datasets : print ( idataset ) data = loader . load_amip_data ( idataset ) print ( data . psl . shape ) assert ( type ( data ) is xr . Dataset ) inmcm4 (360, 120, 180) access1_0 (360, 145, 192) bcc_csm1_1 (360, 64, 128) bcc_csm1_1_m (360, 160, 320) bnu_esm (360, 64, 128) giss_e2_r (1572, 90, 144) cnrm_cm5 (360, 128, 256) ipsl_cm5a_lr (372, 96, 96) ipsl_cm5a_mr (720, 143, 144) ipsl_cm5b_lr (360, 96, 96) mpi_esm_lr (360, 96, 192) mpi_esm_mr (360, 96, 192) noresm1_m (360, 96, 144)","title":"0 data download"},{"location":"notebooks/climate/rcp/0_data_download/#comparing-two-climate-models","text":"In this notebook, I will be comparing two climate reanalysis models: NCEP-DOE Reanalysis 2: Surface ERA5 I will be looking at the following variables: Surface Pressure Mean Sea Level Pressure Total Column Water The idea is simple: these two models should have very similar properties. I will be trying to user RBIG in order to assess how similar these models are. I'll be looking at the following IT measures Entropy Total Correlation Mutual Information If these climate models are that similar, then they should exhibit similar IT measures.","title":"Comparing Two Climate Models"},{"location":"notebooks/climate/rcp/0_data_download/#data-climate-models","text":"import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.amip import DataDownloader from src.data.climate.amip import DataLoader # ESDC tools sys . path . insert ( 0 , f '/home/emmanuel/code/py_esdc' ) from esdc.standardize import normalize_temporal from esdc.grid import regrid_data import cdsapi from zipfile import ZipFile import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\"","title":"Data - Climate Models"},{"location":"notebooks/climate/rcp/0_data_download/#download-datasets","text":"downloader = DataDownloader () downloader . download_all () 2019-11-06 16:03:59,814 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/projections-cmip5-monthly-single-levels 2019-11-06 16:04:00,498 INFO Request is completed 2019-11-06 16:04:00,499 INFO Downloading http://136.156.133.46/cache-compute-0015/cache/data4/70c39ac0-958f-4b70-8ac1-0991f58ba384-psl_Amon_IPSL-CM5B-LR_amip_r1i1p1_197901-200812.nc to /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/ipsl_cm5b_lr/amip_ipsl_cm5b_lr.zip (12.7M) 2019-11-06 16:04:01,760 INFO Download rate 10.1M/s /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/ipsl_cm5b_lr/amip_ipsl_cm5b_lr.zip Already nc... Changing name: /home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/ipsl_cm5b_lr/amip_ipsl_cm5b_lr.nc","title":"Download Datasets"},{"location":"notebooks/climate/rcp/0_data_download/#load-datasets","text":"","title":"Load Datasets"},{"location":"notebooks/climate/rcp/0_data_download/#loader-dataloader","text":"dataset = 'ipsl_cm5b_lr' xr_data = loader.load_amip_data(dataset) xr_data <xarray.Dataset> Dimensions: (bnds: 2, lat: 96, lon: 96, time: 360) Coordinates: * time (time) object 1979-01-16 12:00:00 ... 2008-12-16 12:00:00 * lat (lat) float64 -90.0 -88.11 -86.21 -84.32 ... 86.21 88.11 90.0 * lon (lon) float64 0.0 3.75 7.5 11.25 15.0 ... 345.0 348.8 352.5 356.2 Dimensions without coordinates: bnds Data variables: time_bnds (time, bnds) object dask.array<chunksize=(360, 2), meta=np.ndarray> lat_bnds (lat, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> lon_bnds (lon, bnds) float64 dask.array<chunksize=(96, 2), meta=np.ndarray> psl (time, lat, lon) float32 dask.array<chunksize=(360, 96, 96), meta=np.ndarray> Attributes: institution: IPSL (Institut Pierre Simon Laplace, Paris, France) institute_id: IPSL experiment_id: amip source: IPSL-CM5B-LR (2011) : atmos : LMDZ5 (LMDZ5_NPv3.1... model_id: IPSL-CM5B-LR forcing: Nat,Ant,GHG,SA,Oz,LU,SS,Ds,BC,MD,OC,AA parent_experiment_id: N/A parent_experiment_rip: N/A branch_time: 0.0 contact: ipsl-cmip5 _at_ ipsl.jussieu.fr Data manager : Se... comment: This atmospheric only simulation include natural ... references: Model documentation and further reference availab... initialization_method: 1 physics_version: 1 tracking_id: b06de09a-a87a-4f4d-bb64-63ad47eab808 product: output experiment: AMIP frequency: mon creation_date: 2012-06-11T18:49:23Z history: 2012-06-11T18:49:23Z CMOR rewrote data to comply ... Conventions: CF-1.4 project_id: CMIP5 table_id: Table Amon (31 January 2011) 53b766a395ac41696af4... title: IPSL-CM5B-LR model output prepared for CMIP5 AMIP parent_experiment: N/A modeling_realm: atmos realization: 1 cmor_version: 2.7.1","title":"loader = DataLoader()"},{"location":"notebooks/climate/rcp/0_data_download/#testing","text":"datasets = [ \"inmcm4\" , \"access1_0\" , \"bcc_csm1_1\" , \"bcc_csm1_1_m\" , \"bnu_esm\" , \"giss_e2_r\" , \"cnrm_cm5\" , \"ipsl_cm5a_lr\" , \"ipsl_cm5a_mr\" , \"ipsl_cm5b_lr\" , \"mpi_esm_lr\" , \"mpi_esm_mr\" , \"noresm1_m\" , ] for idataset in datasets : print ( idataset ) data = loader . load_amip_data ( idataset ) print ( data . psl . shape ) assert ( type ( data ) is xr . Dataset ) inmcm4 (360, 120, 180) access1_0 (360, 145, 192) bcc_csm1_1 (360, 64, 128) bcc_csm1_1_m (360, 160, 320) bnu_esm (360, 64, 128) giss_e2_r (1572, 90, 144) cnrm_cm5 (360, 128, 256) ipsl_cm5a_lr (372, 96, 96) ipsl_cm5a_mr (720, 143, 144) ipsl_cm5b_lr (360, 96, 96) mpi_esm_lr (360, 96, 192) mpi_esm_mr (360, 96, 192) noresm1_m (360, 96, 144)","title":"Testing"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Information Theory Measures \u00b6 In this notebook, I will be demonstrating some of the aspects of information theory measures. Data - Climate Models \u00b6 import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.amip import DataDownloader , DataLoader from src.data.climate.era5 import get_era5_data from src.data.climate.ncep import get_ncep_data from src.features.climate.build_features import ( get_time_overlap , check_time_coords , regrid_2_lower_res , get_spatial_cubes , normalize_data ) from src.experiments.climate.amip_global import ( experiment_loop_comparative , experiment_loop_individual ) # Stat Tools from src.models.information.entropy import RBIGEstimator as RBIGENTEST from src.models.information.mutual_information import RBIGEstimator as RBIGMIEST from scipy import stats import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 amip_data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" era5_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/era5/\" ncep_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/ncep/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\" Demo Experiment \u00b6 Experimental Paams \u00b6 class DataArgs : data_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" results_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip\" class CMIPArgs : # Fixed Params spatial_windows = [ 1 , 2 , # Spatial Window for Density Cubes 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] # Free Params variables = [ 'psl' # Mean Surface Pressure ] cmip_models = [ \"inmcm4\" , \"access1_0\" , \"bcc_csm1_1\" , \"bcc_csm1_1_m\" , \"bnu_esm\" , \"giss_e2_r\" , \"cnrm_cm5\" , \"ipsl_cm5a_lr\" , \"ipsl_cm5a_mr\" , \"ipsl_cm5b_lr\" , \"mpi_esm_lr\" , \"mpi_esm_mr\" , \"noresm1_m\" , ] base_models = [ 'ncep' , \"era5\" ] Part I - Grab Data \u00b6 from src.data.climate.amip import get_base_model base_dat = get_base_model ( CMIPArgs . base_models [ 0 ], CMIPArgs . variables [ 0 ]) # base_dat from src.data.climate.cmip5 import get_cmip5_model cmip_dat = get_cmip5_model ( CMIPArgs . cmip_models [ 0 ], CMIPArgs . variables [ 0 ]) # cmip_dat Part II - Regrid Data \u00b6 base_dat , cmip_dat = regrid_2_lower_res ( base_dat , cmip_dat ) assert ( base_dat . shape [ 1 ] == cmip_dat . shape [ 1 ]) assert ( base_dat . shape [ 2 ] == cmip_dat . shape [ 2 ]) # base_dat Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc Part III - Find Overlapping Times \u00b6 base_dat . shape , cmip_dat . shape ((489, 73, 144), (360, 73, 144)) base_dat , cmip_dat = get_time_overlap ( base_dat , cmip_dat ) Part IV - Get Density Cubes \u00b6 base_df = get_spatial_cubes ( base_dat , CMIPArgs . spatial_windows [ 3 ]) cmip_df = get_spatial_cubes ( cmip_dat , CMIPArgs . spatial_windows [ 3 ]) base_df . shape (3543330, 16) Normalize \u00b6 base_df = normalize_data ( base_df ) cmip_df = normalize_data ( cmip_df ) Information Theory Measures \u00b6 Entropy, H( X X ) \u00b6 subsample = 10_000 batch_size = None bootstrap = False ent_est = RBIGENTEST ( batch_size = batch_size , bootstrap = bootstrap , ) ent_est . fit ( base_df [: subsample ]) h = ent_est . score ( base_df [: subsample ]) h -33.116467738349236 with Bootstrap \u00b6 batch_size = 10_000 bootstrap = True n_iterations = 100 ent_est = RBIGENTEST ( batch_size = batch_size , bootstrap = bootstrap , n_iterations = n_iterations ) ent_est . fit ( base_df ) h = ent_est . score ( base_df ) h -31.881844520814997 plt . hist ( ent_est . raw_scores ) -28.48503649185888 plt . hist ( ent_est . raw_scores ) (array([ 1., 0., 1., 0., 9., 11., 26., 28., 14., 10.]), array([-30.9162801 , -30.56606644, -30.21585278, -29.86563913, -29.51542547, -29.16521182, -28.81499816, -28.46478451, -28.11457085, -27.76435719, -27.41414354]), <a list of 10 Patch objects>) W. Batches \u00b6 subsample = 40_000 ent_est = RBIGENTEST ( batch_size = 10_000 ) ent_est . fit ( base_df [: subsample ]) h = ent_est . score ( base_df [: subsample ]) h -31.84759524855099 ent_est . raw_scores [-32.17903374504498, -31.753140917432507, -31.67399995592763, -31.784206375798846] Total Correlation, TC( X X ) \u00b6 subsample = 40_000 tc_est = RBIGMIEST ( batch_size = None ) tc_est . fit ( base_df [: subsample ]) tc = tc_est . score ( base_df [: subsample ]) tc 51.735384060195784 w. Batches \u00b6 subsample = 40_000 tc_est = RBIGMIEST ( batch_size = 10_000 ) tc_est . fit ( base_df [: subsample ]) tc = tc_est . score ( base_df [: subsample ]) tc 50.6219155716329 tc_est . raw_scores [50.29844313632438, 51.03391865505402, 50.72933249988033, 50.425967995272856] Mutual Information, MI( X X ) \u00b6 subsample = 100_000 mi_est = RBIGMIEST ( batch_size = None ) mi_est . fit ( base_df [: subsample ], cmip_df [: subsample ] ) mi = mi_est . score ( base_df [: subsample ]) mi 1.2438747143982896 w. Batches \u00b6 subsample = 100_000 mi_est = RBIGMIEST ( batch_size = 50_000 ) mi_est . fit ( base_df [: subsample ], cmip_df [: subsample ] ) mi = mi_est . score ( base_df [: subsample ]) mi 1.215228412628969 mi_est . raw_values --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-84-f0a0474b33b1> in <module> ----> 1 mi_est . raw_values AttributeError : 'RBIGEstimator' object has no attribute 'raw_values' Mutual Information II, H(X) + H(Y) - H(X,Y) \u00b6 subsample = 100_000 batch_size = 25_000 # H(X) print ( 'H(X)' ) x_ent_est = RBIGENTEST ( batch_size = batch_size ) x_ent_est . fit ( base_df . values [: subsample ]) h_x = x_ent_est . score ( base_df . values [: subsample ]) # H(Y) print ( 'H(Y)' ) y_ent_est = RBIGENTEST ( batch_size = batch_size ) y_ent_est . fit ( cmip_df . values [: subsample ]) h_y = y_ent_est . score ( cmip_df . values [: subsample ]) # H(X,Y) print ( 'H(X,Y)' ) xy_ent_est = RBIGENTEST ( batch_size = 50_000 ) xy_ent_est . fit ( np . hstack ( ( base_df . values [: subsample ], cmip_df . values [: subsample ] ) ), ) h_xy = xy_ent_est . score ( base_df . values [: subsample ]) H(X) H(Y) H(X,Y) # H(X,Y) print ( 'H(X,Y)' ) xy_ent_est = RBIGENTEST ( batch_size = 50_000 ) xy_ent_est . fit ( np . hstack ( ( base_df . values [: subsample ], cmip_df . values [: subsample ] ) ), ) h_xy = xy_ent_est . score ( base_df . values [: subsample ]) h_xy H(X,Y) 165.23978712025018 h_x , h_y , h_xy , h_x + h_y - h_xy (79.10616714936484, 87.19046271977632, 165.45410606367204, 0.8425238054691135) 0.4360788203771051 Correlation: Pearson, Spearman, KendallTau \u00b6 pear = stats . pearsonr ( base_df [: subsample ] . ravel (), cmip_df [: subsample ] . ravel (), ) spear = stats . spearmanr ( base_df [: subsample ] . ravel (), cmip_df [: subsample ] . ravel (), ) kend = stats . kendalltau ( base_df [: subsample ] . ravel (), cmip_df [: subsample ] . ravel (), ) pear [ 0 ], spear [ 0 ], kend [ 0 ]","title":"3.2 demo it measures Copy1"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#information-theory-measures","text":"In this notebook, I will be demonstrating some of the aspects of information theory measures.","title":"Information Theory Measures"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#data-climate-models","text":"import os , sys cwd = os . getcwd () source_path = f \" { cwd } /../../../\" sys . path . insert ( 0 , f ' { source_path } ' ) import numpy as np # Data Loaders from src.data.climate.amip import DataDownloader , DataLoader from src.data.climate.era5 import get_era5_data from src.data.climate.ncep import get_ncep_data from src.features.climate.build_features import ( get_time_overlap , check_time_coords , regrid_2_lower_res , get_spatial_cubes , normalize_data ) from src.experiments.climate.amip_global import ( experiment_loop_comparative , experiment_loop_individual ) # Stat Tools from src.models.information.entropy import RBIGEstimator as RBIGENTEST from src.models.information.mutual_information import RBIGEstimator as RBIGMIEST from scipy import stats import pandas as pd import xarray as xr from tqdm import tqdm from sklearn import preprocessing import seaborn as sns import matplotlib.pyplot as plt plt . style . use ( 'ggplot' ) % matplotlib inline % load_ext autoreload % autoreload 2 amip_data_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" era5_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/era5/\" ncep_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/ncep/\" results_path = f \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/\" fig_path = f \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/climate/\"","title":"Data - Climate Models"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#demo-experiment","text":"","title":"Demo Experiment"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#experimental-paams","text":"class DataArgs : data_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/raw/amip/\" results_path = \"/home/emmanuel/projects/2020_rbig_rs/data/climate/results/amip\" class CMIPArgs : # Fixed Params spatial_windows = [ 1 , 2 , # Spatial Window for Density Cubes 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] # Free Params variables = [ 'psl' # Mean Surface Pressure ] cmip_models = [ \"inmcm4\" , \"access1_0\" , \"bcc_csm1_1\" , \"bcc_csm1_1_m\" , \"bnu_esm\" , \"giss_e2_r\" , \"cnrm_cm5\" , \"ipsl_cm5a_lr\" , \"ipsl_cm5a_mr\" , \"ipsl_cm5b_lr\" , \"mpi_esm_lr\" , \"mpi_esm_mr\" , \"noresm1_m\" , ] base_models = [ 'ncep' , \"era5\" ]","title":"Experimental Paams"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#part-i-grab-data","text":"from src.data.climate.amip import get_base_model base_dat = get_base_model ( CMIPArgs . base_models [ 0 ], CMIPArgs . variables [ 0 ]) # base_dat from src.data.climate.cmip5 import get_cmip5_model cmip_dat = get_cmip5_model ( CMIPArgs . cmip_models [ 0 ], CMIPArgs . variables [ 0 ]) # cmip_dat","title":"Part I - Grab Data"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#part-ii-regrid-data","text":"base_dat , cmip_dat = regrid_2_lower_res ( base_dat , cmip_dat ) assert ( base_dat . shape [ 1 ] == cmip_dat . shape [ 1 ]) assert ( base_dat . shape [ 2 ] == cmip_dat . shape [ 2 ]) # base_dat Create weight file: nearest_s2d_120x180_73x144.nc Remove file nearest_s2d_120x180_73x144.nc","title":"Part II - Regrid Data"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#part-iii-find-overlapping-times","text":"base_dat . shape , cmip_dat . shape ((489, 73, 144), (360, 73, 144)) base_dat , cmip_dat = get_time_overlap ( base_dat , cmip_dat )","title":"Part III - Find Overlapping Times"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#part-iv-get-density-cubes","text":"base_df = get_spatial_cubes ( base_dat , CMIPArgs . spatial_windows [ 3 ]) cmip_df = get_spatial_cubes ( cmip_dat , CMIPArgs . spatial_windows [ 3 ]) base_df . shape (3543330, 16)","title":"Part IV - Get Density Cubes"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#normalize","text":"base_df = normalize_data ( base_df ) cmip_df = normalize_data ( cmip_df )","title":"Normalize"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#information-theory-measures_1","text":"","title":"Information Theory Measures"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#entropy-hxx","text":"subsample = 10_000 batch_size = None bootstrap = False ent_est = RBIGENTEST ( batch_size = batch_size , bootstrap = bootstrap , ) ent_est . fit ( base_df [: subsample ]) h = ent_est . score ( base_df [: subsample ]) h -33.116467738349236","title":"Entropy, H(XX)"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#with-bootstrap","text":"batch_size = 10_000 bootstrap = True n_iterations = 100 ent_est = RBIGENTEST ( batch_size = batch_size , bootstrap = bootstrap , n_iterations = n_iterations ) ent_est . fit ( base_df ) h = ent_est . score ( base_df ) h -31.881844520814997 plt . hist ( ent_est . raw_scores ) -28.48503649185888 plt . hist ( ent_est . raw_scores ) (array([ 1., 0., 1., 0., 9., 11., 26., 28., 14., 10.]), array([-30.9162801 , -30.56606644, -30.21585278, -29.86563913, -29.51542547, -29.16521182, -28.81499816, -28.46478451, -28.11457085, -27.76435719, -27.41414354]), <a list of 10 Patch objects>)","title":"with Bootstrap"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#w-batches","text":"subsample = 40_000 ent_est = RBIGENTEST ( batch_size = 10_000 ) ent_est . fit ( base_df [: subsample ]) h = ent_est . score ( base_df [: subsample ]) h -31.84759524855099 ent_est . raw_scores [-32.17903374504498, -31.753140917432507, -31.67399995592763, -31.784206375798846]","title":"W. Batches"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#total-correlation-tcxx","text":"subsample = 40_000 tc_est = RBIGMIEST ( batch_size = None ) tc_est . fit ( base_df [: subsample ]) tc = tc_est . score ( base_df [: subsample ]) tc 51.735384060195784","title":"Total Correlation, TC(XX)"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#w-batches_1","text":"subsample = 40_000 tc_est = RBIGMIEST ( batch_size = 10_000 ) tc_est . fit ( base_df [: subsample ]) tc = tc_est . score ( base_df [: subsample ]) tc 50.6219155716329 tc_est . raw_scores [50.29844313632438, 51.03391865505402, 50.72933249988033, 50.425967995272856]","title":"w. Batches"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#mutual-information-mixx","text":"subsample = 100_000 mi_est = RBIGMIEST ( batch_size = None ) mi_est . fit ( base_df [: subsample ], cmip_df [: subsample ] ) mi = mi_est . score ( base_df [: subsample ]) mi 1.2438747143982896","title":"Mutual Information, MI(XX)"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#w-batches_2","text":"subsample = 100_000 mi_est = RBIGMIEST ( batch_size = 50_000 ) mi_est . fit ( base_df [: subsample ], cmip_df [: subsample ] ) mi = mi_est . score ( base_df [: subsample ]) mi 1.215228412628969 mi_est . raw_values --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-84-f0a0474b33b1> in <module> ----> 1 mi_est . raw_values AttributeError : 'RBIGEstimator' object has no attribute 'raw_values'","title":"w. Batches"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#mutual-information-ii-hx-hy-hxy","text":"subsample = 100_000 batch_size = 25_000 # H(X) print ( 'H(X)' ) x_ent_est = RBIGENTEST ( batch_size = batch_size ) x_ent_est . fit ( base_df . values [: subsample ]) h_x = x_ent_est . score ( base_df . values [: subsample ]) # H(Y) print ( 'H(Y)' ) y_ent_est = RBIGENTEST ( batch_size = batch_size ) y_ent_est . fit ( cmip_df . values [: subsample ]) h_y = y_ent_est . score ( cmip_df . values [: subsample ]) # H(X,Y) print ( 'H(X,Y)' ) xy_ent_est = RBIGENTEST ( batch_size = 50_000 ) xy_ent_est . fit ( np . hstack ( ( base_df . values [: subsample ], cmip_df . values [: subsample ] ) ), ) h_xy = xy_ent_est . score ( base_df . values [: subsample ]) H(X) H(Y) H(X,Y) # H(X,Y) print ( 'H(X,Y)' ) xy_ent_est = RBIGENTEST ( batch_size = 50_000 ) xy_ent_est . fit ( np . hstack ( ( base_df . values [: subsample ], cmip_df . values [: subsample ] ) ), ) h_xy = xy_ent_est . score ( base_df . values [: subsample ]) h_xy H(X,Y) 165.23978712025018 h_x , h_y , h_xy , h_x + h_y - h_xy (79.10616714936484, 87.19046271977632, 165.45410606367204, 0.8425238054691135) 0.4360788203771051","title":"Mutual Information II, H(X) + H(Y) - H(X,Y)"},{"location":"notebooks/climate/rcp/3.2_demo_it_measures-Copy1/#correlation-pearson-spearman-kendalltau","text":"pear = stats . pearsonr ( base_df [: subsample ] . ravel (), cmip_df [: subsample ] . ravel (), ) spear = stats . spearmanr ( base_df [: subsample ] . ravel (), cmip_df [: subsample ] . ravel (), ) kend = stats . kendalltau ( base_df [: subsample ] . ravel (), cmip_df [: subsample ] . ravel (), ) pear [ 0 ], spear [ 0 ], kend [ 0 ]","title":"Correlation: Pearson, Spearman, KendallTau"},{"location":"notebooks/drought/1.1_drought_features/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Droughts - Pre-Processing \u00b6 In this notebook, I will be going over the preprocessing steps needed before starting the experiments. I will include the following steps: Load Data Select California Fill NANs Smoothing of the VOD signal (savgol filter) Removing the climatology Select drought years and non-drought years Extract density cubes Code \u00b6 import sys , os cwd = os . getcwd () sys . path . insert ( 0 , f ' { cwd } /../../' ) sys . path . insert ( 0 , '/home/emmanuel/code/py_esdc' ) import xarray as xr import pandas as pd import numpy as np # drought tools from src.data.drought.loader import DataLoader from src.features.drought.build_features import ( get_cali_geometry , mask_datacube , smooth_vod_signal , remove_climatology ) from src.visualization.drought.analysis import plot_mean_time # esdc tools from esdc.subset import select_pixel from esdc.shape import ShapeFileExtract , rasterize from esdc.transform import DensityCubes import matplotlib.pyplot as plt import cartopy import cartopy.crs as ccrs plt . style . use ([ 'fivethirtyeight' , 'seaborn-poster' ]) % matplotlib inline % load_ext autoreload % autoreload 2 [autoreload of src.features.drought.build_features failed: Traceback (most recent call last): File \"/home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check superreload(m, reload, self.old_objects) File \"/home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 434, in superreload module = reload(module) File \"/home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/imp.py\", line 315, in reload return importlib.reload(module) File \"/home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/importlib/__init__.py\", line 166, in reload _bootstrap._exec(spec, module) File \"<frozen importlib._bootstrap>\", line 618, in _exec File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module File \"<frozen importlib._bootstrap_external>\", line 781, in get_code File \"<frozen importlib._bootstrap_external>\", line 741, in source_to_code File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed File \"/home/emmanuel/projects/2020_rbig_rs/notebooks/drought/../../src/features/drought/build_features.py\", line 53 def remove_climatology(ds: Union[xr.DataArray, xr.Dataset])-> ds: Union[xr.DataArray, xr.Dataset]: ^ SyntaxError: invalid syntax ] --------------------------------------------------------------------------- ImportError Traceback (most recent call last) <ipython-input-27-c08a15771a60> in <module> 11 # drought tools 12 from src . data . drought . loader import DataLoader ---> 13 from src.features.drought.build_features import ( 14 get_cali_geometry , 15 mask_datacube , ImportError : cannot import name 'remove_climatology' 1. Load Data \u00b6 region = 'conus' sampling = '14D' drought_cube = DataLoader () . load_data ( region , sampling ) pixel = ( - 121 , 37 ) drought_cube <xarray.Dataset> Dimensions: (lat: 461, lon: 865, time: 146) Coordinates: * lat (lat) float64 25.88 25.93 25.98 26.03 ... 48.74 48.79 48.84 48.89 * lon (lon) float64 -124.4 -124.3 -124.2 -124.2 ... -80.64 -80.59 -80.54 * time (time) datetime64[ns] 2010-06-01 2010-06-15 ... 2015-12-22 Data variables: SMADI (lat, lon, time) float64 ... LST (lat, lon, time) float64 ... NDVI (lat, lon, time) float64 ... VOD (lat, lon, time) float64 ... SM (lat, lon, time) float64 ... Verify with a simple plot. plot_mean_time ( drought_cube . LST . sel ( time = slice ( 'June-2010' , 'June-2010' )) ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xarray/core/nanops.py:140: RuntimeWarning: Mean of empty slice return np.nanmean(a, axis=axis, dtype=dtype) 2. Subset California \u00b6 # get california polygon cali_geoms = get_cali_geometry () # get california cube subset cali_cube = mask_datacube ( drought_cube , cali_geoms ) plot_mean_time ( cali_cube . LST . sel ( time = slice ( 'June-2011' , 'June-2011' )) ) 3. Interpolate NANs - Time Dimension \u00b6 # interpolation arguments interp_dim = 'time' method = 'linear' # do interpolation cali_cube_interp = cali_cube . interpolate_na ( dim = interp_dim , method = method ) 4. Smoothing the Signal (VOD) \u00b6 In this section, we will try to smooth the signal with two methods: Simple - Rolling mean Using a savgol filter. Some initial parameters: Window Size = 5 Polynomial Order = 3 We will apply this filter in the time domain only. vod_data = cali_cube_interp . VOD vod_data <xarray.DataArray 'VOD' (lat: 189, lon: 103, time: 146)> array([[[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], ..., [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]]]) Coordinates: * lat (lat) float64 32.53 32.58 32.63 32.68 ... 41.79 41.84 41.89 41.94 * lon (lon) float64 -124.4 -124.3 -124.2 -124.2 ... -118.7 -118.7 -118.6 * time (time) datetime64[ns] 2010-06-01 2010-06-15 ... 2015-12-22 4.1 - Savgol Filter \u00b6 from scipy.signal import savgol_filter # select example vod_data_ex = select_pixel ( vod_data , pixel ) # savgol filter params window_length = 5 polyorder = 3 # apply savgol filter vod_smooth_filter = savgol_filter ( vod_data_ex , window_length = window_length , polyorder = polyorder ) fig , ax = plt . subplots ( nrows = 2 , figsize = ( 10 , 10 )) ax [ 0 ] . plot ( vod_data_ex ) ax [ 0 ] . set_title ( 'Original Data' ) ax [ 1 ] . plot ( vod_smooth_filter ) ax [ 1 ] . set_title ( 'After Savgol Filter' ) plt . show () 4.2 - Rolling Window \u00b6 # select example vod_data_ex = select_pixel ( vod_data , pixel ) # savgol filter params window_length = 2 # apply savgol filter vod_smooth_roll = vod_data_ex . rolling ( time = window_length , center = True ) . mean () fig , ax = plt . subplots ( nrows = 2 , figsize = ( 10 , 10 )) ax [ 0 ] . plot ( vod_data_ex ) ax [ 0 ] . set_title ( 'Original Data' ) ax [ 1 ] . plot ( vod_smooth_roll ) ax [ 1 ] . set_title ( 'After Rolling Mean' ) plt . show () 4.3 - Difference \u00b6 vod_smooth_diff = vod_smooth_filter - vod_smooth_roll fig , ax = plt . subplots ( nrows = 4 , figsize = ( 10 , 10 )) ax [ 0 ] . plot ( vod_data_ex ) ax [ 0 ] . set_title ( 'Original' ) ax [ 1 ] . plot ( vod_smooth_filter ) ax [ 1 ] . set_title ( 'Savgol Filter' ) ax [ 2 ] . plot ( vod_smooth_roll ) ax [ 2 ] . set_title ( 'Rolling Mean' ) ax [ 3 ] . plot ( vod_smooth_diff ) ax [ 3 ] . set_title ( 'Difference' ) # Scale the Difference Y-Limits ymax = np . max ([ vod_smooth_filter . max (), vod_smooth_roll . max ()]) ymin = np . min ([ vod_smooth_filter . min (), vod_smooth_roll . min ()]) center = ( ymax - ymin ) ymax = ymax - center ymin = center - ymin ax [ 3 ] . set_ylim ([ 0 - ymin , 0 + ymax ]) plt . tight_layout () plt . show () 4.3 - Apply Rolling Mean to the whole dataset \u00b6 cali_cube_interp = smooth_vod_signal ( cali_cube_interp , window_length = 2 , center = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xarray/core/nanops.py:140: RuntimeWarning: Mean of empty slice return np.nanmean(a, axis=axis, dtype=dtype) 5. Remove Climatology \u00b6 When I mean 'climatology', I mean the difference between observations and typical weather for a particular season. The anomalies should not show up in the seasonal cycle. I'll just do a very simple removal. I'll calculate the monthly mean wrt time and then remove that from each month from the original datacube. Steps Climatalogy - Monthly Mean for the 6 years Remove Climatology - Climatology from each month # calculate the climatology cali_climatology_mean = calculate_monthly_mean ( cali_cube_interp ) # remove climatology cali_anomalies = cali_cube . groupby ( 'time.month' ) - cali_climatology_mean Simple check where we look at the original and the new. variables = [ 'LST' , 'VOD' , 'NDVI' , 'SM' ] for ivariable in variables : fig , ax = plt . subplots ( nrows = 3 , figsize = ( 10 , 10 )) # Before Climatology select_pixel ( cali_cube_interp [ ivariable ], pixel ) . plot ( ax = ax [ 0 ]) ax [ 0 ] . set_title ( 'Original Time Series' ) # Climatology select_pixel ( cali_climatology_mean [ ivariable ], pixel ) . plot ( ax = ax [ 1 ]) ax [ 1 ] . set_title ( 'Climatology' ) # After Climatology select_pixel ( cali_anomalies [ ivariable ], pixel ) . plot ( ax = ax [ 2 ]) ax [ 2 ] . set_title ( 'After Climatology Median Removed' ) plt . tight_layout () plt . show () 6. EMData \u00b6 I extract the dates for the drought events for california. This will allow me to separate the drought years and non-drought years. ! ls / media / disk / databases / SMADI / EMDAT_validation / EMDAT_Drought_info.dbf EMDAT_Drought_info.shx EMDAT_Drought_info.shp figuresStudy_regions.png shape_files = '/media/disk/databases/SMADI/EMDAT_validation/' shapefiles_clf = ShapeFileExtract () shapefiles_clf . import_shape_files ( shape_files ); # Extract Europe query = 'LOCATION' subqueries = [ 'California' ] cali_droughts = shapefiles_clf . extract_queries ( query = query , subqueries = subqueries ) cali_droughts .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID ISO LOCATION Date_Start Date_End UTC_Start UTC_End geometry 105 106 USA California 01-Jun-2012 31-Dec-2012 735021 735234 (POLYGON ((-117.2328491210938 32.7764053344726... 132 133 USA California 01-Jan-2014 31-Dec-2014 735600 735964 (POLYGON ((-117.2328491210938 32.7764053344726... 153 154 USA California 01-Jan-2015 31-Dec-2015 735965 736329 (POLYGON ((-117.2328491210938 32.7764053344726... So the drought years are: Drought Years 2012 2014 2015 Non-Drought Years 2010 2011 2013 Note : Even though the EM-Data says that the drought year for 2012 is only half a year, we're going to say that that is a full year. # drought cali_anomalies_drought = xr . concat ([ cali_anomalies . sel ( time = slice ( '2012' , '2012' )), cali_anomalies . sel ( time = slice ( '2014' , '2014' )), cali_anomalies . sel ( time = slice ( '2015' , '2015' )), ], dim = 'time' ) # non-drought cali_anomalies_nondrought = xr . concat ([ cali_anomalies . sel ( time = slice ( '2010' , '2010' )), cali_anomalies . sel ( time = slice ( '2011' , '2011' )), cali_anomalies . sel ( time = slice ( '2013' , '2013' )), ], dim = 'time' ) 7. Extract Density Cubes \u00b6 In this step, we will construct 'density cubes'. These are cubes where we add features from a combination of the spatial and/or temporal dimensions. Instead of a single sample, we have a sample that takes into account spatial and/or temporal information. In this experiment, we will only look at temporal information. Our temporal resolution is 14 Days and we want to look at a maximum of 6 months. So: \\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps \\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps <span><span class=\"MathJax_Preview\">\\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps</span><script type=\"math/tex\">\\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps # confirm sub_ = cali_anomalies_drought . isel ( time = slice ( 0 , 12 )) sub_ . time [ 0 ] . data , sub_ . time [ - 1 ] . data (array('2012-01-10T00:00:00.000000000', dtype='datetime64[ns]'), array('2012-06-12T00:00:00.000000000', dtype='datetime64[ns]')) cali_anomalies . sel ( time = slice ( '2012' , '2012' )) <xarray.Dataset> Dimensions: (lat: 189, lon: 103, time: 26) Coordinates: * lon (lon) float64 -124.4 -124.3 -124.2 ... -118.7 -118.7 -118.6 * lat (lat) float64 32.53 32.58 32.63 32.68 ... 41.79 41.84 41.89 41.94 * time (time) datetime64[ns] 2012-01-10 2012-01-24 ... 2012-12-25 month (time) int64 1 1 2 2 3 3 4 4 5 5 5 ... 8 9 9 10 10 10 11 11 12 12 Data variables: SMADI (lat, lon, time) float64 nan nan nan nan nan ... nan nan nan nan LST (lat, lon, time) float64 nan nan nan nan nan ... nan nan nan nan NDVI (lat, lon, time) float64 nan nan nan nan nan ... nan nan nan nan VOD (lat, lon, time) float64 nan nan nan nan nan ... nan nan nan nan SM (lat, lon, time) float64 nan nan nan nan nan ... nan nan nan nan cali_mask (time, lat, lon) float64 nan nan nan nan nan ... nan nan nan nan l1 = [ 'time' , 'lat' , 'lon' , 'depth' ] l2 = [ 'lat' , 'lon' , 'time' ] all ([ i in l1 for i in l2 ]) True So we get roughly 6 months of temporal information in our density cubes. 7.1 - Example Density Cube \u00b6 # example size spatial_window = 1 time_window = 12 # initialize datacube minicuber = DensityCubes ( spatial_window = spatial_window , time_window = time_window ) # initialize dataframes drought_VOD = pd . DataFrame () drought_LST = pd . DataFrame () drought_NDVI = pd . DataFrame () drought_SM = pd . DataFrame () # Group by year and get minicubes for iyear , igroup in cali_anomalies_drought . groupby ( 'time.year' ): print ( f \"Year: { iyear } \" ) # get minicubes for variables drought_VOD = drought_VOD . append ( minicuber . get_minicubes ( igroup . VOD )) drought_LST = drought_LST . append ( minicuber . get_minicubes ( igroup . LST )) drought_NDVI = drought_NDVI . append ( minicuber . get_minicubes ( igroup . NDVI )) drought_SM = drought_SM . append ( minicuber . get_minicubes ( igroup . SM )) Year: 2012 Year: 2014 Year: 2015 drought_VOD . shape , drought_LST . shape , drought_NDVI . shape , drought_SM . shape ((54481, 12), (85995, 12), (73771, 12), (54481, 12))","title":"1.1 drought features"},{"location":"notebooks/drought/1.1_drought_features/#droughts-pre-processing","text":"In this notebook, I will be going over the preprocessing steps needed before starting the experiments. I will include the following steps: Load Data Select California Fill NANs Smoothing of the VOD signal (savgol filter) Removing the climatology Select drought years and non-drought years Extract density cubes","title":"Droughts - Pre-Processing"},{"location":"notebooks/drought/1.1_drought_features/#code","text":"import sys , os cwd = os . getcwd () sys . path . insert ( 0 , f ' { cwd } /../../' ) sys . path . insert ( 0 , '/home/emmanuel/code/py_esdc' ) import xarray as xr import pandas as pd import numpy as np # drought tools from src.data.drought.loader import DataLoader from src.features.drought.build_features import ( get_cali_geometry , mask_datacube , smooth_vod_signal , remove_climatology ) from src.visualization.drought.analysis import plot_mean_time # esdc tools from esdc.subset import select_pixel from esdc.shape import ShapeFileExtract , rasterize from esdc.transform import DensityCubes import matplotlib.pyplot as plt import cartopy import cartopy.crs as ccrs plt . style . use ([ 'fivethirtyeight' , 'seaborn-poster' ]) % matplotlib inline % load_ext autoreload % autoreload 2 [autoreload of src.features.drought.build_features failed: Traceback (most recent call last): File \"/home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check superreload(m, reload, self.old_objects) File \"/home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 434, in superreload module = reload(module) File \"/home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/imp.py\", line 315, in reload return importlib.reload(module) File \"/home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/importlib/__init__.py\", line 166, in reload _bootstrap._exec(spec, module) File \"<frozen importlib._bootstrap>\", line 618, in _exec File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module File \"<frozen importlib._bootstrap_external>\", line 781, in get_code File \"<frozen importlib._bootstrap_external>\", line 741, in source_to_code File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed File \"/home/emmanuel/projects/2020_rbig_rs/notebooks/drought/../../src/features/drought/build_features.py\", line 53 def remove_climatology(ds: Union[xr.DataArray, xr.Dataset])-> ds: Union[xr.DataArray, xr.Dataset]: ^ SyntaxError: invalid syntax ] --------------------------------------------------------------------------- ImportError Traceback (most recent call last) <ipython-input-27-c08a15771a60> in <module> 11 # drought tools 12 from src . data . drought . loader import DataLoader ---> 13 from src.features.drought.build_features import ( 14 get_cali_geometry , 15 mask_datacube , ImportError : cannot import name 'remove_climatology'","title":"Code"},{"location":"notebooks/drought/1.1_drought_features/#1-load-data","text":"region = 'conus' sampling = '14D' drought_cube = DataLoader () . load_data ( region , sampling ) pixel = ( - 121 , 37 ) drought_cube <xarray.Dataset> Dimensions: (lat: 461, lon: 865, time: 146) Coordinates: * lat (lat) float64 25.88 25.93 25.98 26.03 ... 48.74 48.79 48.84 48.89 * lon (lon) float64 -124.4 -124.3 -124.2 -124.2 ... -80.64 -80.59 -80.54 * time (time) datetime64[ns] 2010-06-01 2010-06-15 ... 2015-12-22 Data variables: SMADI (lat, lon, time) float64 ... LST (lat, lon, time) float64 ... NDVI (lat, lon, time) float64 ... VOD (lat, lon, time) float64 ... SM (lat, lon, time) float64 ... Verify with a simple plot. plot_mean_time ( drought_cube . LST . sel ( time = slice ( 'June-2010' , 'June-2010' )) ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xarray/core/nanops.py:140: RuntimeWarning: Mean of empty slice return np.nanmean(a, axis=axis, dtype=dtype)","title":"1. Load Data"},{"location":"notebooks/drought/1.1_drought_features/#2-subset-california","text":"# get california polygon cali_geoms = get_cali_geometry () # get california cube subset cali_cube = mask_datacube ( drought_cube , cali_geoms ) plot_mean_time ( cali_cube . LST . sel ( time = slice ( 'June-2011' , 'June-2011' )) )","title":"2. Subset California"},{"location":"notebooks/drought/1.1_drought_features/#3-interpolate-nans-time-dimension","text":"# interpolation arguments interp_dim = 'time' method = 'linear' # do interpolation cali_cube_interp = cali_cube . interpolate_na ( dim = interp_dim , method = method )","title":"3. Interpolate NANs - Time Dimension"},{"location":"notebooks/drought/1.1_drought_features/#4-smoothing-the-signal-vod","text":"In this section, we will try to smooth the signal with two methods: Simple - Rolling mean Using a savgol filter. Some initial parameters: Window Size = 5 Polynomial Order = 3 We will apply this filter in the time domain only. vod_data = cali_cube_interp . VOD vod_data <xarray.DataArray 'VOD' (lat: 189, lon: 103, time: 146)> array([[[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], ..., [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]]]) Coordinates: * lat (lat) float64 32.53 32.58 32.63 32.68 ... 41.79 41.84 41.89 41.94 * lon (lon) float64 -124.4 -124.3 -124.2 -124.2 ... -118.7 -118.7 -118.6 * time (time) datetime64[ns] 2010-06-01 2010-06-15 ... 2015-12-22","title":"4. Smoothing the Signal (VOD)"},{"location":"notebooks/drought/1.1_drought_features/#41-savgol-filter","text":"from scipy.signal import savgol_filter # select example vod_data_ex = select_pixel ( vod_data , pixel ) # savgol filter params window_length = 5 polyorder = 3 # apply savgol filter vod_smooth_filter = savgol_filter ( vod_data_ex , window_length = window_length , polyorder = polyorder ) fig , ax = plt . subplots ( nrows = 2 , figsize = ( 10 , 10 )) ax [ 0 ] . plot ( vod_data_ex ) ax [ 0 ] . set_title ( 'Original Data' ) ax [ 1 ] . plot ( vod_smooth_filter ) ax [ 1 ] . set_title ( 'After Savgol Filter' ) plt . show ()","title":"4.1 - Savgol Filter"},{"location":"notebooks/drought/1.1_drought_features/#42-rolling-window","text":"# select example vod_data_ex = select_pixel ( vod_data , pixel ) # savgol filter params window_length = 2 # apply savgol filter vod_smooth_roll = vod_data_ex . rolling ( time = window_length , center = True ) . mean () fig , ax = plt . subplots ( nrows = 2 , figsize = ( 10 , 10 )) ax [ 0 ] . plot ( vod_data_ex ) ax [ 0 ] . set_title ( 'Original Data' ) ax [ 1 ] . plot ( vod_smooth_roll ) ax [ 1 ] . set_title ( 'After Rolling Mean' ) plt . show ()","title":"4.2 - Rolling Window"},{"location":"notebooks/drought/1.1_drought_features/#43-difference","text":"vod_smooth_diff = vod_smooth_filter - vod_smooth_roll fig , ax = plt . subplots ( nrows = 4 , figsize = ( 10 , 10 )) ax [ 0 ] . plot ( vod_data_ex ) ax [ 0 ] . set_title ( 'Original' ) ax [ 1 ] . plot ( vod_smooth_filter ) ax [ 1 ] . set_title ( 'Savgol Filter' ) ax [ 2 ] . plot ( vod_smooth_roll ) ax [ 2 ] . set_title ( 'Rolling Mean' ) ax [ 3 ] . plot ( vod_smooth_diff ) ax [ 3 ] . set_title ( 'Difference' ) # Scale the Difference Y-Limits ymax = np . max ([ vod_smooth_filter . max (), vod_smooth_roll . max ()]) ymin = np . min ([ vod_smooth_filter . min (), vod_smooth_roll . min ()]) center = ( ymax - ymin ) ymax = ymax - center ymin = center - ymin ax [ 3 ] . set_ylim ([ 0 - ymin , 0 + ymax ]) plt . tight_layout () plt . show ()","title":"4.3 - Difference"},{"location":"notebooks/drought/1.1_drought_features/#43-apply-rolling-mean-to-the-whole-dataset","text":"cali_cube_interp = smooth_vod_signal ( cali_cube_interp , window_length = 2 , center = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xarray/core/nanops.py:140: RuntimeWarning: Mean of empty slice return np.nanmean(a, axis=axis, dtype=dtype)","title":"4.3 - Apply Rolling Mean to the whole dataset"},{"location":"notebooks/drought/1.1_drought_features/#5-remove-climatology","text":"When I mean 'climatology', I mean the difference between observations and typical weather for a particular season. The anomalies should not show up in the seasonal cycle. I'll just do a very simple removal. I'll calculate the monthly mean wrt time and then remove that from each month from the original datacube. Steps Climatalogy - Monthly Mean for the 6 years Remove Climatology - Climatology from each month # calculate the climatology cali_climatology_mean = calculate_monthly_mean ( cali_cube_interp ) # remove climatology cali_anomalies = cali_cube . groupby ( 'time.month' ) - cali_climatology_mean Simple check where we look at the original and the new. variables = [ 'LST' , 'VOD' , 'NDVI' , 'SM' ] for ivariable in variables : fig , ax = plt . subplots ( nrows = 3 , figsize = ( 10 , 10 )) # Before Climatology select_pixel ( cali_cube_interp [ ivariable ], pixel ) . plot ( ax = ax [ 0 ]) ax [ 0 ] . set_title ( 'Original Time Series' ) # Climatology select_pixel ( cali_climatology_mean [ ivariable ], pixel ) . plot ( ax = ax [ 1 ]) ax [ 1 ] . set_title ( 'Climatology' ) # After Climatology select_pixel ( cali_anomalies [ ivariable ], pixel ) . plot ( ax = ax [ 2 ]) ax [ 2 ] . set_title ( 'After Climatology Median Removed' ) plt . tight_layout () plt . show ()","title":"5. Remove Climatology"},{"location":"notebooks/drought/1.1_drought_features/#6-emdata","text":"I extract the dates for the drought events for california. This will allow me to separate the drought years and non-drought years. ! ls / media / disk / databases / SMADI / EMDAT_validation / EMDAT_Drought_info.dbf EMDAT_Drought_info.shx EMDAT_Drought_info.shp figuresStudy_regions.png shape_files = '/media/disk/databases/SMADI/EMDAT_validation/' shapefiles_clf = ShapeFileExtract () shapefiles_clf . import_shape_files ( shape_files ); # Extract Europe query = 'LOCATION' subqueries = [ 'California' ] cali_droughts = shapefiles_clf . extract_queries ( query = query , subqueries = subqueries ) cali_droughts .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID ISO LOCATION Date_Start Date_End UTC_Start UTC_End geometry 105 106 USA California 01-Jun-2012 31-Dec-2012 735021 735234 (POLYGON ((-117.2328491210938 32.7764053344726... 132 133 USA California 01-Jan-2014 31-Dec-2014 735600 735964 (POLYGON ((-117.2328491210938 32.7764053344726... 153 154 USA California 01-Jan-2015 31-Dec-2015 735965 736329 (POLYGON ((-117.2328491210938 32.7764053344726... So the drought years are: Drought Years 2012 2014 2015 Non-Drought Years 2010 2011 2013 Note : Even though the EM-Data says that the drought year for 2012 is only half a year, we're going to say that that is a full year. # drought cali_anomalies_drought = xr . concat ([ cali_anomalies . sel ( time = slice ( '2012' , '2012' )), cali_anomalies . sel ( time = slice ( '2014' , '2014' )), cali_anomalies . sel ( time = slice ( '2015' , '2015' )), ], dim = 'time' ) # non-drought cali_anomalies_nondrought = xr . concat ([ cali_anomalies . sel ( time = slice ( '2010' , '2010' )), cali_anomalies . sel ( time = slice ( '2011' , '2011' )), cali_anomalies . sel ( time = slice ( '2013' , '2013' )), ], dim = 'time' )","title":"6. EMData"},{"location":"notebooks/drought/1.1_drought_features/#7-extract-density-cubes","text":"In this step, we will construct 'density cubes'. These are cubes where we add features from a combination of the spatial and/or temporal dimensions. Instead of a single sample, we have a sample that takes into account spatial and/or temporal information. In this experiment, we will only look at temporal information. Our temporal resolution is 14 Days and we want to look at a maximum of 6 months. So: \\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps \\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps <span><span class=\"MathJax_Preview\">\\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps</span><script type=\"math/tex\">\\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps # confirm sub_ = cali_anomalies_drought . isel ( time = slice ( 0 , 12 )) sub_ . time [ 0 ] . data , sub_ . time [ - 1 ] . data (array('2012-01-10T00:00:00.000000000', dtype='datetime64[ns]'), array('2012-06-12T00:00:00.000000000', dtype='datetime64[ns]')) cali_anomalies . sel ( time = slice ( '2012' , '2012' )) <xarray.Dataset> Dimensions: (lat: 189, lon: 103, time: 26) Coordinates: * lon (lon) float64 -124.4 -124.3 -124.2 ... -118.7 -118.7 -118.6 * lat (lat) float64 32.53 32.58 32.63 32.68 ... 41.79 41.84 41.89 41.94 * time (time) datetime64[ns] 2012-01-10 2012-01-24 ... 2012-12-25 month (time) int64 1 1 2 2 3 3 4 4 5 5 5 ... 8 9 9 10 10 10 11 11 12 12 Data variables: SMADI (lat, lon, time) float64 nan nan nan nan nan ... nan nan nan nan LST (lat, lon, time) float64 nan nan nan nan nan ... nan nan nan nan NDVI (lat, lon, time) float64 nan nan nan nan nan ... nan nan nan nan VOD (lat, lon, time) float64 nan nan nan nan nan ... nan nan nan nan SM (lat, lon, time) float64 nan nan nan nan nan ... nan nan nan nan cali_mask (time, lat, lon) float64 nan nan nan nan nan ... nan nan nan nan l1 = [ 'time' , 'lat' , 'lon' , 'depth' ] l2 = [ 'lat' , 'lon' , 'time' ] all ([ i in l1 for i in l2 ]) True So we get roughly 6 months of temporal information in our density cubes.","title":"7. Extract Density Cubes"},{"location":"notebooks/drought/1.1_drought_features/#71-example-density-cube","text":"# example size spatial_window = 1 time_window = 12 # initialize datacube minicuber = DensityCubes ( spatial_window = spatial_window , time_window = time_window ) # initialize dataframes drought_VOD = pd . DataFrame () drought_LST = pd . DataFrame () drought_NDVI = pd . DataFrame () drought_SM = pd . DataFrame () # Group by year and get minicubes for iyear , igroup in cali_anomalies_drought . groupby ( 'time.year' ): print ( f \"Year: { iyear } \" ) # get minicubes for variables drought_VOD = drought_VOD . append ( minicuber . get_minicubes ( igroup . VOD )) drought_LST = drought_LST . append ( minicuber . get_minicubes ( igroup . LST )) drought_NDVI = drought_NDVI . append ( minicuber . get_minicubes ( igroup . NDVI )) drought_SM = drought_SM . append ( minicuber . get_minicubes ( igroup . SM )) Year: 2012 Year: 2014 Year: 2015 drought_VOD . shape , drought_LST . shape , drought_NDVI . shape , drought_SM . shape ((54481, 12), (85995, 12), (73771, 12), (54481, 12))","title":"7.1 - Example Density Cube"},{"location":"notebooks/drought/1_drought_features/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Droughts - Pre-Processing \u00b6 In this notebook, I will be going over the preprocessing steps needed before starting the experiments. I will include the following steps: Load Data Select California Fill NANs Smoothing of the VOD signal (savgol filter) Removing the climatology Select drought years and non-drought years Extract density cubes Code \u00b6 import sys , os cwd = os . getcwd () sys . path . insert ( 0 , f ' { cwd } /../../' ) sys . path . insert ( 0 , '/home/emmanuel/code/py_esdc' ) import xarray as xr import pandas as pd import numpy as np # drought tools from src.data.drought.loader import DataLoader from src.features.drought.build_features import ( get_cali_geometry , mask_datacube , smooth_vod_signal , remove_climatology , get_cali_emdata , get_drought_years , get_density_cubes , get_common_elements , normalize ) from src.visualization.drought.analysis import plot_mean_time # esdc tools from esdc.subset import select_pixel from esdc.shape import ShapeFileExtract , rasterize from esdc.transform import DensityCubes import matplotlib.pyplot as plt import cartopy import cartopy.crs as ccrs plt . style . use ([ 'fivethirtyeight' , 'seaborn-poster' ]) % matplotlib inline % load_ext autoreload % autoreload 2 1. Load Data \u00b6 region = 'conus' sampling = '14D' drought_cube = DataLoader () . load_data ( region , sampling ) pixel = ( - 121 , 37 ) drought_cube <xarray.Dataset> Dimensions: (lat: 461, lon: 865, time: 146) Coordinates: * lat (lat) float64 25.88 25.93 25.98 26.03 ... 48.74 48.79 48.84 48.89 * lon (lon) float64 -124.4 -124.3 -124.2 -124.2 ... -80.64 -80.59 -80.54 * time (time) datetime64[ns] 2010-06-01 2010-06-15 ... 2015-12-22 Data variables: SMADI (lat, lon, time) float64 ... LST (lat, lon, time) float64 ... NDVI (lat, lon, time) float64 ... VOD (lat, lon, time) float64 ... SM (lat, lon, time) float64 ... Verify with a simple plot. plot_mean_time ( drought_cube . LST . sel ( time = slice ( 'June-2010' , 'June-2010' )) ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xarray/core/nanops.py:140: RuntimeWarning: Mean of empty slice return np.nanmean(a, axis=axis, dtype=dtype) 2. Subset California \u00b6 # get california polygon cali_geoms = get_cali_geometry () # get california cube subset cali_cube = mask_datacube ( drought_cube , cali_geoms ) plot_mean_time ( cali_cube . LST . sel ( time = slice ( 'June-2011' , 'June-2011' )) ) 3. Interpolate NANs - Time Dimension \u00b6 # interpolation arguments interp_dim = 'time' method = 'linear' # do interpolation cali_cube_interp = cali_cube . interpolate_na ( dim = interp_dim , method = method ) 4. Smoothing the Signal (VOD) \u00b6 In this section, we will try to smooth the signal with two methods: Simple - Rolling mean Using a savgol filter. Some initial parameters: Window Size = 5 Polynomial Order = 3 We will apply this filter in the time domain only. vod_data = cali_cube_interp . VOD vod_data <xarray.DataArray 'VOD' (lat: 189, lon: 103, time: 146)> array([[[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], ..., [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]]]) Coordinates: * lat (lat) float64 32.53 32.58 32.63 32.68 ... 41.79 41.84 41.89 41.94 * lon (lon) float64 -124.4 -124.3 -124.2 -124.2 ... -118.7 -118.7 -118.6 * time (time) datetime64[ns] 2010-06-01 2010-06-15 ... 2015-12-22 4.1 - Savgol Filter \u00b6 from scipy.signal import savgol_filter # select example vod_data_ex = select_pixel ( vod_data , pixel ) # savgol filter params window_length = 5 polyorder = 3 # apply savgol filter vod_smooth_filter = savgol_filter ( vod_data_ex , window_length = window_length , polyorder = polyorder ) fig , ax = plt . subplots ( nrows = 2 , figsize = ( 10 , 10 )) ax [ 0 ] . plot ( vod_data_ex ) ax [ 0 ] . set_title ( 'Original Data' ) ax [ 1 ] . plot ( vod_smooth_filter ) ax [ 1 ] . set_title ( 'After Savgol Filter' ) plt . show () 4.2 - Rolling Window \u00b6 # select example vod_data_ex = select_pixel ( vod_data , pixel ) # savgol filter params window_length = 2 # apply savgol filter vod_smooth_roll = vod_data_ex . rolling ( time = window_length , center = True ) . mean () fig , ax = plt . subplots ( nrows = 2 , figsize = ( 10 , 10 )) ax [ 0 ] . plot ( vod_data_ex ) ax [ 0 ] . set_title ( 'Original Data' ) ax [ 1 ] . plot ( vod_smooth_roll ) ax [ 1 ] . set_title ( 'After Rolling Mean' ) plt . show () 4.3 - Difference \u00b6 vod_smooth_diff = vod_smooth_filter - vod_smooth_roll fig , ax = plt . subplots ( nrows = 4 , figsize = ( 10 , 10 )) ax [ 0 ] . plot ( vod_data_ex ) ax [ 0 ] . set_title ( 'Original' ) ax [ 1 ] . plot ( vod_smooth_filter ) ax [ 1 ] . set_title ( 'Savgol Filter' ) ax [ 2 ] . plot ( vod_smooth_roll ) ax [ 2 ] . set_title ( 'Rolling Mean' ) ax [ 3 ] . plot ( vod_smooth_diff ) ax [ 3 ] . set_title ( 'Difference' ) # Scale the Difference Y-Limits ymax = np . max ([ vod_smooth_filter . max (), vod_smooth_roll . max ()]) ymin = np . min ([ vod_smooth_filter . min (), vod_smooth_roll . min ()]) center = ( ymax - ymin ) ymax = ymax - center ymin = center - ymin ax [ 3 ] . set_ylim ([ 0 - ymin , 0 + ymax ]) plt . tight_layout () plt . show () 4.3 - Apply Rolling Mean to the whole dataset \u00b6 cali_cube_interp = smooth_vod_signal ( cali_cube_interp , window_length = 2 , center = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xarray/core/nanops.py:140: RuntimeWarning: Mean of empty slice return np.nanmean(a, axis=axis, dtype=dtype) 5. Remove Climatology \u00b6 When I mean 'climatology', I mean the difference between observations and typical weather for a particular season. The anomalies should not show up in the seasonal cycle. I'll just do a very simple removal. I'll calculate the monthly mean wrt time and then remove that from each month from the original datacube. Steps Climatalogy - Monthly Mean for the 6 years Remove Climatology - Climatology from each month # remove climatology cali_anomalies , cali_mean = remove_climatology ( cali_cube_interp ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xarray/core/nanops.py:140: RuntimeWarning: Mean of empty slice return np.nanmean(a, axis=axis, dtype=dtype) Simple check where we look at the original and the new. variables = [ 'LST' , 'VOD' , 'NDVI' , 'SM' ] for ivariable in variables : fig , ax = plt . subplots ( nrows = 3 , figsize = ( 10 , 10 )) # Before Climatology select_pixel ( cali_cube_interp [ ivariable ], pixel ) . plot ( ax = ax [ 0 ]) ax [ 0 ] . set_title ( 'Original Time Series' ) # Climatology select_pixel ( cali_mean [ ivariable ], pixel ) . plot ( ax = ax [ 1 ]) ax [ 1 ] . set_title ( 'Climatology' ) # After Climatology select_pixel ( cali_anomalies [ ivariable ], pixel ) . plot ( ax = ax [ 2 ]) ax [ 2 ] . set_title ( 'After Climatology Median Removed' ) plt . tight_layout () plt . show () 6. EMData \u00b6 I extract the dates for the drought events for california. This will allow me to separate the drought years and non-drought years. cali_droughts = get_cali_emdata () cali_droughts .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID ISO LOCATION Date_Start Date_End UTC_Start UTC_End geometry 105 106 USA California 01-Jun-2012 31-Dec-2012 735021 735234 (POLYGON ((-117.2328491210938 32.7764053344726... 132 133 USA California 01-Jan-2014 31-Dec-2014 735600 735964 (POLYGON ((-117.2328491210938 32.7764053344726... 153 154 USA California 01-Jan-2015 31-Dec-2015 735965 736329 (POLYGON ((-117.2328491210938 32.7764053344726... So the drought years are: Drought Years 2012 2014 2015 Non-Drought Years 2010 2011 2013 Note : Even though the EM-Data says that the drought year for 2012 is only half a year, we're going to say that that is a full year. # Drought Years cali_anomalies_drought = get_drought_years ( cali_anomalies , [ '2012' , '2014' , '2015' ] ) # Non-Drought Years cali_anomalies_nondrought = get_drought_years ( cali_anomalies , [ '2010' , '2011' , '2013' ] ) 7. Extract Density Cubes \u00b6 In this step, we will construct 'density cubes'. These are cubes where we add features from a combination of the spatial and/or temporal dimensions. Instead of a single sample, we have a sample that takes into account spatial and/or temporal information. In this experiment, we will only look at temporal information. Our temporal resolution is 14 Days and we want to look at a maximum of 6 months. So: \\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps \\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps <span><span class=\"MathJax_Preview\">\\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps</span><script type=\"math/tex\">\\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps # confirm sub_ = cali_anomalies_drought . isel ( time = slice ( 0 , 12 )) sub_ . time [ 0 ] . data , sub_ . time [ - 1 ] . data (array('2012-01-10T00:00:00.000000000', dtype='datetime64[ns]'), array('2012-06-12T00:00:00.000000000', dtype='datetime64[ns]')) We see that the start date for the year 2012 is 01-10 and the end date is 06-12. It's good enough. So we get roughly 6 months of temporal information in our density cubes. 7.1 - Example Density Cube \u00b6 # window sizes spatial = 1 time = 12 vod_df , lst_df , ndvi_df , sm_df = get_density_cubes ( cali_anomalies_drought , spatial , time ) vod_df . shape , lst_df . shape , ndvi_df . shape , sm_df . shape ((76690, 12), (85995, 12), (82729, 12), (76690, 12)) 8. Find Common Elements \u00b6 Notice how the number of elements is different depending upon the dataset. I believe it is the case that there are less elements for the VOD and the SM datasets. To make a fair comparison, I'll be using only the common elements between the two density cubes. Note : This is also a bit difficult for RBIG to calculate the Mutual Information for datasets that are potentially so different in their domain. vod_df , lst_df = get_common_elements ( vod_df , lst_df ) vod_df . shape , lst_df . shape ((76690, 12), (76690, 12))","title":"1 drought features"},{"location":"notebooks/drought/1_drought_features/#droughts-pre-processing","text":"In this notebook, I will be going over the preprocessing steps needed before starting the experiments. I will include the following steps: Load Data Select California Fill NANs Smoothing of the VOD signal (savgol filter) Removing the climatology Select drought years and non-drought years Extract density cubes","title":"Droughts - Pre-Processing"},{"location":"notebooks/drought/1_drought_features/#code","text":"import sys , os cwd = os . getcwd () sys . path . insert ( 0 , f ' { cwd } /../../' ) sys . path . insert ( 0 , '/home/emmanuel/code/py_esdc' ) import xarray as xr import pandas as pd import numpy as np # drought tools from src.data.drought.loader import DataLoader from src.features.drought.build_features import ( get_cali_geometry , mask_datacube , smooth_vod_signal , remove_climatology , get_cali_emdata , get_drought_years , get_density_cubes , get_common_elements , normalize ) from src.visualization.drought.analysis import plot_mean_time # esdc tools from esdc.subset import select_pixel from esdc.shape import ShapeFileExtract , rasterize from esdc.transform import DensityCubes import matplotlib.pyplot as plt import cartopy import cartopy.crs as ccrs plt . style . use ([ 'fivethirtyeight' , 'seaborn-poster' ]) % matplotlib inline % load_ext autoreload % autoreload 2","title":"Code"},{"location":"notebooks/drought/1_drought_features/#1-load-data","text":"region = 'conus' sampling = '14D' drought_cube = DataLoader () . load_data ( region , sampling ) pixel = ( - 121 , 37 ) drought_cube <xarray.Dataset> Dimensions: (lat: 461, lon: 865, time: 146) Coordinates: * lat (lat) float64 25.88 25.93 25.98 26.03 ... 48.74 48.79 48.84 48.89 * lon (lon) float64 -124.4 -124.3 -124.2 -124.2 ... -80.64 -80.59 -80.54 * time (time) datetime64[ns] 2010-06-01 2010-06-15 ... 2015-12-22 Data variables: SMADI (lat, lon, time) float64 ... LST (lat, lon, time) float64 ... NDVI (lat, lon, time) float64 ... VOD (lat, lon, time) float64 ... SM (lat, lon, time) float64 ... Verify with a simple plot. plot_mean_time ( drought_cube . LST . sel ( time = slice ( 'June-2010' , 'June-2010' )) ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xarray/core/nanops.py:140: RuntimeWarning: Mean of empty slice return np.nanmean(a, axis=axis, dtype=dtype)","title":"1. Load Data"},{"location":"notebooks/drought/1_drought_features/#2-subset-california","text":"# get california polygon cali_geoms = get_cali_geometry () # get california cube subset cali_cube = mask_datacube ( drought_cube , cali_geoms ) plot_mean_time ( cali_cube . LST . sel ( time = slice ( 'June-2011' , 'June-2011' )) )","title":"2. Subset California"},{"location":"notebooks/drought/1_drought_features/#3-interpolate-nans-time-dimension","text":"# interpolation arguments interp_dim = 'time' method = 'linear' # do interpolation cali_cube_interp = cali_cube . interpolate_na ( dim = interp_dim , method = method )","title":"3. Interpolate NANs - Time Dimension"},{"location":"notebooks/drought/1_drought_features/#4-smoothing-the-signal-vod","text":"In this section, we will try to smooth the signal with two methods: Simple - Rolling mean Using a savgol filter. Some initial parameters: Window Size = 5 Polynomial Order = 3 We will apply this filter in the time domain only. vod_data = cali_cube_interp . VOD vod_data <xarray.DataArray 'VOD' (lat: 189, lon: 103, time: 146)> array([[[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], ..., [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]], [[nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], ..., [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan], [nan, nan, nan, ..., nan, nan, nan]]]) Coordinates: * lat (lat) float64 32.53 32.58 32.63 32.68 ... 41.79 41.84 41.89 41.94 * lon (lon) float64 -124.4 -124.3 -124.2 -124.2 ... -118.7 -118.7 -118.6 * time (time) datetime64[ns] 2010-06-01 2010-06-15 ... 2015-12-22","title":"4. Smoothing the Signal (VOD)"},{"location":"notebooks/drought/1_drought_features/#41-savgol-filter","text":"from scipy.signal import savgol_filter # select example vod_data_ex = select_pixel ( vod_data , pixel ) # savgol filter params window_length = 5 polyorder = 3 # apply savgol filter vod_smooth_filter = savgol_filter ( vod_data_ex , window_length = window_length , polyorder = polyorder ) fig , ax = plt . subplots ( nrows = 2 , figsize = ( 10 , 10 )) ax [ 0 ] . plot ( vod_data_ex ) ax [ 0 ] . set_title ( 'Original Data' ) ax [ 1 ] . plot ( vod_smooth_filter ) ax [ 1 ] . set_title ( 'After Savgol Filter' ) plt . show ()","title":"4.1 - Savgol Filter"},{"location":"notebooks/drought/1_drought_features/#42-rolling-window","text":"# select example vod_data_ex = select_pixel ( vod_data , pixel ) # savgol filter params window_length = 2 # apply savgol filter vod_smooth_roll = vod_data_ex . rolling ( time = window_length , center = True ) . mean () fig , ax = plt . subplots ( nrows = 2 , figsize = ( 10 , 10 )) ax [ 0 ] . plot ( vod_data_ex ) ax [ 0 ] . set_title ( 'Original Data' ) ax [ 1 ] . plot ( vod_smooth_roll ) ax [ 1 ] . set_title ( 'After Rolling Mean' ) plt . show ()","title":"4.2 - Rolling Window"},{"location":"notebooks/drought/1_drought_features/#43-difference","text":"vod_smooth_diff = vod_smooth_filter - vod_smooth_roll fig , ax = plt . subplots ( nrows = 4 , figsize = ( 10 , 10 )) ax [ 0 ] . plot ( vod_data_ex ) ax [ 0 ] . set_title ( 'Original' ) ax [ 1 ] . plot ( vod_smooth_filter ) ax [ 1 ] . set_title ( 'Savgol Filter' ) ax [ 2 ] . plot ( vod_smooth_roll ) ax [ 2 ] . set_title ( 'Rolling Mean' ) ax [ 3 ] . plot ( vod_smooth_diff ) ax [ 3 ] . set_title ( 'Difference' ) # Scale the Difference Y-Limits ymax = np . max ([ vod_smooth_filter . max (), vod_smooth_roll . max ()]) ymin = np . min ([ vod_smooth_filter . min (), vod_smooth_roll . min ()]) center = ( ymax - ymin ) ymax = ymax - center ymin = center - ymin ax [ 3 ] . set_ylim ([ 0 - ymin , 0 + ymax ]) plt . tight_layout () plt . show ()","title":"4.3 - Difference"},{"location":"notebooks/drought/1_drought_features/#43-apply-rolling-mean-to-the-whole-dataset","text":"cali_cube_interp = smooth_vod_signal ( cali_cube_interp , window_length = 2 , center = True ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xarray/core/nanops.py:140: RuntimeWarning: Mean of empty slice return np.nanmean(a, axis=axis, dtype=dtype)","title":"4.3 - Apply Rolling Mean to the whole dataset"},{"location":"notebooks/drought/1_drought_features/#5-remove-climatology","text":"When I mean 'climatology', I mean the difference between observations and typical weather for a particular season. The anomalies should not show up in the seasonal cycle. I'll just do a very simple removal. I'll calculate the monthly mean wrt time and then remove that from each month from the original datacube. Steps Climatalogy - Monthly Mean for the 6 years Remove Climatology - Climatology from each month # remove climatology cali_anomalies , cali_mean = remove_climatology ( cali_cube_interp ) /home/emmanuel/.conda/envs/2019_rbig_ad/lib/python3.6/site-packages/xarray/core/nanops.py:140: RuntimeWarning: Mean of empty slice return np.nanmean(a, axis=axis, dtype=dtype) Simple check where we look at the original and the new. variables = [ 'LST' , 'VOD' , 'NDVI' , 'SM' ] for ivariable in variables : fig , ax = plt . subplots ( nrows = 3 , figsize = ( 10 , 10 )) # Before Climatology select_pixel ( cali_cube_interp [ ivariable ], pixel ) . plot ( ax = ax [ 0 ]) ax [ 0 ] . set_title ( 'Original Time Series' ) # Climatology select_pixel ( cali_mean [ ivariable ], pixel ) . plot ( ax = ax [ 1 ]) ax [ 1 ] . set_title ( 'Climatology' ) # After Climatology select_pixel ( cali_anomalies [ ivariable ], pixel ) . plot ( ax = ax [ 2 ]) ax [ 2 ] . set_title ( 'After Climatology Median Removed' ) plt . tight_layout () plt . show ()","title":"5. Remove Climatology"},{"location":"notebooks/drought/1_drought_features/#6-emdata","text":"I extract the dates for the drought events for california. This will allow me to separate the drought years and non-drought years. cali_droughts = get_cali_emdata () cali_droughts .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } ID ISO LOCATION Date_Start Date_End UTC_Start UTC_End geometry 105 106 USA California 01-Jun-2012 31-Dec-2012 735021 735234 (POLYGON ((-117.2328491210938 32.7764053344726... 132 133 USA California 01-Jan-2014 31-Dec-2014 735600 735964 (POLYGON ((-117.2328491210938 32.7764053344726... 153 154 USA California 01-Jan-2015 31-Dec-2015 735965 736329 (POLYGON ((-117.2328491210938 32.7764053344726... So the drought years are: Drought Years 2012 2014 2015 Non-Drought Years 2010 2011 2013 Note : Even though the EM-Data says that the drought year for 2012 is only half a year, we're going to say that that is a full year. # Drought Years cali_anomalies_drought = get_drought_years ( cali_anomalies , [ '2012' , '2014' , '2015' ] ) # Non-Drought Years cali_anomalies_nondrought = get_drought_years ( cali_anomalies , [ '2010' , '2011' , '2013' ] )","title":"6. EMData"},{"location":"notebooks/drought/1_drought_features/#7-extract-density-cubes","text":"In this step, we will construct 'density cubes'. These are cubes where we add features from a combination of the spatial and/or temporal dimensions. Instead of a single sample, we have a sample that takes into account spatial and/or temporal information. In this experiment, we will only look at temporal information. Our temporal resolution is 14 Days and we want to look at a maximum of 6 months. So: \\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps \\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps <span><span class=\"MathJax_Preview\">\\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps</span><script type=\"math/tex\">\\Bigg\\lfloor \\frac{6 \\: months}{\\frac{14\\: days}{30 \\: days} \\:\\times 1 \\: month} \\Bigg\\rfloor = 12 \\: time \\: stamps # confirm sub_ = cali_anomalies_drought . isel ( time = slice ( 0 , 12 )) sub_ . time [ 0 ] . data , sub_ . time [ - 1 ] . data (array('2012-01-10T00:00:00.000000000', dtype='datetime64[ns]'), array('2012-06-12T00:00:00.000000000', dtype='datetime64[ns]')) We see that the start date for the year 2012 is 01-10 and the end date is 06-12. It's good enough. So we get roughly 6 months of temporal information in our density cubes.","title":"7. Extract Density Cubes"},{"location":"notebooks/drought/1_drought_features/#71-example-density-cube","text":"# window sizes spatial = 1 time = 12 vod_df , lst_df , ndvi_df , sm_df = get_density_cubes ( cali_anomalies_drought , spatial , time ) vod_df . shape , lst_df . shape , ndvi_df . shape , sm_df . shape ((76690, 12), (85995, 12), (82729, 12), (76690, 12))","title":"7.1 - Example Density Cube"},{"location":"notebooks/drought/1_drought_features/#8-find-common-elements","text":"Notice how the number of elements is different depending upon the dataset. I believe it is the case that there are less elements for the VOD and the SM datasets. To make a fair comparison, I'll be using only the common elements between the two density cubes. Note : This is also a bit difficult for RBIG to calculate the Mutual Information for datasets that are potentially so different in their domain. vod_df , lst_df = get_common_elements ( vod_df , lst_df ) vod_df . shape , lst_df . shape ((76690, 12), (76690, 12))","title":"8. Find Common Elements"},{"location":"notebooks/drought/2_experiment_example/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Experiment Example \u00b6 import sys , os cwd = os . getcwd () sys . path . insert ( 0 , f ' { cwd } /../../' ) sys . path . insert ( 0 , '/home/emmanuel/code/py_esdc' ) import xarray as xr import pandas as pd import numpy as np # drought tools from src.data.drought.loader import DataLoader from src.features.drought.build_features import ( get_cali_geometry , mask_datacube , smooth_vod_signal , remove_climatology , get_cali_emdata , get_drought_years , get_density_cubes , get_common_elements_many , normalize ) from src.visualization.drought.analysis import plot_mean_time # esdc tools from esdc.subset import select_pixel from esdc.shape import ShapeFileExtract , rasterize from esdc.transform import DensityCubes # RBIG from src.models.train_models import run_rbig_models from sklearn.preprocessing import StandardScaler from scipy import stats from tqdm import tqdm import matplotlib.pyplot as plt import cartopy import cartopy.crs as ccrs plt . style . use ([ 'fivethirtyeight' , 'seaborn-poster' ]) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload # Load Data region = 'conus' sampling = '14D' drought_cube = DataLoader () . load_data ( region , sampling ) # Subset california cali_geoms = get_cali_geometry () drought_cube = mask_datacube ( drought_cube , cali_geoms ) # interpolate # interpolation arguments interp_dim = 'time' method = 'linear' # do interpolation drought_cube = drought_cube . interpolate_na ( dim = interp_dim , method = method ) # remove climatology drought_cube , _ = remove_climatology ( drought_cube ) # drought years drought_years = { \"2010\" : False , \"2011\" : False , \"2012\" : True , \"2013\" : False , \"2014\" : True , \"2015\" : True , } # MI elements common_vars = [ ( 'VOD' , 'NDVI' ), ( 'VOD' , 'LST' ), ( 'VOD' , 'SM' ), ( 'NDVI' , 'LST' ), ( 'NDVI' , 'SM' ), ( 'LST' , 'SM' ) ] variables = [ 'VOD' , 'NDVI' , 'SM' , 'LST' ] Experiment I - Individual Measurements \u00b6 In this part, we will look at the standard individual measurements such as Entropy, H Total Correlation, TC time_steps = range ( 1 , 12 ) spatial = 1 results_df_single = pd . DataFrame () with tqdm ( drought_cube . groupby ( 'time.year' )) as years_bar : # group datacube by years for iyear , icube in years_bar : # Loop through time steps for itime_step in time_steps : # extract density cubes vod_df , lst_df , ndvi_df , sm_df = get_density_cubes ( icube , spatial , itime_step ) # get common elements dfs = get_common_elements_many ([ vod_df , lst_df , ndvi_df , sm_df ]) vod_df , lst_df , ndvi_df , sm_df = dfs [ 0 ], dfs [ 1 ], dfs [ 2 ], dfs [ 3 ] variables = { 'VOD' : vod_df , 'NDVI' : ndvi_df , 'SM' : sm_df , 'LST' : lst_df } # do calculations for H, TC for iname , idata in variables . items (): # normalize data X_norm = StandardScaler () . fit_transform ( idata ) # entropy, total correlation tc , h , t_ = run_rbig_models ( X_norm , measure = \"t\" , random_state = 123 ) # get H and TC results_df_single = results_df_single . append ({ 'year' : iyear , 'drought' : drought_years [ iyear ], 'samples' : X_norm . shape [ 0 ], 'dimensions' : X_norm . shape [ 1 ], 'temporal' : itime_step , 'variable' : iname , 'tc' : tc , 'h' : h , 'time' : t_ , }, ignore_index = True ) postfix = dict ( Dims = f \" { itime_step } \" , Variable = f \" { iname } \" , ) years_bar . set_postfix ( postfix ) # do calculations for break break 0%| | 0/6 [00:03<?, ?it/s, Dims=1, Variable=LST] Experiment II - Comparing Measurements \u00b6 In this experiment, we will look at different combinations of variables. The following measurements will be calculated and compared: Pearson Correlation Spearman Correlation Mutual Information HSIC... time_steps = range ( 1 , 12 ) spatial = 1 results_df_single = pd . DataFrame () with tqdm ( drought_cube . groupby ( 'time.year' )) as years_bar : # group datacube by years for iyear , icube in years_bar : # Loop through time steps for itime_step in time_steps : # extract density cubes vod_df , lst_df , ndvi_df , sm_df = get_density_cubes ( icube , spatial , itime_step ) # get common elements dfs = get_common_elements_many ([ vod_df , lst_df , ndvi_df , sm_df ]) vod_df , lst_df , ndvi_df , sm_df = dfs [ 0 ], dfs [ 1 ], dfs [ 2 ], dfs [ 3 ] variables = { 'VOD' : vod_df , 'NDVI' : ndvi_df , 'SM' : sm_df , 'LST' : lst_df } # do calculations for H, TC for ( ivar1 , ivar2 ) in common_vars : # for iname, idata in variables.items(): # Pearson coeffcient pears = stats . pearsonr ( variables [ ivar1 ] . values . ravel (), variables [ ivar2 ] . values . ravel () )[ 0 ] # Spearman Coefficient spears = stats . spearmanr ( variables [ ivar1 ] . values . ravel (), variables [ ivar2 ] . values . ravel () )[ 0 ] # normalize data X_norm = StandardScaler () . fit_transform ( variables [ ivar1 ]) Y_norm = StandardScaler () . fit_transform ( variables [ ivar2 ]) # entropy, total correlation mi , t_ = run_rbig_models ( X_norm , Y_norm , measure = \"mi\" , random_state = 123 ) # get H and TC results_df_single = results_df_single . append ({ 'year' : iyear , 'drought' : drought_years [ str ( iyear )], 'samples' : X_norm . shape [ 0 ], 'dimensions' : X_norm . shape [ 1 ], 'temporal' : itime_step , 'variable1' : ivar1 , 'variable2' : ivar2 , 'pearson' : pears , 'mi' : mi , 'time' : t_ , }, ignore_index = True ) postfix = dict ( Year = f \" { iyear } \" , Dims = f \" { itime_step } \" , Variables = f \" { ivar1 } - { ivar2 } \" , MI = f \" { mi : .3f } \" , Pear = f \" { pears : .3f } \" , Spear = f \" { spears : .3f } \" , ) years_bar . set_postfix ( postfix ) # do calculations for break break 0%| | 0/6 [00:18<?, ?it/s, Year=2010, Dims=1, Variables=LST-SM, MI=0.183, Pear=-0.232, Spear=-0.214] results_df_single . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dimensions mi samples temporal time variable1 variable2 0 1.0 0.014735 25779.0 1.0 2.484674 VOD NDVI 1 1.0 0.024350 25779.0 1.0 2.564911 VOD LST 2 1.0 0.157174 25779.0 1.0 2.743430 VOD SM 3 1.0 0.019120 25779.0 1.0 2.564871 NDVI LST 4 1.0 0.059311 25779.0 1.0 2.583604 NDVI SM stats . spearmanr ( variables [ ivar1 ] . values . ravel (), variables [ ivar2 ] . values . ravel ()) SpearmanrResult(correlation=0.08307110054436087, pvalue=1.0326962564352802e-40)","title":"2 experiment example"},{"location":"notebooks/drought/2_experiment_example/#experiment-example","text":"import sys , os cwd = os . getcwd () sys . path . insert ( 0 , f ' { cwd } /../../' ) sys . path . insert ( 0 , '/home/emmanuel/code/py_esdc' ) import xarray as xr import pandas as pd import numpy as np # drought tools from src.data.drought.loader import DataLoader from src.features.drought.build_features import ( get_cali_geometry , mask_datacube , smooth_vod_signal , remove_climatology , get_cali_emdata , get_drought_years , get_density_cubes , get_common_elements_many , normalize ) from src.visualization.drought.analysis import plot_mean_time # esdc tools from esdc.subset import select_pixel from esdc.shape import ShapeFileExtract , rasterize from esdc.transform import DensityCubes # RBIG from src.models.train_models import run_rbig_models from sklearn.preprocessing import StandardScaler from scipy import stats from tqdm import tqdm import matplotlib.pyplot as plt import cartopy import cartopy.crs as ccrs plt . style . use ([ 'fivethirtyeight' , 'seaborn-poster' ]) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload # Load Data region = 'conus' sampling = '14D' drought_cube = DataLoader () . load_data ( region , sampling ) # Subset california cali_geoms = get_cali_geometry () drought_cube = mask_datacube ( drought_cube , cali_geoms ) # interpolate # interpolation arguments interp_dim = 'time' method = 'linear' # do interpolation drought_cube = drought_cube . interpolate_na ( dim = interp_dim , method = method ) # remove climatology drought_cube , _ = remove_climatology ( drought_cube ) # drought years drought_years = { \"2010\" : False , \"2011\" : False , \"2012\" : True , \"2013\" : False , \"2014\" : True , \"2015\" : True , } # MI elements common_vars = [ ( 'VOD' , 'NDVI' ), ( 'VOD' , 'LST' ), ( 'VOD' , 'SM' ), ( 'NDVI' , 'LST' ), ( 'NDVI' , 'SM' ), ( 'LST' , 'SM' ) ] variables = [ 'VOD' , 'NDVI' , 'SM' , 'LST' ]","title":"Experiment Example"},{"location":"notebooks/drought/2_experiment_example/#experiment-i-individual-measurements","text":"In this part, we will look at the standard individual measurements such as Entropy, H Total Correlation, TC time_steps = range ( 1 , 12 ) spatial = 1 results_df_single = pd . DataFrame () with tqdm ( drought_cube . groupby ( 'time.year' )) as years_bar : # group datacube by years for iyear , icube in years_bar : # Loop through time steps for itime_step in time_steps : # extract density cubes vod_df , lst_df , ndvi_df , sm_df = get_density_cubes ( icube , spatial , itime_step ) # get common elements dfs = get_common_elements_many ([ vod_df , lst_df , ndvi_df , sm_df ]) vod_df , lst_df , ndvi_df , sm_df = dfs [ 0 ], dfs [ 1 ], dfs [ 2 ], dfs [ 3 ] variables = { 'VOD' : vod_df , 'NDVI' : ndvi_df , 'SM' : sm_df , 'LST' : lst_df } # do calculations for H, TC for iname , idata in variables . items (): # normalize data X_norm = StandardScaler () . fit_transform ( idata ) # entropy, total correlation tc , h , t_ = run_rbig_models ( X_norm , measure = \"t\" , random_state = 123 ) # get H and TC results_df_single = results_df_single . append ({ 'year' : iyear , 'drought' : drought_years [ iyear ], 'samples' : X_norm . shape [ 0 ], 'dimensions' : X_norm . shape [ 1 ], 'temporal' : itime_step , 'variable' : iname , 'tc' : tc , 'h' : h , 'time' : t_ , }, ignore_index = True ) postfix = dict ( Dims = f \" { itime_step } \" , Variable = f \" { iname } \" , ) years_bar . set_postfix ( postfix ) # do calculations for break break 0%| | 0/6 [00:03<?, ?it/s, Dims=1, Variable=LST]","title":"Experiment I - Individual Measurements"},{"location":"notebooks/drought/2_experiment_example/#experiment-ii-comparing-measurements","text":"In this experiment, we will look at different combinations of variables. The following measurements will be calculated and compared: Pearson Correlation Spearman Correlation Mutual Information HSIC... time_steps = range ( 1 , 12 ) spatial = 1 results_df_single = pd . DataFrame () with tqdm ( drought_cube . groupby ( 'time.year' )) as years_bar : # group datacube by years for iyear , icube in years_bar : # Loop through time steps for itime_step in time_steps : # extract density cubes vod_df , lst_df , ndvi_df , sm_df = get_density_cubes ( icube , spatial , itime_step ) # get common elements dfs = get_common_elements_many ([ vod_df , lst_df , ndvi_df , sm_df ]) vod_df , lst_df , ndvi_df , sm_df = dfs [ 0 ], dfs [ 1 ], dfs [ 2 ], dfs [ 3 ] variables = { 'VOD' : vod_df , 'NDVI' : ndvi_df , 'SM' : sm_df , 'LST' : lst_df } # do calculations for H, TC for ( ivar1 , ivar2 ) in common_vars : # for iname, idata in variables.items(): # Pearson coeffcient pears = stats . pearsonr ( variables [ ivar1 ] . values . ravel (), variables [ ivar2 ] . values . ravel () )[ 0 ] # Spearman Coefficient spears = stats . spearmanr ( variables [ ivar1 ] . values . ravel (), variables [ ivar2 ] . values . ravel () )[ 0 ] # normalize data X_norm = StandardScaler () . fit_transform ( variables [ ivar1 ]) Y_norm = StandardScaler () . fit_transform ( variables [ ivar2 ]) # entropy, total correlation mi , t_ = run_rbig_models ( X_norm , Y_norm , measure = \"mi\" , random_state = 123 ) # get H and TC results_df_single = results_df_single . append ({ 'year' : iyear , 'drought' : drought_years [ str ( iyear )], 'samples' : X_norm . shape [ 0 ], 'dimensions' : X_norm . shape [ 1 ], 'temporal' : itime_step , 'variable1' : ivar1 , 'variable2' : ivar2 , 'pearson' : pears , 'mi' : mi , 'time' : t_ , }, ignore_index = True ) postfix = dict ( Year = f \" { iyear } \" , Dims = f \" { itime_step } \" , Variables = f \" { ivar1 } - { ivar2 } \" , MI = f \" { mi : .3f } \" , Pear = f \" { pears : .3f } \" , Spear = f \" { spears : .3f } \" , ) years_bar . set_postfix ( postfix ) # do calculations for break break 0%| | 0/6 [00:18<?, ?it/s, Year=2010, Dims=1, Variables=LST-SM, MI=0.183, Pear=-0.232, Spear=-0.214] results_df_single . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } dimensions mi samples temporal time variable1 variable2 0 1.0 0.014735 25779.0 1.0 2.484674 VOD NDVI 1 1.0 0.024350 25779.0 1.0 2.564911 VOD LST 2 1.0 0.157174 25779.0 1.0 2.743430 VOD SM 3 1.0 0.019120 25779.0 1.0 2.564871 NDVI LST 4 1.0 0.059311 25779.0 1.0 2.583604 NDVI SM stats . spearmanr ( variables [ ivar1 ] . values . ravel (), variables [ ivar2 ] . values . ravel ()) SpearmanrResult(correlation=0.08307110054436087, pvalue=1.0326962564352802e-40)","title":"Experiment II - Comparing Measurements"},{"location":"notebooks/drought/visualization/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Results - Round I \u00b6 * J. Emmanuel Johnson * 11 th Nov, 2019 Recap \u00b6 Recall that we are looking at different IT measures and how they compare when we look at drought years (2012, 2014, 2015) and non-drought years (2010, 2011, 2013). We vary the amount of temporal features that we are adding; i.e. we increase the number of previous time steps available for our samples. We can divide the IT measures we use into two groups: Individual Measures - where we measure each variable independenly. Entropy - expected (average) amount of uncertainty Total Correlation - amount of redundant information within the features Comparative Measures - where we compare multiple variables to one another Mutual Information - amount of shared information between two multivariate datasets. Pearsons Correlation Coefficient - the amount of correlation that can be found between two datasets. The hypothesis would be: Individual Measures - we see some trend that perhaps gives us intuition that there could be a 'sweet' spot for the amount of temporal dimensions to use. Comparative Measures - the IT measures would exhibit a similar trend we saw for the individual measures but there should depend on the two variables we are comparing. For example the MI between SM and VOD should be higher than between SM and VOD. Comparative Measures - the pearson correlation measures won't be so helpful in this case because it is a linear method that shouldn't do a good job at capturing the nonlinear variability/interactions that we expect. Concerns \u00b6 Calculating the Pearson coefficient The pearson coeff isn't a multivariate measure (I don't think). So to do it for multi-dimensional data, I simply 'unraveled' the array such that I compared sample-to-sample and feature-to-feature. But I'm not sure if this is correct. Climatology I removed the climatology but I would like to see what happens when I don't remove the climatology. I don't think the results are much different between the non-drought and drought years. So I'm wondering if this has more of an affect. Hypothesis Should there be a difference between the drought and non-drought years? And have we sufficiently captured these differences with just 3 years each? (drought and non-drought)? Code \u00b6 import sys sys . path . insert ( 0 , '/home/emmanuel/projects/2019_rbig_ad/src' ) sys . path . append ( '/home/emmanuel/code/py_esdc' ) sys . path . append ( '/home/emmanuel/code/rbig' ) # DataCube PreProcessing from scipy.io import savemat , loadmat import geopandas as geopd from rasterio import features # Main Libraries import numpy as np import scipy.io as scio import xarray as xr import pandas as pd import seaborn as sns from datetime import date import time # IT Algorithms from rbig import RBIG , RBIGMI # ML Preprocessing from sklearn.preprocessing import normalize from sklearn.model_selection import train_test_split from scipy import signal # Plotting import cartopy.crs as ccrs import cartopy.feature as cfeature import matplotlib.pyplot as plt % matplotlib inline plt . style . use ([ 'seaborn-poster' ]) import tqdm # Utilities import warnings warnings . simplefilter ( 'ignore' , category = FutureWarning ) # Notebook Specifics % load_ext autoreload % autoreload 2 plt . style . available ['seaborn-dark-palette', 'classic', 'ggplot', 'seaborn-dark', 'seaborn-pastel', 'seaborn-bright', 'seaborn-deep', 'tableau-colorblind10', 'seaborn-talk', 'fast', 'seaborn-ticks', 'seaborn-white', 'bmh', 'fivethirtyeight', 'seaborn-muted', '_classic_test', 'grayscale', 'seaborn-darkgrid', 'seaborn-poster', 'seaborn', 'seaborn-whitegrid', 'dark_background', 'seaborn-paper', 'seaborn-colorblind', 'seaborn-notebook', 'Solarize_Light2'] FIG_PATH = '/home/emmanuel/projects/2020_rbig_rs/reports/figures/drought/individual/' DATA_PATH = '/home/emmanuel/projects/2020_rbig_rs/data/drought/results/' datasets = [ 'exp_ind_v2.csv' , 'exp_group_v2.csv' ] Experiment I - Individual Variables \u00b6 data = pd . read_csv ( DATA_PATH + datasets [ 0 ], index_col = [ 0 ]) data [ 'drought' ] = np . where ( data [ 'drought' ] == 1 , True , False ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } drought h samples tc temporal time variable year 0 False 1.405693 25779.0 0.000000 1.0 0.551456 VOD 2010.0 1 False 1.311283 25779.0 0.000000 1.0 0.557524 NDVI 2010.0 2 False 1.141273 25779.0 0.000000 1.0 0.547523 SM 2010.0 3 False 1.364679 25779.0 0.000000 1.0 0.547514 LST 2010.0 4 False 2.680166 24108.0 0.128393 2.0 1.434613 VOD 2010.0 Normalize \u00b6 # normalize data [ 'h_norm' ] = data [ 'h' ] . div ( data . temporal ) data [ 'tc_norm' ] = data [ 'tc' ] . div ( data . temporal ) Entropy \u00b6 def plot_entropy ( data , normalized = False , save = True , drought = True ): fig , ax = plt . subplots ( figsize = ( 10 , 7 )) if drought == 'on' : drought = 'drought' data = data [ data [ 'year' ] . isin ([ 2012 , 2014 , 2015 ])] style = None elif drought == 'off' : drought = 'nondrought' style = None data = data [ data [ 'year' ] . isin ([ 2010 , 2011 , 2013 ])] elif drought == 'both' : drought = 'both' style = 'drought' else : raise ValueError ( 'Unrecognized drought state: ' , drought ) if normalized : y = 'h_norm' else : y = 'h' sns . lineplot ( x = \"temporal\" , y = y , hue = 'variable' , data = data , style = style , marker = 'o' , ) ax . set_xlabel ( 'Temporal Dims' ) ax . set_ylabel ( 'Entropy' ) # plt.legend(['NDVI', 'LST', 'SM', 'VOD']) plt . tight_layout () plt . show () if normalized and save : fig . savefig ( f \" { FIG_PATH } H_norm_individual_ { drought } .png\" , frameon = False , ) elif save : fig . savefig ( f \" { FIG_PATH } H_individual_ { drought } .png\" , frameon = False , ) Drought Years vs Non-Drought Years \u00b6 # plot_entropy(data, normalized=True, save=False, drought='on') # plot_entropy(data, normalized=True, save=False, drought='off') plot_entropy ( data , normalized = True , save = False , drought = 'both' ) Total Correlation \u00b6 def plot_tc ( data , normalized = False , save = True , drought = True ): fig , ax = plt . subplots ( figsize = ( 10 , 7 )) if drought == 'on' : drought = 'drought' data = data [ data [ 'year' ] . isin ([ 2012 , 2014 , 2015 ])] style = None elif drought == 'off' : drought = 'nondrought' style = None data = data [ data [ 'year' ] . isin ([ 2010 , 2011 , 2013 ])] elif drought == 'both' : drought = 'both' style = 'drought' else : raise ValueError ( 'Unrecognized drought state: ' , drought ) if normalized : y = 'tc_norm' else : y = 'tc' sns . lineplot ( x = \"temporal\" , y = y , hue = 'variable' , data = data , style = style , marker = 'o' , ) ax . set_xlabel ( 'Temporal Dims' ) ax . set_ylabel ( 'Total Correlation' ) # plt.legend(['NDVI', 'LST', 'SM', 'VOD']) plt . tight_layout () plt . show () if normalized and save : fig . savefig ( f \" { FIG_PATH } TC_norm_individual_ { drought } .png\" , frameon = False , ) elif save : fig . savefig ( f \" { FIG_PATH } TC_individual_ { drought } .png\" , frameon = False , ) Drought Years vs Non-Drought Years \u00b6 # plot_tc(data, normalized=True, save=False, drought='on') # plot_tc(data, normalized=True, save=False, drought='off') plot_tc ( data , normalized = True , save = False , drought = 'both' ) Experiment II - Comparing Variables \u00b6 data_group = pd . read_csv ( DATA_PATH + datasets [ 1 ], index_col = [ 0 ]) data_group [ 'drought' ] = np . where ( data_group [ 'drought' ] == 1 , True , False ) data_group . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } drought mi pearson samples spearman temporal time variable1 variable2 year 0 False 0.014735 0.064995 25779.0 0.083071 1.0 2.844188 VOD NDVI 2010.0 1 False 0.024350 0.008711 25779.0 0.024544 1.0 2.887552 VOD LST 2010.0 2 False 0.142746 0.150556 25779.0 0.290016 1.0 3.263144 VOD SM 2010.0 3 False 0.019120 -0.107464 25779.0 -0.118719 1.0 2.836338 NDVI LST 2010.0 4 False 0.059311 0.211504 25779.0 0.181002 1.0 2.830751 NDVI SM 2010.0 Normalize \u00b6 # normalize data_group [ 'mi_norm' ] = data_group [ 'mi' ] . div ( data_group . temporal ) # cond1 = data_group['variable1'] == 'NDVI' # cond2 = data_group['variable2'] == 'NDVI' # data_group.loc[cond1 & cond2, ['variable1', 'variable2']] = data_group.loc[cond1 & cond2, ['variable2', 'variable1']].values def move_variables ( df : pd . DataFrame , variable : str ) -> pd . DataFrame : # cond1 = df['variable1'] == variable cond = df [ 'variable2' ] == variable df . loc [ cond , [ 'variable2' , 'variable1' ] ] = df . loc [ cond , [ 'variable1' , 'variable2' ] ] . values return df df_new = move_variables ( data_group , 'NDVI' ) df_new . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } drought mi pearson samples spearman temporal time variable1 variable2 year mi_norm 0 False 0.014735 0.064995 25779.0 0.083071 1.0 2.844188 NDVI VOD 2010.0 0.014735 1 False 0.024350 0.008711 25779.0 0.024544 1.0 2.887552 VOD LST 2010.0 0.024350 2 False 0.142746 0.150556 25779.0 0.290016 1.0 3.263144 VOD SM 2010.0 0.142746 3 False 0.019120 -0.107464 25779.0 -0.118719 1.0 2.836338 NDVI LST 2010.0 0.019120 4 False 0.059311 0.211504 25779.0 0.181002 1.0 2.830751 NDVI SM 2010.0 0.059311 Mutual Information \u00b6 def plot_mutual_info ( data , normalized = False , save = True , variable = 'VOD' , drought = True ): fig , ax = plt . subplots ( figsize = ( 10 , 7 )) if drought == 'on' : drought = 'drought' data = data [ data [ 'year' ] . isin ([ 2012 , 2014 , 2015 ])] style = None elif drought == 'off' : drought = 'nondrought' style = None data = data [ data [ 'year' ] . isin ([ 2010 , 2011 , 2013 ])] elif drought == 'both' : drought = 'both' style = 'drought' else : raise ValueError ( 'Unrecognized drought state: ' , drought ) # Select variable data = move_variables ( data , variable ) data = data [ data [ 'variable1' ] == variable ] # print(data.variable2) if normalized : y = 'mi_norm' else : y = 'mi' sns . lineplot ( x = \"temporal\" , y = y , hue = 'variable2' , data = data , style = style , marker = 'o' , ) ax . set_xlabel ( 'Temporal Dims' ) ax . set_ylabel ( 'Mutual Information' ) # plt.legend(['NDVI', 'LST', 'SM', 'VOD']) plt . tight_layout () plt . show () if normalized and save : fig . savefig ( f \" { FIG_PATH } MI_norm_individual_ { drought } .png\" , frameon = False , ) elif save : fig . savefig ( f \" { FIG_PATH } MI_individual_ { drought } .png\" , frameon = False , ) def plot_pearson ( data , normalized = False , save = True , variable = 'VOD' , drought = True ): fig , ax = plt . subplots ( figsize = ( 10 , 7 )) if drought == 'on' : drought = 'drought' data = data [ data [ 'year' ] . isin ([ 2012 , 2014 , 2015 ])] style = None elif drought == 'off' : drought = 'nondrought' style = None data = data [ data [ 'year' ] . isin ([ 2010 , 2011 , 2013 ])] elif drought == 'both' : drought = 'both' style = 'drought' else : raise ValueError ( 'Unrecognized drought state: ' , drought ) # Select variable data = move_variables ( data , variable ) data = data [ data [ 'variable1' ] == variable ] # print(data.variable2) if normalized : y = 'pearson' else : y = 'pearson' sns . lineplot ( x = \"temporal\" , y = y , hue = 'variable2' , style = style , data = data , marker = 'o' , ) ax . set_xlabel ( 'Temporal Dims' ) ax . set_ylabel ( 'Pearson' ) # plt.legend(['NDVI', 'LST', 'SM', 'VOD']) plt . tight_layout () plt . show () if normalized and save : fig . savefig ( f \" { FIG_PATH } Pear_norm_individual_ { drought } .png\" , frameon = False , ) elif save : fig . savefig ( f \" { FIG_PATH } Pear_individual_ { drought } .png\" , frameon = False , ) VOD \u00b6 Drought Years \u00b6 plot_mutual_info ( data_group , normalized = True , save = True , variable = 'VOD' , drought = 'on' ) plot_pearson ( data_group , normalized = False , save = True , variable = 'VOD' , drought = 'on' ) Non-Drought Years \u00b6 plot_mutual_info ( data_group , normalized = True , save = True , variable = 'VOD' , drought = 'off' ) plot_pearson ( data_group , normalized = False , save = True , variable = 'VOD' , drought = 'off' ) Drought and Non-Drought Years \u00b6 plot_mutual_info ( data_group , normalized = True , save = True , variable = 'VOD' , drought = 'both' ) plot_pearson ( data_group , normalized = False , save = True , variable = 'VOD' , drought = 'both' ) NDVI \u00b6 Drought Years \u00b6 plot_mutual_info ( data_group , normalized = True , save = True , variable = 'NDVI' , drought = 'on' ) plot_pearson ( data_group , normalized = True , save = True , variable = 'NDVI' , drought = 'on' ) Non-Drought Years \u00b6 plot_mutual_info ( data_group , normalized = True , save = True , variable = 'NDVI' , drought = 'both' ) plot_pearson ( data_group , normalized = True , save = True , variable = 'NDVI' , drought = 'both' ) LST \u00b6 Drought Years \u00b6 plot_mutual_info ( data_group , normalized = True , save = True , variable = 'LST' , drought = 'on' ) plot_pearson ( data_group , normalized = True , save = True , variable = 'LST' , drought = 'on' ) Non-Drought Years \u00b6 plot_mutual_info ( data_group , normalized = True , save = True , variable = 'LST' , drought = 'off' ) plot_pearson ( data_group , normalized = True , save = True , variable = 'LST' , drought = 'off' ) Drought and Non-Drought Years \u00b6 plot_mutual_info ( data_group , normalized = True , save = True , variable = 'LST' , drought = 'both' ) plot_pearson ( data_group , normalized = False , save = True , variable = 'LST' , drought = 'both' ) SM \u00b6 Drought Years \u00b6 plot_mutual_info ( data_group , normalized = True , save = True , variable = 'SM' , drought = 'on' ) plot_pearson ( data_group , normalized = True , save = True , variable = 'SM' , drought = 'on' ) Non-Drought Years \u00b6 plot_mutual_info ( data_group , normalized = True , save = True , variable = 'SM' , drought = 'off' ) plot_pearson ( data_group , normalized = True , save = True , variable = 'SM' , drought = 'off' ) Drought and Non-Drought Years \u00b6 plot_mutual_info ( data_group , normalized = True , save = True , variable = 'SM' , drought = 'both' ) plot_pearson ( data_group , normalized = True , save = True , variable = 'SM' , drought = 'both' )","title":"Visualization"},{"location":"notebooks/drought/visualization/#results-round-i","text":"* J. Emmanuel Johnson * 11 th Nov, 2019","title":"Results - Round I"},{"location":"notebooks/drought/visualization/#recap","text":"Recall that we are looking at different IT measures and how they compare when we look at drought years (2012, 2014, 2015) and non-drought years (2010, 2011, 2013). We vary the amount of temporal features that we are adding; i.e. we increase the number of previous time steps available for our samples. We can divide the IT measures we use into two groups: Individual Measures - where we measure each variable independenly. Entropy - expected (average) amount of uncertainty Total Correlation - amount of redundant information within the features Comparative Measures - where we compare multiple variables to one another Mutual Information - amount of shared information between two multivariate datasets. Pearsons Correlation Coefficient - the amount of correlation that can be found between two datasets. The hypothesis would be: Individual Measures - we see some trend that perhaps gives us intuition that there could be a 'sweet' spot for the amount of temporal dimensions to use. Comparative Measures - the IT measures would exhibit a similar trend we saw for the individual measures but there should depend on the two variables we are comparing. For example the MI between SM and VOD should be higher than between SM and VOD. Comparative Measures - the pearson correlation measures won't be so helpful in this case because it is a linear method that shouldn't do a good job at capturing the nonlinear variability/interactions that we expect.","title":"Recap"},{"location":"notebooks/drought/visualization/#concerns","text":"Calculating the Pearson coefficient The pearson coeff isn't a multivariate measure (I don't think). So to do it for multi-dimensional data, I simply 'unraveled' the array such that I compared sample-to-sample and feature-to-feature. But I'm not sure if this is correct. Climatology I removed the climatology but I would like to see what happens when I don't remove the climatology. I don't think the results are much different between the non-drought and drought years. So I'm wondering if this has more of an affect. Hypothesis Should there be a difference between the drought and non-drought years? And have we sufficiently captured these differences with just 3 years each? (drought and non-drought)?","title":"Concerns"},{"location":"notebooks/drought/visualization/#code","text":"import sys sys . path . insert ( 0 , '/home/emmanuel/projects/2019_rbig_ad/src' ) sys . path . append ( '/home/emmanuel/code/py_esdc' ) sys . path . append ( '/home/emmanuel/code/rbig' ) # DataCube PreProcessing from scipy.io import savemat , loadmat import geopandas as geopd from rasterio import features # Main Libraries import numpy as np import scipy.io as scio import xarray as xr import pandas as pd import seaborn as sns from datetime import date import time # IT Algorithms from rbig import RBIG , RBIGMI # ML Preprocessing from sklearn.preprocessing import normalize from sklearn.model_selection import train_test_split from scipy import signal # Plotting import cartopy.crs as ccrs import cartopy.feature as cfeature import matplotlib.pyplot as plt % matplotlib inline plt . style . use ([ 'seaborn-poster' ]) import tqdm # Utilities import warnings warnings . simplefilter ( 'ignore' , category = FutureWarning ) # Notebook Specifics % load_ext autoreload % autoreload 2 plt . style . available ['seaborn-dark-palette', 'classic', 'ggplot', 'seaborn-dark', 'seaborn-pastel', 'seaborn-bright', 'seaborn-deep', 'tableau-colorblind10', 'seaborn-talk', 'fast', 'seaborn-ticks', 'seaborn-white', 'bmh', 'fivethirtyeight', 'seaborn-muted', '_classic_test', 'grayscale', 'seaborn-darkgrid', 'seaborn-poster', 'seaborn', 'seaborn-whitegrid', 'dark_background', 'seaborn-paper', 'seaborn-colorblind', 'seaborn-notebook', 'Solarize_Light2'] FIG_PATH = '/home/emmanuel/projects/2020_rbig_rs/reports/figures/drought/individual/' DATA_PATH = '/home/emmanuel/projects/2020_rbig_rs/data/drought/results/' datasets = [ 'exp_ind_v2.csv' , 'exp_group_v2.csv' ]","title":"Code"},{"location":"notebooks/drought/visualization/#experiment-i-individual-variables","text":"data = pd . read_csv ( DATA_PATH + datasets [ 0 ], index_col = [ 0 ]) data [ 'drought' ] = np . where ( data [ 'drought' ] == 1 , True , False ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } drought h samples tc temporal time variable year 0 False 1.405693 25779.0 0.000000 1.0 0.551456 VOD 2010.0 1 False 1.311283 25779.0 0.000000 1.0 0.557524 NDVI 2010.0 2 False 1.141273 25779.0 0.000000 1.0 0.547523 SM 2010.0 3 False 1.364679 25779.0 0.000000 1.0 0.547514 LST 2010.0 4 False 2.680166 24108.0 0.128393 2.0 1.434613 VOD 2010.0","title":"Experiment I - Individual Variables"},{"location":"notebooks/drought/visualization/#normalize","text":"# normalize data [ 'h_norm' ] = data [ 'h' ] . div ( data . temporal ) data [ 'tc_norm' ] = data [ 'tc' ] . div ( data . temporal )","title":"Normalize"},{"location":"notebooks/drought/visualization/#entropy","text":"def plot_entropy ( data , normalized = False , save = True , drought = True ): fig , ax = plt . subplots ( figsize = ( 10 , 7 )) if drought == 'on' : drought = 'drought' data = data [ data [ 'year' ] . isin ([ 2012 , 2014 , 2015 ])] style = None elif drought == 'off' : drought = 'nondrought' style = None data = data [ data [ 'year' ] . isin ([ 2010 , 2011 , 2013 ])] elif drought == 'both' : drought = 'both' style = 'drought' else : raise ValueError ( 'Unrecognized drought state: ' , drought ) if normalized : y = 'h_norm' else : y = 'h' sns . lineplot ( x = \"temporal\" , y = y , hue = 'variable' , data = data , style = style , marker = 'o' , ) ax . set_xlabel ( 'Temporal Dims' ) ax . set_ylabel ( 'Entropy' ) # plt.legend(['NDVI', 'LST', 'SM', 'VOD']) plt . tight_layout () plt . show () if normalized and save : fig . savefig ( f \" { FIG_PATH } H_norm_individual_ { drought } .png\" , frameon = False , ) elif save : fig . savefig ( f \" { FIG_PATH } H_individual_ { drought } .png\" , frameon = False , )","title":"Entropy"},{"location":"notebooks/drought/visualization/#drought-years-vs-non-drought-years","text":"# plot_entropy(data, normalized=True, save=False, drought='on') # plot_entropy(data, normalized=True, save=False, drought='off') plot_entropy ( data , normalized = True , save = False , drought = 'both' )","title":"Drought Years vs Non-Drought Years"},{"location":"notebooks/drought/visualization/#total-correlation","text":"def plot_tc ( data , normalized = False , save = True , drought = True ): fig , ax = plt . subplots ( figsize = ( 10 , 7 )) if drought == 'on' : drought = 'drought' data = data [ data [ 'year' ] . isin ([ 2012 , 2014 , 2015 ])] style = None elif drought == 'off' : drought = 'nondrought' style = None data = data [ data [ 'year' ] . isin ([ 2010 , 2011 , 2013 ])] elif drought == 'both' : drought = 'both' style = 'drought' else : raise ValueError ( 'Unrecognized drought state: ' , drought ) if normalized : y = 'tc_norm' else : y = 'tc' sns . lineplot ( x = \"temporal\" , y = y , hue = 'variable' , data = data , style = style , marker = 'o' , ) ax . set_xlabel ( 'Temporal Dims' ) ax . set_ylabel ( 'Total Correlation' ) # plt.legend(['NDVI', 'LST', 'SM', 'VOD']) plt . tight_layout () plt . show () if normalized and save : fig . savefig ( f \" { FIG_PATH } TC_norm_individual_ { drought } .png\" , frameon = False , ) elif save : fig . savefig ( f \" { FIG_PATH } TC_individual_ { drought } .png\" , frameon = False , )","title":"Total Correlation"},{"location":"notebooks/drought/visualization/#drought-years-vs-non-drought-years_1","text":"# plot_tc(data, normalized=True, save=False, drought='on') # plot_tc(data, normalized=True, save=False, drought='off') plot_tc ( data , normalized = True , save = False , drought = 'both' )","title":"Drought Years vs Non-Drought Years"},{"location":"notebooks/drought/visualization/#experiment-ii-comparing-variables","text":"data_group = pd . read_csv ( DATA_PATH + datasets [ 1 ], index_col = [ 0 ]) data_group [ 'drought' ] = np . where ( data_group [ 'drought' ] == 1 , True , False ) data_group . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } drought mi pearson samples spearman temporal time variable1 variable2 year 0 False 0.014735 0.064995 25779.0 0.083071 1.0 2.844188 VOD NDVI 2010.0 1 False 0.024350 0.008711 25779.0 0.024544 1.0 2.887552 VOD LST 2010.0 2 False 0.142746 0.150556 25779.0 0.290016 1.0 3.263144 VOD SM 2010.0 3 False 0.019120 -0.107464 25779.0 -0.118719 1.0 2.836338 NDVI LST 2010.0 4 False 0.059311 0.211504 25779.0 0.181002 1.0 2.830751 NDVI SM 2010.0","title":"Experiment II - Comparing Variables"},{"location":"notebooks/drought/visualization/#normalize_1","text":"# normalize data_group [ 'mi_norm' ] = data_group [ 'mi' ] . div ( data_group . temporal ) # cond1 = data_group['variable1'] == 'NDVI' # cond2 = data_group['variable2'] == 'NDVI' # data_group.loc[cond1 & cond2, ['variable1', 'variable2']] = data_group.loc[cond1 & cond2, ['variable2', 'variable1']].values def move_variables ( df : pd . DataFrame , variable : str ) -> pd . DataFrame : # cond1 = df['variable1'] == variable cond = df [ 'variable2' ] == variable df . loc [ cond , [ 'variable2' , 'variable1' ] ] = df . loc [ cond , [ 'variable1' , 'variable2' ] ] . values return df df_new = move_variables ( data_group , 'NDVI' ) df_new . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } drought mi pearson samples spearman temporal time variable1 variable2 year mi_norm 0 False 0.014735 0.064995 25779.0 0.083071 1.0 2.844188 NDVI VOD 2010.0 0.014735 1 False 0.024350 0.008711 25779.0 0.024544 1.0 2.887552 VOD LST 2010.0 0.024350 2 False 0.142746 0.150556 25779.0 0.290016 1.0 3.263144 VOD SM 2010.0 0.142746 3 False 0.019120 -0.107464 25779.0 -0.118719 1.0 2.836338 NDVI LST 2010.0 0.019120 4 False 0.059311 0.211504 25779.0 0.181002 1.0 2.830751 NDVI SM 2010.0 0.059311","title":"Normalize"},{"location":"notebooks/drought/visualization/#mutual-information","text":"def plot_mutual_info ( data , normalized = False , save = True , variable = 'VOD' , drought = True ): fig , ax = plt . subplots ( figsize = ( 10 , 7 )) if drought == 'on' : drought = 'drought' data = data [ data [ 'year' ] . isin ([ 2012 , 2014 , 2015 ])] style = None elif drought == 'off' : drought = 'nondrought' style = None data = data [ data [ 'year' ] . isin ([ 2010 , 2011 , 2013 ])] elif drought == 'both' : drought = 'both' style = 'drought' else : raise ValueError ( 'Unrecognized drought state: ' , drought ) # Select variable data = move_variables ( data , variable ) data = data [ data [ 'variable1' ] == variable ] # print(data.variable2) if normalized : y = 'mi_norm' else : y = 'mi' sns . lineplot ( x = \"temporal\" , y = y , hue = 'variable2' , data = data , style = style , marker = 'o' , ) ax . set_xlabel ( 'Temporal Dims' ) ax . set_ylabel ( 'Mutual Information' ) # plt.legend(['NDVI', 'LST', 'SM', 'VOD']) plt . tight_layout () plt . show () if normalized and save : fig . savefig ( f \" { FIG_PATH } MI_norm_individual_ { drought } .png\" , frameon = False , ) elif save : fig . savefig ( f \" { FIG_PATH } MI_individual_ { drought } .png\" , frameon = False , ) def plot_pearson ( data , normalized = False , save = True , variable = 'VOD' , drought = True ): fig , ax = plt . subplots ( figsize = ( 10 , 7 )) if drought == 'on' : drought = 'drought' data = data [ data [ 'year' ] . isin ([ 2012 , 2014 , 2015 ])] style = None elif drought == 'off' : drought = 'nondrought' style = None data = data [ data [ 'year' ] . isin ([ 2010 , 2011 , 2013 ])] elif drought == 'both' : drought = 'both' style = 'drought' else : raise ValueError ( 'Unrecognized drought state: ' , drought ) # Select variable data = move_variables ( data , variable ) data = data [ data [ 'variable1' ] == variable ] # print(data.variable2) if normalized : y = 'pearson' else : y = 'pearson' sns . lineplot ( x = \"temporal\" , y = y , hue = 'variable2' , style = style , data = data , marker = 'o' , ) ax . set_xlabel ( 'Temporal Dims' ) ax . set_ylabel ( 'Pearson' ) # plt.legend(['NDVI', 'LST', 'SM', 'VOD']) plt . tight_layout () plt . show () if normalized and save : fig . savefig ( f \" { FIG_PATH } Pear_norm_individual_ { drought } .png\" , frameon = False , ) elif save : fig . savefig ( f \" { FIG_PATH } Pear_individual_ { drought } .png\" , frameon = False , )","title":"Mutual Information"},{"location":"notebooks/drought/visualization/#vod","text":"","title":"VOD"},{"location":"notebooks/drought/visualization/#drought-years","text":"plot_mutual_info ( data_group , normalized = True , save = True , variable = 'VOD' , drought = 'on' ) plot_pearson ( data_group , normalized = False , save = True , variable = 'VOD' , drought = 'on' )","title":"Drought Years"},{"location":"notebooks/drought/visualization/#non-drought-years","text":"plot_mutual_info ( data_group , normalized = True , save = True , variable = 'VOD' , drought = 'off' ) plot_pearson ( data_group , normalized = False , save = True , variable = 'VOD' , drought = 'off' )","title":"Non-Drought Years"},{"location":"notebooks/drought/visualization/#drought-and-non-drought-years","text":"plot_mutual_info ( data_group , normalized = True , save = True , variable = 'VOD' , drought = 'both' ) plot_pearson ( data_group , normalized = False , save = True , variable = 'VOD' , drought = 'both' )","title":"Drought and Non-Drought Years"},{"location":"notebooks/drought/visualization/#ndvi","text":"","title":"NDVI"},{"location":"notebooks/drought/visualization/#drought-years_1","text":"plot_mutual_info ( data_group , normalized = True , save = True , variable = 'NDVI' , drought = 'on' ) plot_pearson ( data_group , normalized = True , save = True , variable = 'NDVI' , drought = 'on' )","title":"Drought Years"},{"location":"notebooks/drought/visualization/#non-drought-years_1","text":"plot_mutual_info ( data_group , normalized = True , save = True , variable = 'NDVI' , drought = 'both' ) plot_pearson ( data_group , normalized = True , save = True , variable = 'NDVI' , drought = 'both' )","title":"Non-Drought Years"},{"location":"notebooks/drought/visualization/#lst","text":"","title":"LST"},{"location":"notebooks/drought/visualization/#drought-years_2","text":"plot_mutual_info ( data_group , normalized = True , save = True , variable = 'LST' , drought = 'on' ) plot_pearson ( data_group , normalized = True , save = True , variable = 'LST' , drought = 'on' )","title":"Drought Years"},{"location":"notebooks/drought/visualization/#non-drought-years_2","text":"plot_mutual_info ( data_group , normalized = True , save = True , variable = 'LST' , drought = 'off' ) plot_pearson ( data_group , normalized = True , save = True , variable = 'LST' , drought = 'off' )","title":"Non-Drought Years"},{"location":"notebooks/drought/visualization/#drought-and-non-drought-years_1","text":"plot_mutual_info ( data_group , normalized = True , save = True , variable = 'LST' , drought = 'both' ) plot_pearson ( data_group , normalized = False , save = True , variable = 'LST' , drought = 'both' )","title":"Drought and Non-Drought Years"},{"location":"notebooks/drought/visualization/#sm","text":"","title":"SM"},{"location":"notebooks/drought/visualization/#drought-years_3","text":"plot_mutual_info ( data_group , normalized = True , save = True , variable = 'SM' , drought = 'on' ) plot_pearson ( data_group , normalized = True , save = True , variable = 'SM' , drought = 'on' )","title":"Drought Years"},{"location":"notebooks/drought/visualization/#non-drought-years_3","text":"plot_mutual_info ( data_group , normalized = True , save = True , variable = 'SM' , drought = 'off' ) plot_pearson ( data_group , normalized = True , save = True , variable = 'SM' , drought = 'off' )","title":"Non-Drought Years"},{"location":"notebooks/drought/visualization/#drought-and-non-drought-years_2","text":"plot_mutual_info ( data_group , normalized = True , save = True , variable = 'SM' , drought = 'both' ) plot_pearson ( data_group , normalized = True , save = True , variable = 'SM' , drought = 'both' )","title":"Drought and Non-Drought Years"},{"location":"notebooks/explore/kernel_alignment/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Exploring: Variation of Information \u00b6 Background \u00b6 My projects involve trying to compare the outputs of different climate models. There are currently more than 20+ climate models from different companies and each of them try to produce the most accurate prediction of some physical phenomena, e.g. Sea Surface Temperature, Mean Sea Level Pressure, etc. However, it's a difficult task to provide accurate comparison techniques for each of the models. There exist some methods such as the mean and standard deviation. There is also a very common framework of visually summarizing this information in the form of Taylor Diagrams. However, the drawback of using these methods is that they are typically non-linear methods and they cannot handle multidimensional, multivariate data. Another way to measure similarity would be in the family of Information Theory Measures (ITMs). Instead of directly measuring first-order output statistics, these methods summarize the information via a probability distribution function (PDF) of the dataset. These can measure non-linear relationships and are naturally multivariate that offers solutions to the shortcomings of the standard methods. I would like to explore this and see if this is a useful way of summarizing information. This is removing the Code \u00b6 import numpy as np import seaborn as sns import pandas as pd import statsmodels.api as smi import matplotlib.pyplot as plt plt . style . use ( 'seaborn-talk' ) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload SAVE_PATH = \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/explore/vi/\" Data \u00b6 We will use the classic dataset for Anscombe's quartet. This is a staple dataset which shows how we need to take care when comparing two datasets. In the example, we will show how visually, two datasets will look similar, but using a correlation measure like the Pearson's coefficient will fail because it is not able to capture the non-linear relationship between the two distributions. # load dataset df_anscombe = sns . load_dataset ( 'anscombe' ) df_anscombe . dataset . unique () array(['I', 'II', 'III', 'IV'], dtype=object) def get_case ( df : pd . DataFrame , case : str = 'I' ): return df [ df [ 'dataset' ] == case ] def plot_cases ( df : pd . DataFrame , case : str = 'I' , save = True ): df = get_case ( df , case ) plt . figure ( figsize = ( 4 , 4 )) pts = sns . jointplot ( x = \"x\" , y = \"y\" , data = df , kind = \"regplot\" , ) plt . tight_layout () if save is not None : plt . savefig ( SAVE_PATH + f 'demo_case { case } .png' ) return None plot_cases ( df_anscombe , 'III' ) <Figure size 288x288 with 0 Axes> This is a very simple case where we have a linear relationship between the datasets. The regression plot above shows a linear line that is fit between the two distributions. We can also see the marginal distributions (the histograms) for X and Y. As you can see, they are definitely similar. But now, we are going to look at a way to summarize this information. Mathematics \u00b6 There are a few important quantities to consider when we need to represent the statistics and compare two datasets. Variance Covariance Correlation Root Mean Squared Covariance \u00b6 The covariance is a measure to determine how much two variances change. The covariance between X and Y is given by: C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) where N N is the number of elements in both datasets. Notice how this formula assumes that the number of samples for X and Y are equivalent. This measure is unbounded as it can have a value between -\\infty -\\infty and \\infty \\infty . Let's look at an example of how to calculate this below. # covariance formula def cov ( X , Y ): n_samples = X . shape [ 0 ] # get mean X_mu = X . mean () Y_mu = Y . mean () cov_xy = 0 # loop through the data points for ix in range ( n_samples ): cov_xy += ( X . values [ ix ] - X_mu ) * ( Y . values [ ix ] - Y_mu ) return cov_xy / n_samples # extract the data X = get_case ( df_anscombe , 'I' )[ 'x' ] Y = get_case ( df_anscombe , 'I' )[ 'y' ] # get covariance cov_xy = cov ( X , Y ) print ( cov_xy ) 5.000909090909091 X . values [:, None ] . reshape ( - 1 , 1 ) . shape (11, 1) That number is fairly meaningless now. But we can compare the covariance number of this versus the other cases. Refactor \u00b6 We can remove the loop by doing a matrix multiplication. C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) where X,Y \\in \\mathbb{R}^{N\\times 1} X,Y \\in \\mathbb{R}^{N\\times 1} np . dot ( X [:, None ] . T - X . mean (), Y [:, None ] - Y . mean ()) / X . shape [ 0 ] # covariance formula def cov ( X , Y ): n_samples = X . shape [ 0 ] # get mean X_mu = X . mean () Y_mu = Y . mean () # remove mean from data X -= X_mu Y -= Y_mu # Ensure 2d X = np . atleast_2d ( X ) . reshape ( - 1 , 1 ) Y = np . atleast_2d ( Y ) . reshape ( - 1 , 1 ) # calculate the covariance cov_xy = X . T @ Y return ( cov_xy / n_samples ) . item () def test_anscombe ( func , save_name = None ): fig , axs = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 7 , 5 )) for iax , icase in zip ( axs . ravel (), [ 'I' , 'II' , 'III' , 'IV' ]): # data X = get_case ( df_anscombe , icase )[ 'x' ] Y = get_case ( df_anscombe , icase )[ 'y' ] output = func ( X . values , Y . values ) iax . scatter ( X . values , Y . values , label = f \"Case { icase } : $C$= { output : .2f } \" ) iax . legend () # iax.legend(f\"Case: {icase}\") # get covariance # print(f\"Case {icase}: {cov_xy.item()}\") plt . tight_layout () if save_name is not None : plt . savefig ( SAVE_PATH + f \"demo_ { save_name } .png\" ) plt . show () test_anscombe ( cov , 'cov' ) So, we see that the covariance doesn't seem to change very much between datasets. Multi-Dimensional \u00b6 X = np . random . rand ( 20 , 5 ) Y = 0.5 * X # calculate empirical covariance cov = X . T @ Y assert cov . shape == ( X . shape [ 1 ], Y . shape [ 1 ]) cov array([[3.18630117, 2.56365049, 2.57152415, 2.56805358, 2.11704571], [2.56365049, 3.02897306, 2.48391556, 2.41456641, 2.52907783], [2.57152415, 2.48391556, 3.6830925 , 2.86734156, 2.76159258], [2.56805358, 2.41456641, 2.86734156, 3.31376171, 2.55167058], [2.11704571, 2.52907783, 2.76159258, 2.55167058, 3.3544363 ]]) np . linalg . eigvals ( cov ) . sum () 16.566564739213316 np . trace ( cov ) 16.566564739213327 Correlation \u00b6 This is the normalized version of the covariance measured mentioned above. This is done by dividing the covariance by the product of the standard deviation of the two samples X and Y. So the forumaltion is: \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} With this normalization, we now have a measure that is bounded between -1 and 1. This makes it much more interpretable and also invariant to isotropic scaling, \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+} def corr ( X , Y ): # get standard deviation X_std , Y_std = X . std (), Y . std () # calculate the correlation cov_xy = cov ( X , Y ) # calculate the correlation return ( cov_xy / ( X_std * Y_std )) . item () corr_xy = corr ( X , Y ) print ( corr_xy ) 0.7420788540814529 Now that it is bounded between -1 and 1, this value let's us know that this value is equivalent to being close to 1. So fairly similar. test_anscombe ( corr , 'corr' ) So at this point, this is a bit of a red flag. All of the \\rho \\rho values are the same but we can see very clearly that there are some key differences between the distributions. The covariance nor the correlation measure gave us useful information. Root Mean Squared \u00b6 This is a popular measure for measuring the errors between two datasets. More or less, it is a covariance measure that penalizes higher deviations between the datasets. # covariance formula def rmse ( X , Y ): n_samples = X . shape [ 0 ] # get mean X_mu = X . mean () Y_mu = Y . mean () # remove mean from data X -= X_mu Y -= Y_mu # calculate the squared covariance cov_xy = np . average (( X - Y ) ** 2 , axis = 0 ) return np . sqrt (( cov_xy )) rmse_xy = rmse ( X , Y ) print ( rmse_xy ) 1.9373411958379736 Refactor \u00b6 The scikit-learn library has a built-in mean_sqared_error function which you can call and then use the np.sqrt on the output. from sklearn.metrics import mean_squared_error def rmse ( X , Y ): # calculate the squared covariance rmse_xy = mean_squared_error ( X , Y ) return np . sqrt ( rmse_xy ) rmse_xy = rmse ( X , Y ) print ( rmse_xy ) 1.9373411958379736 test_anscombe ( rmse , 'rmse' ) Again, the same story with a different measurement; no change per dataset. Taylor Diagram \u00b6 The Taylor Diagram was a way to summarize the data statistics in a way that was easy to interpret. It used the relationship between the covariance, the correlation and the root mean squared error via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\text{RMSE}^2 = \\sigma_{\\text{obs}}^2 + \\sigma_{\\text{sim}}^2 - 2 \\sigma_r \\sigma_t \\rho \\text{RMSE}^2 = \\sigma_{\\text{obs}}^2 + \\sigma_{\\text{sim}}^2 - 2 \\sigma_r \\sigma_t \\rho The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the standard deviation of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the standard deviation of the simulated data \\rho \\rho - the correlation coefficient RMSE RMSE - the root mean squared difference between the two datasets So let's do a quick example where we calculate these quantities def taylor_coeffs ( X , Y , means = False ): # subtract means if means : X = X - X . mean () Y = Y - Y . mean () # std observations a = X . std () # std simulated b = Y . std () # correlation coefficient corr_ab = corr ( X , Y ) # RMSE rmse_ab = rmse ( X , Y ) # save coefficients data = { 'a' : a , 'b' : b , 'rho' : corr_ab , 'theta' : np . arccos ( corr_ab ), 'rmse' : rmse_ab } return data # Model I X = get_case ( df_anscombe , 'I' )[ 'x' ] Y = get_case ( df_anscombe , 'I' )[ 'y' ] data1 = taylor_coeffs ( X , Y ) # Model II X = get_case ( df_anscombe , 'II' )[ 'x' ] Y = get_case ( df_anscombe , 'II' )[ 'y' ] data2 = taylor_coeffs ( X , Y ) # Model III X = get_case ( df_anscombe , 'III' )[ 'x' ] Y = get_case ( df_anscombe , 'III' )[ 'y' ] data3 = taylor_coeffs ( X , Y ) # # Model IV # X = get_case(df_anscombe, 'IV')['x'] # Y = get_case(df_anscombe, 'IV')['y'] # data4 = taylor_coeffs(X, Y) np . arccos ( 0.9 ) 0.45102681179626236 import matplotlib.pyplot as plt import numpy as np theta = np . linspace ( 0 , np . pi ) r = np . sin ( theta ) fig = plt . figure ( figsize = ( 7 , 5 )) ax = fig . add_subplot ( 111 , polar = True ) m = ax . scatter ( 0 , data1 [ 'a' ], s = 100 , alpha = 0.75 , label = 'Data' , zorder = 0 ) m1 = ax . scatter ( data1 [ 'theta' ], data1 [ 'b' ], s = 100 , alpha = 0.75 , marker = 'x' , label = 'Model I' ) m1 = ax . scatter ( data2 [ 'theta' ], data2 [ 'b' ], s = 100 , alpha = 0.75 , marker = '+' , label = 'Model II' ) m1 = ax . scatter ( data3 [ 'theta' ], data3 [ 'b' ], s = 100 , alpha = 0.75 , marker = '.' , label = 'Model III' ) # m1 = ax.scatter(data4['theta'], data4['b'], s=100, alpha=0.75, marker='o', label='Model II') # ax.plot(0) ax . set_ylim ([ 0 , 5 ]) # ax.set_xticks([0.1, 0.2, 0.3, 0.9]) # ax.set_xticklabels([1.0, 0.9, 0.8, 0.6, 0.3, 0.2, 0.1]) # m1 = ax.scatter(theta1, a, s=50, alpha=0.75) # m1 = ax.scatter(theta1, a, s=50, alpha=0.75) c = ax . plot ( theta , data1 [ 'a' ] * np . ones ( theta . shape ), color = 'black' , linestyle = 'dashed' , alpha = 0.75 ) ax . set_xlabel ( 'Standard Deviation' , labelpad = 20 ) ax . set_ylabel ( 'Standard Deviation' , labelpad = 20 ) plt . legend () ax . set_thetamin ( 0 ) ax . set_thetamax ( 90 ) plt . tight_layout () plt . savefig ( SAVE_PATH + 'demo_taylor.png' ) plt . show () data1 [ 'b' ], data1 [ 'rho' ] (2.031568135925815, 0.7422004694043999) fig = plt . figure ( figsize = ( 25 , 7 )) dia = TaylorDiagram ( data1 [ 'a' ], fig , rect = 122 , label = 'Model' , extend = False ) dia . samplePoints [ 0 ] . set_color ( 'r' ) dia . add_sample ( data1 [ 'b' ], data1 [ 'rho' ], marker = 'x' , ms = 10 , ls = '-' , mfc = 'k' , mec = 'k' , label = 'Model I' ) dia . add_sample ( data2 [ 'b' ], data2 [ 'rho' ], marker = 'x' , label = 'Model I' ) plt . scatter ( theta1 , b1 , s = 100 , alpha = 0.75 , marker = 'x' , label = 'Model II' ) plt . scatter ( theta2 , b2 , s = 100 , alpha = 0.75 , marker = '+' , label = 'Model II' ) plt . scatter ( theta3 , b3 , s = 100 , alpha = 0.75 , marker = '.' , label = 'Model II' ) plt . scatter ( theta4 , b4 , s = 100 , alpha = 0.75 , marker = 'o' , label = 'Model II' ) # contours = dia.add_contours(levels=5, colors='0.5') # plt.clabel(contours, inline=1, fontsize=10, fmt='%.0f') dia . add_grid () fig . legend ( dia . samplePoints , [ p . get_label () for p in dia . samplePoints ], numpoints = 1 , prop = dict ( size = 'small' ), loc = 'upper right' ) plt . show () Mutual Information \u00b6 In this section, I will be doing the same thing as before except this time I will use the equivalent Information Theory Measures. In principle, they should be better at capturing non-linear relationships and I will be able to add different representations using spatial-temporal information. Entropy \u00b6 This is the simplest and it is analogous to the standard deviation \\sigma \\sigma . Entropy is defined by H(X) = - \\int_{X} f(x) \\log f(x) dx H(X) = - \\int_{X} f(x) \\log f(x) dx This is the expected amount of uncertainty present in a given distributin function f(X) f(X) . It captures the amount of surprise within a distribution. So if there are a large number of low probability events, then the expected uncertainty will be higher. Whereas distributions with fairly equally likely events will have low entropy values as there are not many surprise events, e.g. Uniform. kde = smi . nonparametric . KDEUnivariate ( Y ) kde . fit () print ( kde . entropy ) plt . plot ( kde . support , kde . density ) 1.9670005831544313 [<matplotlib.lines.Line2D at 0x7faec83543c8>] import scipy.stats def entropy ( data , method = 'counts' ): if method == 'counts' : _ , pdata = np . unique ( data , return_counts = True ) entropy = scipy . stats . entropy ( pdata ) elif method == 'kde' : kde = smi . nonparametric . KDEUnivariate ( data ) kde . fit () entropy = kde . entropy else : raise ValueError ( 'Unrecognized method.' ) return entropy Hx = entropy ( X , 'counts' ) Hy = entropy ( Y , 'counts' ) print ( Hx , Hy ) 2.3978952727983707 2.3978952727983707 Mutual Information \u00b6 Given two distributions X and Y, we can calculate the mutual information as I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy where p(x,y) p(x,y) is the joint probability and p_x(x), p_y(y) p_x(x), p_y(y) are the marginal probabilities of X X and Y Y respectively. We can also express the mutual information as a function of the Entropy H(X) H(X) I(X,Y)=H(X) + H(Y) - H(X,Y) I(X,Y)=H(X) + H(Y) - H(X,Y) <span><span class=\"MathJax_Preview\">I(X,Y)=H(X) + H(Y) - H(X,Y)</span><script type=\"math/tex\">I(X,Y)=H(X) + H(Y) - H(X,Y) def mutual_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) return Hx + Hy - Hxy Hxy = entropy ( pd . concat (( X , Y ))) mi_xy = mutual_info ( X . values , Y . values ) print ( mi_xy ) 2.203120400100416 test_anscombe ( mutual_info , 'kde' ) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/sandbox/nonparametric/kernels.py:204: RuntimeWarning: divide by zero encountered in double_scalars w = 1. / (h * n) * np.sum(self((xs-x)/h), axis=0) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/sandbox/nonparametric/kernels.py:204: RuntimeWarning: divide by zero encountered in true_divide w = 1. / (h * n) * np.sum(self((xs-x)/h), axis=0) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/sandbox/nonparametric/kernels.py:204: RuntimeWarning: invalid value encountered in multiply w = 1. / (h * n) * np.sum(self((xs-x)/h), axis=0) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/nonparametric/kde.py:232: IntegrationWarning: The occurrence of roundoff error is detected, which prevents the requested tolerance from being achieved. The error may be underestimated. return -integrate.quad(entr, a, b, args=(endog,))[0] def norm_mutual_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) # mutual information mi_xy = Hx + Hy - Hxy return ( mi_xy / ( np . sqrt ( Hx * Hy ))) test_anscombe ( norm_mutual_info , 'nkde' ) def red_mutual_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) # mutual information mi_xy = Hx + Hy - Hxy return ( 2 * mi_xy / ( Hx + Hy )) test_anscombe ( red_mutual_info , 'rkde' ) Variation of Information \u00b6 $$ \\begin{aligned} VI(X,Y) &= H(X) + H(Y) - 2I(X,Y) \\ &= I(X,X) + I(Y,Y) - 2I(X,Y) \\end{aligned}$$ def variation_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) # mutual information mi_xy = Hx + Hy - Hxy # variation of information vi_xy = Hx + Hy - 2 * mi_xy return vi_xy test_anscombe ( variation_info , 'vikde' ) RVI-Based Diagram \u00b6 Analagous to the Taylor Diagram, we can summarize the ITMs in a way that was easy to interpret. It used the relationship between the entropy, the mutual information and the normalized mutual information via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} where The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the entropy of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the entropy of the simulated data \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} - the normalized mutual information RMSE RMSE - the variation of information between the two datasets h_a = entropy ( X , 'counts' ) h_b = entropy ( Y , 'kde' ) print ( 'H(X),H(Y):' , h_a , h_b ) # joint entropy h_ab = entropy ( pd . concat (( X , Y )), 'kde' ) print ( 'H(X,Y):' , h_ab ) # mutual information mi_ab = h_a + h_b - h_ab print ( 'MI(X,Y):' , mi_ab ) # normalized mutual information nmi_ab = mi_ab / np . sqrt ( h_a * h_b ) print ( 'NMI(X,Y):' , nmi_ab ) # scaled mutual info smi_ab = mi_ab * ( h_ab / ( h_a * h_b )) print ( 'SMI(X,Y):' , smi_ab ) # cos rho term c_ab = 2 * smi_ab - 1 print ( 'C_XY:' , c_ab ) # vi vi = h_a + h_b - 2 * np . sqrt ( h_a * h_b ) * nmi_ab print ( 'VI(X,Y):' , vi ) H(X),H(Y): 2.3978952727983707 1.967000583154429 H(X,Y): 2.51573500302577 MI(X,Y): 1.8491608529270298 NMI(X,Y): 0.8514464531077769 SMI(X,Y): 0.986290574938243 C_XY: 0.972581149876486 VI(X,Y): 0.6665741500987403 def vi_coeffs ( X , Y , method = 'counts' ): # entropy observations h_a = entropy ( X , method ) # entropy simulated h_b = entropy ( Y , method ) # joint entropy h_ab = entropy ( pd . concat (( X , Y )), method ) # mutual information mi_ab = h_a + h_b - h_ab # normalized mutual information nmi_ab = mi_ab / np . sqrt ( h_a * h_b ) # scaled mutual information smi_ab = 2 * mi_ab * ( h_ab / ( h_a * h_b )) - 1 # vi vi_ab = h_a + h_b - 2 * np . sqrt (( h_a * h_b )) * nmi_ab # save coefficients data = { 'h_a' : h_a , 'h_b' : h_b , 'nmi' : nmi_ab , 'smi' : smi_ab , 'theta' : np . arccos ( nmi_ab ), 'vi' : vi_ab } return data # Model I X = get_case ( df_anscombe , 'I' )[ 'x' ] Y = get_case ( df_anscombe , 'I' )[ 'y' ] data1 = vi_coeffs ( X , Y , 'kde' ) print ( data1 ) # Model II X = get_case ( df_anscombe , 'II' )[ 'x' ] Y = get_case ( df_anscombe , 'II' )[ 'y' ] data2 = vi_coeffs ( X , Y , 'kde' ) print ( data2 ) # Model III X = get_case ( df_anscombe , 'III' )[ 'x' ] Y = get_case ( df_anscombe , 'III' )[ 'y' ] data3 = vi_coeffs ( X , Y , 'kde' ) print ( data3 ) # # Model IV # X = get_case(df_anscombe, 'IV')['x'] # Y = get_case(df_anscombe, 'IV')['y'] # data4 = vi_coeffs(X, Y) # print(data4) {'h_a': 2.7518548199717574, 'h_b': 2.2080793522856723, 'nmi': 0.9751276249886281, 'smi': 1.0224173292681424, 'theta': 0.2235002023733858, 'vi': 0.15251984951237407} {'h_a': 2.7518548199717574, 'h_b': 2.1269598407974937, 'nmi': 0.9842327930994981, 'smi': 1.0321990011233386, 'theta': 0.1778134760779669, 'vi': 0.11647649545914085} {'h_a': 2.7518548199717574, 'h_b': 1.967000583154429, 'nmi': 0.9469416711631927, 'smi': 1.0478734393463243, 'theta': 0.32721332665482533, 'vi': 0.31261460292535403} import matplotlib.pyplot as plt import numpy as np theta = np . linspace ( 0 , np . pi ) r = np . sin ( theta ) fig = plt . figure ( figsize = ( 7 , 5 )) ax = fig . add_subplot ( 111 , polar = True ) m = ax . scatter ( 0 , data1 [ 'h_a' ], s = 200 , alpha = 0.75 , label = 'Data' , zorder = 0 ) m1 = ax . scatter ( data1 [ 'theta' ], data1 [ 'h_b' ], s = 150 , alpha = 0.75 , marker = 'x' , label = 'Model I' ) m1 = ax . scatter ( data2 [ 'theta' ], data2 [ 'h_b' ], s = 150 , alpha = 0.75 , marker = 'o' , label = 'Model II' ) m1 = ax . scatter ( data3 [ 'theta' ], data3 [ 'h_b' ], s = 150 , alpha = 0.75 , marker = '.' , label = 'Model III' ) # m1 = ax.scatter(theta4, b4, s=100, alpha=0.75, marker='o', label='Model II') # ax.plot(0) ax . set_ylim ([ 0 , 3 ]) # ax.set_xticks([0.1, 0.2, 0.3, 0.9]) # ax.set_xticklabels([1.0, 0.9, 0.8, 0.6, 0.3, 0.2, 0.1]) # m1 = ax.scatter(theta1, a, s=50, alpha=0.75) # m1 = ax.scatter(theta1, a, s=50, alpha=0.75) c = ax . plot ( theta , data1 [ 'h_a' ] * np . ones ( theta . shape ), color = 'black' , linestyle = 'dashed' , alpha = 0.75 ) ax . set_xlabel ( 'Entropy' , labelpad = 20 ) ax . set_ylabel ( 'Entropy' , labelpad = 20 ) plt . legend () ax . set_thetamin ( 0 ) ax . set_thetamax ( 90 ) plt . tight_layout () plt . savefig ( SAVE_PATH + 'demo_vi.png' ) plt . show ()","title":"Kernel alignment"},{"location":"notebooks/explore/kernel_alignment/#exploring-variation-of-information","text":"","title":"Exploring: Variation of Information"},{"location":"notebooks/explore/kernel_alignment/#background","text":"My projects involve trying to compare the outputs of different climate models. There are currently more than 20+ climate models from different companies and each of them try to produce the most accurate prediction of some physical phenomena, e.g. Sea Surface Temperature, Mean Sea Level Pressure, etc. However, it's a difficult task to provide accurate comparison techniques for each of the models. There exist some methods such as the mean and standard deviation. There is also a very common framework of visually summarizing this information in the form of Taylor Diagrams. However, the drawback of using these methods is that they are typically non-linear methods and they cannot handle multidimensional, multivariate data. Another way to measure similarity would be in the family of Information Theory Measures (ITMs). Instead of directly measuring first-order output statistics, these methods summarize the information via a probability distribution function (PDF) of the dataset. These can measure non-linear relationships and are naturally multivariate that offers solutions to the shortcomings of the standard methods. I would like to explore this and see if this is a useful way of summarizing information. This is removing the","title":"Background"},{"location":"notebooks/explore/kernel_alignment/#code","text":"import numpy as np import seaborn as sns import pandas as pd import statsmodels.api as smi import matplotlib.pyplot as plt plt . style . use ( 'seaborn-talk' ) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload SAVE_PATH = \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/explore/vi/\"","title":"Code"},{"location":"notebooks/explore/kernel_alignment/#data","text":"We will use the classic dataset for Anscombe's quartet. This is a staple dataset which shows how we need to take care when comparing two datasets. In the example, we will show how visually, two datasets will look similar, but using a correlation measure like the Pearson's coefficient will fail because it is not able to capture the non-linear relationship between the two distributions. # load dataset df_anscombe = sns . load_dataset ( 'anscombe' ) df_anscombe . dataset . unique () array(['I', 'II', 'III', 'IV'], dtype=object) def get_case ( df : pd . DataFrame , case : str = 'I' ): return df [ df [ 'dataset' ] == case ] def plot_cases ( df : pd . DataFrame , case : str = 'I' , save = True ): df = get_case ( df , case ) plt . figure ( figsize = ( 4 , 4 )) pts = sns . jointplot ( x = \"x\" , y = \"y\" , data = df , kind = \"regplot\" , ) plt . tight_layout () if save is not None : plt . savefig ( SAVE_PATH + f 'demo_case { case } .png' ) return None plot_cases ( df_anscombe , 'III' ) <Figure size 288x288 with 0 Axes> This is a very simple case where we have a linear relationship between the datasets. The regression plot above shows a linear line that is fit between the two distributions. We can also see the marginal distributions (the histograms) for X and Y. As you can see, they are definitely similar. But now, we are going to look at a way to summarize this information.","title":"Data"},{"location":"notebooks/explore/kernel_alignment/#mathematics","text":"There are a few important quantities to consider when we need to represent the statistics and compare two datasets. Variance Covariance Correlation Root Mean Squared","title":"Mathematics"},{"location":"notebooks/explore/kernel_alignment/#covariance","text":"The covariance is a measure to determine how much two variances change. The covariance between X and Y is given by: C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) where N N is the number of elements in both datasets. Notice how this formula assumes that the number of samples for X and Y are equivalent. This measure is unbounded as it can have a value between -\\infty -\\infty and \\infty \\infty . Let's look at an example of how to calculate this below. # covariance formula def cov ( X , Y ): n_samples = X . shape [ 0 ] # get mean X_mu = X . mean () Y_mu = Y . mean () cov_xy = 0 # loop through the data points for ix in range ( n_samples ): cov_xy += ( X . values [ ix ] - X_mu ) * ( Y . values [ ix ] - Y_mu ) return cov_xy / n_samples # extract the data X = get_case ( df_anscombe , 'I' )[ 'x' ] Y = get_case ( df_anscombe , 'I' )[ 'y' ] # get covariance cov_xy = cov ( X , Y ) print ( cov_xy ) 5.000909090909091 X . values [:, None ] . reshape ( - 1 , 1 ) . shape (11, 1) That number is fairly meaningless now. But we can compare the covariance number of this versus the other cases.","title":"Covariance"},{"location":"notebooks/explore/kernel_alignment/#refactor","text":"We can remove the loop by doing a matrix multiplication. C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) where X,Y \\in \\mathbb{R}^{N\\times 1} X,Y \\in \\mathbb{R}^{N\\times 1} np . dot ( X [:, None ] . T - X . mean (), Y [:, None ] - Y . mean ()) / X . shape [ 0 ] # covariance formula def cov ( X , Y ): n_samples = X . shape [ 0 ] # get mean X_mu = X . mean () Y_mu = Y . mean () # remove mean from data X -= X_mu Y -= Y_mu # Ensure 2d X = np . atleast_2d ( X ) . reshape ( - 1 , 1 ) Y = np . atleast_2d ( Y ) . reshape ( - 1 , 1 ) # calculate the covariance cov_xy = X . T @ Y return ( cov_xy / n_samples ) . item () def test_anscombe ( func , save_name = None ): fig , axs = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 7 , 5 )) for iax , icase in zip ( axs . ravel (), [ 'I' , 'II' , 'III' , 'IV' ]): # data X = get_case ( df_anscombe , icase )[ 'x' ] Y = get_case ( df_anscombe , icase )[ 'y' ] output = func ( X . values , Y . values ) iax . scatter ( X . values , Y . values , label = f \"Case { icase } : $C$= { output : .2f } \" ) iax . legend () # iax.legend(f\"Case: {icase}\") # get covariance # print(f\"Case {icase}: {cov_xy.item()}\") plt . tight_layout () if save_name is not None : plt . savefig ( SAVE_PATH + f \"demo_ { save_name } .png\" ) plt . show () test_anscombe ( cov , 'cov' ) So, we see that the covariance doesn't seem to change very much between datasets.","title":"Refactor"},{"location":"notebooks/explore/kernel_alignment/#multi-dimensional","text":"X = np . random . rand ( 20 , 5 ) Y = 0.5 * X # calculate empirical covariance cov = X . T @ Y assert cov . shape == ( X . shape [ 1 ], Y . shape [ 1 ]) cov array([[3.18630117, 2.56365049, 2.57152415, 2.56805358, 2.11704571], [2.56365049, 3.02897306, 2.48391556, 2.41456641, 2.52907783], [2.57152415, 2.48391556, 3.6830925 , 2.86734156, 2.76159258], [2.56805358, 2.41456641, 2.86734156, 3.31376171, 2.55167058], [2.11704571, 2.52907783, 2.76159258, 2.55167058, 3.3544363 ]]) np . linalg . eigvals ( cov ) . sum () 16.566564739213316 np . trace ( cov ) 16.566564739213327","title":"Multi-Dimensional"},{"location":"notebooks/explore/kernel_alignment/#correlation","text":"This is the normalized version of the covariance measured mentioned above. This is done by dividing the covariance by the product of the standard deviation of the two samples X and Y. So the forumaltion is: \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} With this normalization, we now have a measure that is bounded between -1 and 1. This makes it much more interpretable and also invariant to isotropic scaling, \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+} def corr ( X , Y ): # get standard deviation X_std , Y_std = X . std (), Y . std () # calculate the correlation cov_xy = cov ( X , Y ) # calculate the correlation return ( cov_xy / ( X_std * Y_std )) . item () corr_xy = corr ( X , Y ) print ( corr_xy ) 0.7420788540814529 Now that it is bounded between -1 and 1, this value let's us know that this value is equivalent to being close to 1. So fairly similar. test_anscombe ( corr , 'corr' ) So at this point, this is a bit of a red flag. All of the \\rho \\rho values are the same but we can see very clearly that there are some key differences between the distributions. The covariance nor the correlation measure gave us useful information.","title":"Correlation"},{"location":"notebooks/explore/kernel_alignment/#root-mean-squared","text":"This is a popular measure for measuring the errors between two datasets. More or less, it is a covariance measure that penalizes higher deviations between the datasets. # covariance formula def rmse ( X , Y ): n_samples = X . shape [ 0 ] # get mean X_mu = X . mean () Y_mu = Y . mean () # remove mean from data X -= X_mu Y -= Y_mu # calculate the squared covariance cov_xy = np . average (( X - Y ) ** 2 , axis = 0 ) return np . sqrt (( cov_xy )) rmse_xy = rmse ( X , Y ) print ( rmse_xy ) 1.9373411958379736","title":"Root Mean Squared"},{"location":"notebooks/explore/kernel_alignment/#refactor_1","text":"The scikit-learn library has a built-in mean_sqared_error function which you can call and then use the np.sqrt on the output. from sklearn.metrics import mean_squared_error def rmse ( X , Y ): # calculate the squared covariance rmse_xy = mean_squared_error ( X , Y ) return np . sqrt ( rmse_xy ) rmse_xy = rmse ( X , Y ) print ( rmse_xy ) 1.9373411958379736 test_anscombe ( rmse , 'rmse' ) Again, the same story with a different measurement; no change per dataset.","title":"Refactor"},{"location":"notebooks/explore/kernel_alignment/#taylor-diagram","text":"The Taylor Diagram was a way to summarize the data statistics in a way that was easy to interpret. It used the relationship between the covariance, the correlation and the root mean squared error via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\text{RMSE}^2 = \\sigma_{\\text{obs}}^2 + \\sigma_{\\text{sim}}^2 - 2 \\sigma_r \\sigma_t \\rho \\text{RMSE}^2 = \\sigma_{\\text{obs}}^2 + \\sigma_{\\text{sim}}^2 - 2 \\sigma_r \\sigma_t \\rho The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the standard deviation of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the standard deviation of the simulated data \\rho \\rho - the correlation coefficient RMSE RMSE - the root mean squared difference between the two datasets So let's do a quick example where we calculate these quantities def taylor_coeffs ( X , Y , means = False ): # subtract means if means : X = X - X . mean () Y = Y - Y . mean () # std observations a = X . std () # std simulated b = Y . std () # correlation coefficient corr_ab = corr ( X , Y ) # RMSE rmse_ab = rmse ( X , Y ) # save coefficients data = { 'a' : a , 'b' : b , 'rho' : corr_ab , 'theta' : np . arccos ( corr_ab ), 'rmse' : rmse_ab } return data # Model I X = get_case ( df_anscombe , 'I' )[ 'x' ] Y = get_case ( df_anscombe , 'I' )[ 'y' ] data1 = taylor_coeffs ( X , Y ) # Model II X = get_case ( df_anscombe , 'II' )[ 'x' ] Y = get_case ( df_anscombe , 'II' )[ 'y' ] data2 = taylor_coeffs ( X , Y ) # Model III X = get_case ( df_anscombe , 'III' )[ 'x' ] Y = get_case ( df_anscombe , 'III' )[ 'y' ] data3 = taylor_coeffs ( X , Y ) # # Model IV # X = get_case(df_anscombe, 'IV')['x'] # Y = get_case(df_anscombe, 'IV')['y'] # data4 = taylor_coeffs(X, Y) np . arccos ( 0.9 ) 0.45102681179626236 import matplotlib.pyplot as plt import numpy as np theta = np . linspace ( 0 , np . pi ) r = np . sin ( theta ) fig = plt . figure ( figsize = ( 7 , 5 )) ax = fig . add_subplot ( 111 , polar = True ) m = ax . scatter ( 0 , data1 [ 'a' ], s = 100 , alpha = 0.75 , label = 'Data' , zorder = 0 ) m1 = ax . scatter ( data1 [ 'theta' ], data1 [ 'b' ], s = 100 , alpha = 0.75 , marker = 'x' , label = 'Model I' ) m1 = ax . scatter ( data2 [ 'theta' ], data2 [ 'b' ], s = 100 , alpha = 0.75 , marker = '+' , label = 'Model II' ) m1 = ax . scatter ( data3 [ 'theta' ], data3 [ 'b' ], s = 100 , alpha = 0.75 , marker = '.' , label = 'Model III' ) # m1 = ax.scatter(data4['theta'], data4['b'], s=100, alpha=0.75, marker='o', label='Model II') # ax.plot(0) ax . set_ylim ([ 0 , 5 ]) # ax.set_xticks([0.1, 0.2, 0.3, 0.9]) # ax.set_xticklabels([1.0, 0.9, 0.8, 0.6, 0.3, 0.2, 0.1]) # m1 = ax.scatter(theta1, a, s=50, alpha=0.75) # m1 = ax.scatter(theta1, a, s=50, alpha=0.75) c = ax . plot ( theta , data1 [ 'a' ] * np . ones ( theta . shape ), color = 'black' , linestyle = 'dashed' , alpha = 0.75 ) ax . set_xlabel ( 'Standard Deviation' , labelpad = 20 ) ax . set_ylabel ( 'Standard Deviation' , labelpad = 20 ) plt . legend () ax . set_thetamin ( 0 ) ax . set_thetamax ( 90 ) plt . tight_layout () plt . savefig ( SAVE_PATH + 'demo_taylor.png' ) plt . show () data1 [ 'b' ], data1 [ 'rho' ] (2.031568135925815, 0.7422004694043999) fig = plt . figure ( figsize = ( 25 , 7 )) dia = TaylorDiagram ( data1 [ 'a' ], fig , rect = 122 , label = 'Model' , extend = False ) dia . samplePoints [ 0 ] . set_color ( 'r' ) dia . add_sample ( data1 [ 'b' ], data1 [ 'rho' ], marker = 'x' , ms = 10 , ls = '-' , mfc = 'k' , mec = 'k' , label = 'Model I' ) dia . add_sample ( data2 [ 'b' ], data2 [ 'rho' ], marker = 'x' , label = 'Model I' ) plt . scatter ( theta1 , b1 , s = 100 , alpha = 0.75 , marker = 'x' , label = 'Model II' ) plt . scatter ( theta2 , b2 , s = 100 , alpha = 0.75 , marker = '+' , label = 'Model II' ) plt . scatter ( theta3 , b3 , s = 100 , alpha = 0.75 , marker = '.' , label = 'Model II' ) plt . scatter ( theta4 , b4 , s = 100 , alpha = 0.75 , marker = 'o' , label = 'Model II' ) # contours = dia.add_contours(levels=5, colors='0.5') # plt.clabel(contours, inline=1, fontsize=10, fmt='%.0f') dia . add_grid () fig . legend ( dia . samplePoints , [ p . get_label () for p in dia . samplePoints ], numpoints = 1 , prop = dict ( size = 'small' ), loc = 'upper right' ) plt . show ()","title":"Taylor Diagram"},{"location":"notebooks/explore/kernel_alignment/#mutual-information","text":"In this section, I will be doing the same thing as before except this time I will use the equivalent Information Theory Measures. In principle, they should be better at capturing non-linear relationships and I will be able to add different representations using spatial-temporal information.","title":"Mutual Information"},{"location":"notebooks/explore/kernel_alignment/#entropy","text":"This is the simplest and it is analogous to the standard deviation \\sigma \\sigma . Entropy is defined by H(X) = - \\int_{X} f(x) \\log f(x) dx H(X) = - \\int_{X} f(x) \\log f(x) dx This is the expected amount of uncertainty present in a given distributin function f(X) f(X) . It captures the amount of surprise within a distribution. So if there are a large number of low probability events, then the expected uncertainty will be higher. Whereas distributions with fairly equally likely events will have low entropy values as there are not many surprise events, e.g. Uniform. kde = smi . nonparametric . KDEUnivariate ( Y ) kde . fit () print ( kde . entropy ) plt . plot ( kde . support , kde . density ) 1.9670005831544313 [<matplotlib.lines.Line2D at 0x7faec83543c8>] import scipy.stats def entropy ( data , method = 'counts' ): if method == 'counts' : _ , pdata = np . unique ( data , return_counts = True ) entropy = scipy . stats . entropy ( pdata ) elif method == 'kde' : kde = smi . nonparametric . KDEUnivariate ( data ) kde . fit () entropy = kde . entropy else : raise ValueError ( 'Unrecognized method.' ) return entropy Hx = entropy ( X , 'counts' ) Hy = entropy ( Y , 'counts' ) print ( Hx , Hy ) 2.3978952727983707 2.3978952727983707","title":"Entropy"},{"location":"notebooks/explore/kernel_alignment/#mutual-information_1","text":"Given two distributions X and Y, we can calculate the mutual information as I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy where p(x,y) p(x,y) is the joint probability and p_x(x), p_y(y) p_x(x), p_y(y) are the marginal probabilities of X X and Y Y respectively. We can also express the mutual information as a function of the Entropy H(X) H(X) I(X,Y)=H(X) + H(Y) - H(X,Y) I(X,Y)=H(X) + H(Y) - H(X,Y) <span><span class=\"MathJax_Preview\">I(X,Y)=H(X) + H(Y) - H(X,Y)</span><script type=\"math/tex\">I(X,Y)=H(X) + H(Y) - H(X,Y) def mutual_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) return Hx + Hy - Hxy Hxy = entropy ( pd . concat (( X , Y ))) mi_xy = mutual_info ( X . values , Y . values ) print ( mi_xy ) 2.203120400100416 test_anscombe ( mutual_info , 'kde' ) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/sandbox/nonparametric/kernels.py:204: RuntimeWarning: divide by zero encountered in double_scalars w = 1. / (h * n) * np.sum(self((xs-x)/h), axis=0) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/sandbox/nonparametric/kernels.py:204: RuntimeWarning: divide by zero encountered in true_divide w = 1. / (h * n) * np.sum(self((xs-x)/h), axis=0) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/sandbox/nonparametric/kernels.py:204: RuntimeWarning: invalid value encountered in multiply w = 1. / (h * n) * np.sum(self((xs-x)/h), axis=0) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/nonparametric/kde.py:232: IntegrationWarning: The occurrence of roundoff error is detected, which prevents the requested tolerance from being achieved. The error may be underestimated. return -integrate.quad(entr, a, b, args=(endog,))[0] def norm_mutual_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) # mutual information mi_xy = Hx + Hy - Hxy return ( mi_xy / ( np . sqrt ( Hx * Hy ))) test_anscombe ( norm_mutual_info , 'nkde' ) def red_mutual_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) # mutual information mi_xy = Hx + Hy - Hxy return ( 2 * mi_xy / ( Hx + Hy )) test_anscombe ( red_mutual_info , 'rkde' )","title":"Mutual Information"},{"location":"notebooks/explore/kernel_alignment/#variation-of-information","text":"$$ \\begin{aligned} VI(X,Y) &= H(X) + H(Y) - 2I(X,Y) \\ &= I(X,X) + I(Y,Y) - 2I(X,Y) \\end{aligned}$$ def variation_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) # mutual information mi_xy = Hx + Hy - Hxy # variation of information vi_xy = Hx + Hy - 2 * mi_xy return vi_xy test_anscombe ( variation_info , 'vikde' )","title":"Variation of Information"},{"location":"notebooks/explore/kernel_alignment/#rvi-based-diagram","text":"Analagous to the Taylor Diagram, we can summarize the ITMs in a way that was easy to interpret. It used the relationship between the entropy, the mutual information and the normalized mutual information via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} where The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the entropy of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the entropy of the simulated data \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} - the normalized mutual information RMSE RMSE - the variation of information between the two datasets h_a = entropy ( X , 'counts' ) h_b = entropy ( Y , 'kde' ) print ( 'H(X),H(Y):' , h_a , h_b ) # joint entropy h_ab = entropy ( pd . concat (( X , Y )), 'kde' ) print ( 'H(X,Y):' , h_ab ) # mutual information mi_ab = h_a + h_b - h_ab print ( 'MI(X,Y):' , mi_ab ) # normalized mutual information nmi_ab = mi_ab / np . sqrt ( h_a * h_b ) print ( 'NMI(X,Y):' , nmi_ab ) # scaled mutual info smi_ab = mi_ab * ( h_ab / ( h_a * h_b )) print ( 'SMI(X,Y):' , smi_ab ) # cos rho term c_ab = 2 * smi_ab - 1 print ( 'C_XY:' , c_ab ) # vi vi = h_a + h_b - 2 * np . sqrt ( h_a * h_b ) * nmi_ab print ( 'VI(X,Y):' , vi ) H(X),H(Y): 2.3978952727983707 1.967000583154429 H(X,Y): 2.51573500302577 MI(X,Y): 1.8491608529270298 NMI(X,Y): 0.8514464531077769 SMI(X,Y): 0.986290574938243 C_XY: 0.972581149876486 VI(X,Y): 0.6665741500987403 def vi_coeffs ( X , Y , method = 'counts' ): # entropy observations h_a = entropy ( X , method ) # entropy simulated h_b = entropy ( Y , method ) # joint entropy h_ab = entropy ( pd . concat (( X , Y )), method ) # mutual information mi_ab = h_a + h_b - h_ab # normalized mutual information nmi_ab = mi_ab / np . sqrt ( h_a * h_b ) # scaled mutual information smi_ab = 2 * mi_ab * ( h_ab / ( h_a * h_b )) - 1 # vi vi_ab = h_a + h_b - 2 * np . sqrt (( h_a * h_b )) * nmi_ab # save coefficients data = { 'h_a' : h_a , 'h_b' : h_b , 'nmi' : nmi_ab , 'smi' : smi_ab , 'theta' : np . arccos ( nmi_ab ), 'vi' : vi_ab } return data # Model I X = get_case ( df_anscombe , 'I' )[ 'x' ] Y = get_case ( df_anscombe , 'I' )[ 'y' ] data1 = vi_coeffs ( X , Y , 'kde' ) print ( data1 ) # Model II X = get_case ( df_anscombe , 'II' )[ 'x' ] Y = get_case ( df_anscombe , 'II' )[ 'y' ] data2 = vi_coeffs ( X , Y , 'kde' ) print ( data2 ) # Model III X = get_case ( df_anscombe , 'III' )[ 'x' ] Y = get_case ( df_anscombe , 'III' )[ 'y' ] data3 = vi_coeffs ( X , Y , 'kde' ) print ( data3 ) # # Model IV # X = get_case(df_anscombe, 'IV')['x'] # Y = get_case(df_anscombe, 'IV')['y'] # data4 = vi_coeffs(X, Y) # print(data4) {'h_a': 2.7518548199717574, 'h_b': 2.2080793522856723, 'nmi': 0.9751276249886281, 'smi': 1.0224173292681424, 'theta': 0.2235002023733858, 'vi': 0.15251984951237407} {'h_a': 2.7518548199717574, 'h_b': 2.1269598407974937, 'nmi': 0.9842327930994981, 'smi': 1.0321990011233386, 'theta': 0.1778134760779669, 'vi': 0.11647649545914085} {'h_a': 2.7518548199717574, 'h_b': 1.967000583154429, 'nmi': 0.9469416711631927, 'smi': 1.0478734393463243, 'theta': 0.32721332665482533, 'vi': 0.31261460292535403} import matplotlib.pyplot as plt import numpy as np theta = np . linspace ( 0 , np . pi ) r = np . sin ( theta ) fig = plt . figure ( figsize = ( 7 , 5 )) ax = fig . add_subplot ( 111 , polar = True ) m = ax . scatter ( 0 , data1 [ 'h_a' ], s = 200 , alpha = 0.75 , label = 'Data' , zorder = 0 ) m1 = ax . scatter ( data1 [ 'theta' ], data1 [ 'h_b' ], s = 150 , alpha = 0.75 , marker = 'x' , label = 'Model I' ) m1 = ax . scatter ( data2 [ 'theta' ], data2 [ 'h_b' ], s = 150 , alpha = 0.75 , marker = 'o' , label = 'Model II' ) m1 = ax . scatter ( data3 [ 'theta' ], data3 [ 'h_b' ], s = 150 , alpha = 0.75 , marker = '.' , label = 'Model III' ) # m1 = ax.scatter(theta4, b4, s=100, alpha=0.75, marker='o', label='Model II') # ax.plot(0) ax . set_ylim ([ 0 , 3 ]) # ax.set_xticks([0.1, 0.2, 0.3, 0.9]) # ax.set_xticklabels([1.0, 0.9, 0.8, 0.6, 0.3, 0.2, 0.1]) # m1 = ax.scatter(theta1, a, s=50, alpha=0.75) # m1 = ax.scatter(theta1, a, s=50, alpha=0.75) c = ax . plot ( theta , data1 [ 'h_a' ] * np . ones ( theta . shape ), color = 'black' , linestyle = 'dashed' , alpha = 0.75 ) ax . set_xlabel ( 'Entropy' , labelpad = 20 ) ax . set_ylabel ( 'Entropy' , labelpad = 20 ) plt . legend () ax . set_thetamin ( 0 ) ax . set_thetamax ( 90 ) plt . tight_layout () plt . savefig ( SAVE_PATH + 'demo_vi.png' ) plt . show ()","title":"RVI-Based Diagram"},{"location":"notebooks/explore/variation_of_information/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Exploring: Variation of Information \u00b6 Background \u00b6 My projects involve trying to compare the outputs of different climate models. There are currently more than 20+ climate models from different companies and each of them try to produce the most accurate prediction of some physical phenomena, e.g. Sea Surface Temperature, Mean Sea Level Pressure, etc. However, it's a difficult task to provide accurate comparison techniques for each of the models. There exist some methods such as the mean and standard deviation. There is also a very common framework of visually summarizing this information in the form of Taylor Diagrams. However, the drawback of using these methods is that they are typically non-linear methods and they cannot handle multidimensional, multivariate data. Another way to measure similarity would be in the family of Information Theory Measures (ITMs). Instead of directly measuring first-order output statistics, these methods summarize the information via a probability distribution function (PDF) of the dataset. These can measure non-linear relationships and are naturally multivariate that offers solutions to the shortcomings of the standard methods. I would like to explore this and see if this is a useful way of summarizing information. This is removing the Code \u00b6 import numpy as np import seaborn as sns import pandas as pd import statsmodels.api as smi import sys sys . path . insert ( 0 , '/home/emmanuel/code/kernel_model_zoo/' ) import matplotlib.pyplot as plt plt . style . use ( 'seaborn-talk' ) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload from kernellib.dependence import HSIC SAVE_PATH = \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/explore/vi/\" Data \u00b6 We will use the classic dataset for Anscombe's quartet. This is a staple dataset which shows how we need to take care when comparing two datasets. In the example, we will show how visually, two datasets will look similar, but using a correlation measure like the Pearson's coefficient will fail because it is not able to capture the non-linear relationship between the two distributions. # load dataset df_anscombe = sns . load_dataset ( 'anscombe' ) df_anscombe . dataset . unique () array(['I', 'II', 'III', 'IV'], dtype=object) def get_case ( df : pd . DataFrame , case : str = 'I' ): return df [ df [ 'dataset' ] == case ] def plot_cases ( df : pd . DataFrame , case : str = 'I' , save = True , plot_type = 'reg' ): df = get_case ( df , case ) plt . figure ( figsize = ( 4 , 4 )) if plot_type == 'reg' : pts = sns . regplot ( x = \"x\" , y = \"y\" , data = df , ) elif plot_type == 'joint' : pts = sns . jointplot ( x = \"x\" , y = \"y\" , data = df , kind = \"regplot\" , ) elif plot_type == 'density' : pts = sns . jointplot ( x = \"x\" , y = \"y\" , data = df , kind = \"kde\" , ) else : raise ValueError ( '' ) plt . xlabel ( \"\" ) plt . ylabel ( \"\" ) plt . xticks ([]) plt . yticks ([]) # plt.axis('off') plt . tight_layout () if save is not None : plt . savefig ( SAVE_PATH + f 'demo_case { case } _ { plot_type } .png' , dpi = 200 , transparent = True ) return None plot_cases ( df_anscombe , 'III' , plot_type = 'reg' ) This is a very simple case where we have a linear relationship between the datasets. The regression plot above shows a linear line that is fit between the two distributions. We can also see the marginal distributions (the histograms) for X and Y. As you can see, they are definitely similar. But now, we are going to look at a way to summarize this information. Mathematics \u00b6 There are a few important quantities to consider when we need to represent the statistics and compare two datasets. Variance Covariance Correlation Root Mean Squared Covariance \u00b6 The covariance is a measure to determine how much two variances change. The covariance between X and Y is given by: C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) where N N is the number of elements in both datasets. Notice how this formula assumes that the number of samples for X and Y are equivalent. This measure is unbounded as it can have a value between -\\infty -\\infty and \\infty \\infty . Let's look at an example of how to calculate this below. # covariance formula def cov ( X , Y ): n_samples = X . shape [ 0 ] # get mean X_mu = X . mean () Y_mu = Y . mean () cov_xy = 0 # loop through the data points for ix in range ( n_samples ): cov_xy += ( X . values [ ix ] - X_mu ) * ( Y . values [ ix ] - Y_mu ) return cov_xy / n_samples # extract the data X = get_case ( df_anscombe , 'I' )[ 'x' ] Y = get_case ( df_anscombe , 'I' )[ 'y' ] # get covariance cov_xy = cov ( X , Y ) print ( cov_xy ) 5.000909090909091 X . values [:, None ] . reshape ( - 1 , 1 ) . shape (11, 1) That number is fairly meaningless now. But we can compare the covariance number of this versus the other cases. Refactor \u00b6 We can remove the loop by doing a matrix multiplication. C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) where X,Y \\in \\mathbb{R}^{N\\times 1} X,Y \\in \\mathbb{R}^{N\\times 1} np . dot ( X [:, None ] . T - X . mean (), Y [:, None ] - Y . mean ()) / X . shape [ 0 ] # covariance formula def cov ( X , Y ): n_samples = X . shape [ 0 ] # get mean X_mu = X . mean () Y_mu = Y . mean () # remove mean from data X -= X_mu Y -= Y_mu # Ensure 2d X = np . atleast_2d ( X ) . reshape ( - 1 , 1 ) Y = np . atleast_2d ( Y ) . reshape ( - 1 , 1 ) # calculate the covariance cov_xy = X . T @ Y return ( cov_xy / n_samples ) . item () def test_anscombe ( func , save_name = None ): fig , axs = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 7 , 5 )) for iax , icase in zip ( axs . ravel (), [ 'I' , 'II' , 'III' , 'IV' ]): # data X = get_case ( df_anscombe , icase )[ 'x' ] Y = get_case ( df_anscombe , icase )[ 'y' ] output = func ( X . values , Y . values ) iax . scatter ( X . values , Y . values , label = f \"Case { icase } : $C$= { output : .2f } \" ) iax . legend () # iax.legend(f\"Case: {icase}\") # get covariance # print(f\"Case {icase}: {cov_xy.item()}\") plt . tight_layout () if save_name is not None : plt . savefig ( SAVE_PATH + f \"demo_ { save_name } .png\" ) plt . show () test_anscombe ( cov , 'cov' ) Multi-Variate (Multi-Dimensional) \u00b6 np . random . seed ( 123 ) X = np . random . randn ( 20 , 2 ) Y = 0.5 * X # calculate covariance matrix cov = np . cov ( X . squeeze (), Y . squeeze ()) print ( X . shape , Y . shape , cov . shape ) (20, 2) (20, 2) (40, 40) cov . shape (40, 40) def cov_hs_features ( X , Y ): # calculate covariance matrix cov_xy = np . cov ( X , Y ) # summarize information cov_sum = np . linalg . norm ( cov , ord = 'fro' ) return cov_sum # ||X.T @ Y||_F - feature space lhs = np . linalg . norm ( X . T @ Y , ord = 'fro' ) ** 2 print ( lhs ) # ||XX.T @ YY.T||_F - sample space mhs = np . trace ( X @ X . T @ Y @ Y . T ) print ( mhs ) 379.05326003069405 379.053260030694 # ||X.T @ Y||_F - feature space lhs = np . linalg . norm ( np . cov ( X , Y ), ord = 'fro' ) ** 2 print ( lhs ) # ||XX.T @ YY.T||_F - sample space mhs = np . trace ( X @ X . T @ Y @ Y . T ) print ( mhs ) 913.0831824181817 379.053260030694 # RHS raw = np . trace ( X @ Y . T ) / np . sqrt ( np . trace ( X @ X . T ) * np . trace ( Y @ Y . T )) print ( raw ) # MHS 1.0 Formula 1 \u00b6 \\frac{tr(XY^\\top}{\\sqrt{tr(XX^\\top)tr(YY^T)}} \\frac{tr(XY^\\top}{\\sqrt{tr(XX^\\top)tr(YY^T)}} <span><span class=\"MathJax_Preview\">\\frac{tr(XY^\\top}{\\sqrt{tr(XX^\\top)tr(YY^T)}}</span><script type=\"math/tex\">\\frac{tr(XY^\\top}{\\sqrt{tr(XX^\\top)tr(YY^T)}} # raw formula raw = np . trace ( X @ Y . T ) / np . sqrt ( np . trace ( X @ X . T ) * np . trace ( Y @ Y . T )) print ( raw ) # numerator numer1 = np . trace ( X @ Y . T ) ** 2 numer2 = np . linalg . norm ( X @ Y . T ) print ( numer1 , numer2 ) 1.0 229.5889532845504 15.152193018984098 Formula II \u00b6 \\frac{tr(XX\\top YY^\\top)}{\\sqrt{tr(XX^\\top XX^\\top)tr(YY^\\top YY^\\top)}} \\frac{tr(XX\\top YY^\\top)}{\\sqrt{tr(XX^\\top XX^\\top)tr(YY^\\top YY^\\top)}} <span><span class=\"MathJax_Preview\">\\frac{tr(XX\\top YY^\\top)}{\\sqrt{tr(XX^\\top XX^\\top)tr(YY^\\top YY^\\top)}}</span><script type=\"math/tex\">\\frac{tr(XX\\top YY^\\top)}{\\sqrt{tr(XX^\\top XX^\\top)tr(YY^\\top YY^\\top)}} # formula 2 S = X @ X . T T = Y @ Y . T raw = np . trace ( S . T @ T ) / np . sqrt ( np . trace ( S . T @ S ) * np . trace ( T . T @ T )) print ( raw ) # numerator numer1 = np . trace ( S . T @ T ) numer2 = np . linalg . norm ( S . T @ T ) print ( numer1 , numer2 ) # denominator denom1 = np . sqrt ( np . trace ( S . T @ S ) * np . trace ( T . T @ T )) denom2 = np . sqrt ( np . linalg . norm ( S . T @ S ) * np . linalg . norm ( T . T @ T )) print ( denom1 , denom2 ) 0.9999999999999999 229.5889532845504 229.58895328455043 229.58895328455043 229.58895328455043 Proposed \u00b6 \\frac{tr(X^\\top Y)}{\\sqrt{tr(X^\\top X)tr(Y^\\top Y)}} \\frac{tr(X^\\top Y)}{\\sqrt{tr(X^\\top X)tr(Y^\\top Y)}} <span><span class=\"MathJax_Preview\">\\frac{tr(X^\\top Y)}{\\sqrt{tr(X^\\top X)tr(Y^\\top Y)}}</span><script type=\"math/tex\">\\frac{tr(X^\\top Y)}{\\sqrt{tr(X^\\top X)tr(Y^\\top Y)}} # proposed raw = np . trace ( X . T @ Y ) / np . sqrt ( np . trace ( X . T @ X ) * np . trace ( Y . T @ Y )) print ( raw ) # numerator numer1 = np . trace ( X . T @ Y ) numer2 = np . linalg . norm ( X . T @ Y ) print ( numer1 , numer2 ) 1.0 15.152193018984097 15.152193018984097 cov_feat_norm = cov_hs_features ( X , Y ) print ( cov_feat_norm ) X_ft_norm = cov_hs_features ( X , X ) Y_ft_norm = cov_hs_features ( Y , Y ) corr_feat_norm = cov_feat_norm / ( X_ft_norm * Y_ft_norm ) print ( corr_feat_norm ) 1.1016198535241324 0.9077541556653632 np . inner ( np . cov ( X , X . T ), np . cov ( Y , Y . T )) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-352-625ad9b4cf86> in <module> ----> 1 np . inner ( np . cov ( X , X . T ) , np . cov ( Y , Y . T ) ) <__array_function__ internals> in cov (*args, **kwargs) ~/.conda/envs/it4dnn/lib/python3.6/site-packages/numpy/lib/function_base.py in cov (m, y, rowvar, bias, ddof, fweights, aweights) 2388 if not rowvar and y . shape [ 0 ] != 1 : 2389 y = y . T -> 2390 X = np . concatenate ( ( X , y ) , axis = 0 ) 2391 2392 if ddof is None : <__array_function__ internals> in concatenate (*args, **kwargs) ValueError : all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 1 and the array at index 1 has size 20 def cov_hs_samples ( X , Y ): # calculate samples covariance matrix K_x = np . cov ( X . T ) K_y = np . cov ( Y . T ) # summarize return np . sum ( K_x * K_y ) cov_samp_norm = cov_hs_samples ( X , Y ) print ( cov_samp_norm ) X_samp_norm = cov_hs_samples ( X , X ) Y_samp_norm = cov_hs_samples ( Y , Y ) corr_samp_norm = cov_samp_norm / np . sqrt ( X_samp_norm * Y_samp_norm ) print ( corr_samp_norm ) 0.0014930782114528126 1.0 cov_norm = cov_hs_features ( X , Y ) print ( cov_norm ) 1.1016198535241324 def get_linear_hsic ( X , Y ): hsic_model = HSIC ( kernel = 'linear' , scorer = 'hsic' , bias = True ) hsic_model . fit ( X , Y ); hsic_score = hsic_model . score ( X ) return hsic_score def get_linear_cka ( X , Y ): hsic_model = HSIC ( kernel = 'linear' , scorer = 'tka' ) hsic_model . fit ( X , Y ); hsic_score = hsic_model . score ( X ) return hsic_score cka_score = get_linear_cka ( X , Y ) print ( cka_score ) 1.0000000000000002 hsic_score = get_linear_hsic ( X , Y ) print ( hsic_score ) 0.0060638424362907456 # Samples Covariance Trace np . trace ( np . cov ( X . T ) @ np . cov ( Y . T )) 0.00671893898757977 # Feature Covariance Trace np . linalg . norm ( np . cov ( X , Y ), ord = 'fro' ) 0.20492283589774365 np . linalg . norm ( X . T @ Y , ord = 'fro' ) 13.366924136623567 def corr_hs ( X , Y ): # calculate summarize covariance matrix cov_sum = cov_hs ( X , Y ) # summarize X_sum = cov_hs ( X , X ) Y_sum = cov_hs ( Y , Y ) # calculate correlation return cov_sum / np . sqrt ( X_sum * Y_sum ) corr_sum = corr_hs ( X , Y ) print ( corr_sum ) 1.0 # calculate empirical covariance cov = X . T @ Y assert cov . shape == ( X . shape [ 1 ], Y . shape [ 1 ]) cov array([[2.55059567, 2.2232904 , 1.81886892, 2.1282874 , 2.6284517 ], [2.2232904 , 3.78740257, 2.82952369, 2.78454246, 2.93383308], [1.81886892, 2.82952369, 2.83388253, 2.62597812, 2.54855784], [2.1282874 , 2.78454246, 2.62597812, 2.92247425, 2.66389554], [2.6284517 , 2.93383308, 2.54855784, 2.66389554, 3.48777985]]) So, we see that the covariance doesn't seem to change very much between datasets. Correlation \u00b6 This is the normalized version of the covariance measured mentioned above. This is done by dividing the covariance by the product of the standard deviation of the two samples X and Y. So the forumaltion is: \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} With this normalization, we now have a measure that is bounded between -1 and 1. This makes it much more interpretable and also invariant to isotropic scaling, \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+} def corr ( X , Y ): # get standard deviation X_std , Y_std = X . std (), Y . std () # calculate the correlation cov_xy = cov ( X , Y ) # calculate the correlation return ( cov_xy / ( X_std * Y_std )) . item () corr_xy = corr ( X , Y ) print ( corr_xy ) 0.7422004694043999 Now that it is bounded between -1 and 1, this value let's us know that this value is equivalent to being close to 1. So fairly similar. test_anscombe ( corr , 'corr' ) So at this point, this is a bit of a red flag. All of the \\rho \\rho values are the same but we can see very clearly that there are some key differences between the distributions. The covariance nor the correlation measure gave us useful information. Root Mean Squared \u00b6 This is a popular measure for measuring the errors between two datasets. More or less, it is a covariance measure that penalizes higher deviations between the datasets. # covariance formula def rmse ( X , Y ): n_samples = X . shape [ 0 ] # get mean X_mu = X . mean () Y_mu = Y . mean () # remove mean from data X -= X_mu Y -= Y_mu # calculate the squared covariance cov_xy = np . average (( X - Y ) ** 2 , axis = 0 ) return np . sqrt (( cov_xy )) rmse_xy = rmse ( X , Y ) print ( rmse_xy ) 1.936554834777258 Refactor \u00b6 The scikit-learn library has a built-in mean_sqared_error function which you can call and then use the np.sqrt on the output. from sklearn.metrics import mean_squared_error def rmse ( X , Y ): # calculate the squared covariance rmse_xy = mean_squared_error ( X , Y ) return np . sqrt ( rmse_xy ) rmse_xy = rmse ( X , Y ) print ( rmse_xy ) 1.936554834777258 test_anscombe ( rmse , 'rmse' ) HSIC \u00b6 def get_linear_hsic ( X , Y ): hsic_model = HSIC ( kernel = 'linear' , scorer = 'hsic' , bias = True ) hsic_model . fit ( X [:, None ], Y [:, None ]); hsic_score = hsic_model . score ( X [:, None ]) return hsic_score hsic_score = get_linear_hsic ( X , Y ) print ( hsic_score ) 24.972734710743808 test_anscombe ( get_linear_hsic , 'hsic_lin' ) RBF Kernel \u00b6 def get_rbf_hsic ( X , Y ): hsic_model = HSIC ( kernel = 'rbf' , scorer = 'hsic' ) hsic_model . fit ( X [:, None ], Y [:, None ]); hsic_score = hsic_model . score ( X [:, None ]) return hsic_score test_anscombe ( get_rbf_hsic , 'hsic_rbf' ) Kernel Alignment \u00b6 Linear \u00b6 def get_linear_ka ( X , Y ): hsic_model = HSIC ( kernel = 'linear' , scorer = 'tka' ) hsic_model . fit ( X [:, None ], Y [:, None ]); hsic_score = hsic_model . score ( X [:, None ]) return hsic_score test_anscombe ( get_linear_ka , 'cka_lin' ) RBF Kernel \u00b6 def get_rbf_ka ( X , Y ): hsic_model = HSIC ( kernel = 'rbf' , scorer = 'tka' ) hsic_model . fit ( X [:, None ], Y [:, None ]); hsic_score = hsic_model . score ( X [:, None ]) return hsic_score test_anscombe ( get_rbf_ka , 'ka_rbf' ) Mutual Information \u00b6 In this section, I will be doing the same thing as before except this time I will use the equivalent Information Theory Measures. In principle, they should be better at capturing non-linear relationships and I will be able to add different representations using spatial-temporal information. Entropy \u00b6 This is the simplest and it is analogous to the standard deviation \\sigma \\sigma . Entropy is defined by H(X) = - \\int_{X} f(x) \\log f(x) dx H(X) = - \\int_{X} f(x) \\log f(x) dx This is the expected amount of uncertainty present in a given distributin function f(X) f(X) . It captures the amount of surprise within a distribution. So if there are a large number of low probability events, then the expected uncertainty will be higher. Whereas distributions with fairly equally likely events will have low entropy values as there are not many surprise events, e.g. Uniform. kde = smi . nonparametric . KDEUnivariate ( Y ) kde . fit () print ( kde . entropy ) plt . plot ( kde . support , kde . density ) 1.9670005831544313 [<matplotlib.lines.Line2D at 0x7faec83543c8>] import scipy.stats def entropy ( data , method = 'counts' ): if method == 'counts' : _ , pdata = np . unique ( data , return_counts = True ) entropy = scipy . stats . entropy ( pdata ) elif method == 'kde' : kde = smi . nonparametric . KDEUnivariate ( data ) kde . fit () entropy = kde . entropy else : raise ValueError ( 'Unrecognized method.' ) return entropy Hx = entropy ( X , 'counts' ) Hy = entropy ( Y , 'counts' ) print ( Hx , Hy ) 2.3978952727983707 2.3978952727983707 Mutual Information \u00b6 Given two distributions X and Y, we can calculate the mutual information as I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy where p(x,y) p(x,y) is the joint probability and p_x(x), p_y(y) p_x(x), p_y(y) are the marginal probabilities of X X and Y Y respectively. We can also express the mutual information as a function of the Entropy H(X) H(X) I(X,Y)=H(X) + H(Y) - H(X,Y) I(X,Y)=H(X) + H(Y) - H(X,Y) <span><span class=\"MathJax_Preview\">I(X,Y)=H(X) + H(Y) - H(X,Y)</span><script type=\"math/tex\">I(X,Y)=H(X) + H(Y) - H(X,Y) def mutual_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) return Hx + Hy - Hxy Hxy = entropy ( pd . concat (( X , Y ))) mi_xy = mutual_info ( X . values , Y . values ) print ( mi_xy ) 2.203120400100416 test_anscombe ( mutual_info , 'kde' ) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/sandbox/nonparametric/kernels.py:204: RuntimeWarning: divide by zero encountered in double_scalars w = 1. / (h * n) * np.sum(self((xs-x)/h), axis=0) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/sandbox/nonparametric/kernels.py:204: RuntimeWarning: divide by zero encountered in true_divide w = 1. / (h * n) * np.sum(self((xs-x)/h), axis=0) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/sandbox/nonparametric/kernels.py:204: RuntimeWarning: invalid value encountered in multiply w = 1. / (h * n) * np.sum(self((xs-x)/h), axis=0) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/nonparametric/kde.py:232: IntegrationWarning: The occurrence of roundoff error is detected, which prevents the requested tolerance from being achieved. The error may be underestimated. return -integrate.quad(entr, a, b, args=(endog,))[0] def norm_mutual_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) # mutual information mi_xy = Hx + Hy - Hxy return ( mi_xy / ( np . sqrt ( Hx * Hy ))) test_anscombe ( norm_mutual_info , 'nkde' ) def red_mutual_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) # mutual information mi_xy = Hx + Hy - Hxy return ( 2 * mi_xy / ( Hx + Hy )) test_anscombe ( red_mutual_info , 'rkde' ) Variation of Information \u00b6 $$ \\begin{aligned} VI(X,Y) &= H(X) + H(Y) - 2I(X,Y) \\ &= I(X,X) + I(Y,Y) - 2I(X,Y) \\end{aligned}$$ def variation_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) # mutual information mi_xy = Hx + Hy - Hxy # variation of information vi_xy = Hx + Hy - 2 * mi_xy return vi_xy test_anscombe ( variation_info , 'vikde' ) RVI-Based Diagram \u00b6 Analagous to the Taylor Diagram, we can summarize the ITMs in a way that was easy to interpret. It used the relationship between the entropy, the mutual information and the normalized mutual information via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} where The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the entropy of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the entropy of the simulated data \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} - the normalized mutual information RMSE RMSE - the variation of information between the two datasets h_a = entropy ( X , 'counts' ) h_b = entropy ( Y , 'kde' ) print ( 'H(X),H(Y):' , h_a , h_b ) # joint entropy h_ab = entropy ( pd . concat (( X , Y )), 'kde' ) print ( 'H(X,Y):' , h_ab ) # mutual information mi_ab = h_a + h_b - h_ab print ( 'MI(X,Y):' , mi_ab ) # normalized mutual information nmi_ab = mi_ab / np . sqrt ( h_a * h_b ) print ( 'NMI(X,Y):' , nmi_ab ) # scaled mutual info smi_ab = mi_ab * ( h_ab / ( h_a * h_b )) print ( 'SMI(X,Y):' , smi_ab ) # cos rho term c_ab = 2 * smi_ab - 1 print ( 'C_XY:' , c_ab ) # vi vi = h_a + h_b - 2 * np . sqrt ( h_a * h_b ) * nmi_ab print ( 'VI(X,Y):' , vi ) H(X),H(Y): 2.3978952727983707 1.967000583154429 H(X,Y): 2.51573500302577 MI(X,Y): 1.8491608529270298 NMI(X,Y): 0.8514464531077769 SMI(X,Y): 0.986290574938243 C_XY: 0.972581149876486 VI(X,Y): 0.6665741500987403 def vi_coeffs ( X , Y , method = 'counts' ): # entropy observations h_a = entropy ( X , method ) # entropy simulated h_b = entropy ( Y , method ) # joint entropy h_ab = entropy ( pd . concat (( X , Y )), method ) # mutual information mi_ab = h_a + h_b - h_ab # normalized mutual information nmi_ab = mi_ab / np . sqrt ( h_a * h_b ) # scaled mutual information smi_ab = 2 * mi_ab * ( h_ab / ( h_a * h_b )) - 1 # vi vi_ab = h_a + h_b - 2 * np . sqrt (( h_a * h_b )) * nmi_ab # save coefficients data = { 'h_a' : h_a , 'h_b' : h_b , 'nmi' : nmi_ab , 'smi' : smi_ab , 'theta' : np . arccos ( nmi_ab ), 'vi' : vi_ab } return data # Model I X = get_case ( df_anscombe , 'I' )[ 'x' ] Y = get_case ( df_anscombe , 'I' )[ 'y' ] data1 = vi_coeffs ( X , Y , 'kde' ) print ( data1 ) # Model II X = get_case ( df_anscombe , 'II' )[ 'x' ] Y = get_case ( df_anscombe , 'II' )[ 'y' ] data2 = vi_coeffs ( X , Y , 'kde' ) print ( data2 ) # Model III X = get_case ( df_anscombe , 'III' )[ 'x' ] Y = get_case ( df_anscombe , 'III' )[ 'y' ] data3 = vi_coeffs ( X , Y , 'kde' ) print ( data3 ) # # Model IV # X = get_case(df_anscombe, 'IV')['x'] # Y = get_case(df_anscombe, 'IV')['y'] # data4 = vi_coeffs(X, Y) # print(data4) {'h_a': 2.7518548199717574, 'h_b': 2.2080793522856723, 'nmi': 0.9751276249886281, 'smi': 1.0224173292681424, 'theta': 0.2235002023733858, 'vi': 0.15251984951237407} {'h_a': 2.7518548199717574, 'h_b': 2.1269598407974937, 'nmi': 0.9842327930994981, 'smi': 1.0321990011233386, 'theta': 0.1778134760779669, 'vi': 0.11647649545914085} {'h_a': 2.7518548199717574, 'h_b': 1.967000583154429, 'nmi': 0.9469416711631927, 'smi': 1.0478734393463243, 'theta': 0.32721332665482533, 'vi': 0.31261460292535403} import matplotlib.pyplot as plt import numpy as np theta = np . linspace ( 0 , np . pi ) r = np . sin ( theta ) fig = plt . figure ( figsize = ( 7 , 5 )) ax = fig . add_subplot ( 111 , polar = True ) m = ax . scatter ( 0 , data1 [ 'h_a' ], s = 200 , alpha = 0.75 , label = 'Data' , zorder = 0 ) m1 = ax . scatter ( data1 [ 'theta' ], data1 [ 'h_b' ], s = 150 , alpha = 0.75 , marker = 'x' , label = 'Model I' ) m1 = ax . scatter ( data2 [ 'theta' ], data2 [ 'h_b' ], s = 150 , alpha = 0.75 , marker = 'o' , label = 'Model II' ) m1 = ax . scatter ( data3 [ 'theta' ], data3 [ 'h_b' ], s = 150 , alpha = 0.75 , marker = '.' , label = 'Model III' ) # m1 = ax.scatter(theta4, b4, s=100, alpha=0.75, marker='o', label='Model II') # ax.plot(0) ax . set_ylim ([ 0 , 3 ]) # ax.set_xticks([0.1, 0.2, 0.3, 0.9]) # ax.set_xticklabels([1.0, 0.9, 0.8, 0.6, 0.3, 0.2, 0.1]) # m1 = ax.scatter(theta1, a, s=50, alpha=0.75) # m1 = ax.scatter(theta1, a, s=50, alpha=0.75) c = ax . plot ( theta , data1 [ 'h_a' ] * np . ones ( theta . shape ), color = 'black' , linestyle = 'dashed' , alpha = 0.75 ) ax . set_xlabel ( 'Entropy' , labelpad = 20 ) ax . set_ylabel ( 'Entropy' , labelpad = 20 ) plt . legend () ax . set_thetamin ( 0 ) ax . set_thetamax ( 90 ) plt . tight_layout () plt . savefig ( SAVE_PATH + 'demo_vi.png' ) plt . show ()","title":"Variation of information"},{"location":"notebooks/explore/variation_of_information/#exploring-variation-of-information","text":"","title":"Exploring: Variation of Information"},{"location":"notebooks/explore/variation_of_information/#background","text":"My projects involve trying to compare the outputs of different climate models. There are currently more than 20+ climate models from different companies and each of them try to produce the most accurate prediction of some physical phenomena, e.g. Sea Surface Temperature, Mean Sea Level Pressure, etc. However, it's a difficult task to provide accurate comparison techniques for each of the models. There exist some methods such as the mean and standard deviation. There is also a very common framework of visually summarizing this information in the form of Taylor Diagrams. However, the drawback of using these methods is that they are typically non-linear methods and they cannot handle multidimensional, multivariate data. Another way to measure similarity would be in the family of Information Theory Measures (ITMs). Instead of directly measuring first-order output statistics, these methods summarize the information via a probability distribution function (PDF) of the dataset. These can measure non-linear relationships and are naturally multivariate that offers solutions to the shortcomings of the standard methods. I would like to explore this and see if this is a useful way of summarizing information. This is removing the","title":"Background"},{"location":"notebooks/explore/variation_of_information/#code","text":"import numpy as np import seaborn as sns import pandas as pd import statsmodels.api as smi import sys sys . path . insert ( 0 , '/home/emmanuel/code/kernel_model_zoo/' ) import matplotlib.pyplot as plt plt . style . use ( 'seaborn-talk' ) % matplotlib inline % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload from kernellib.dependence import HSIC SAVE_PATH = \"/home/emmanuel/projects/2020_rbig_rs/reports/figures/explore/vi/\"","title":"Code"},{"location":"notebooks/explore/variation_of_information/#data","text":"We will use the classic dataset for Anscombe's quartet. This is a staple dataset which shows how we need to take care when comparing two datasets. In the example, we will show how visually, two datasets will look similar, but using a correlation measure like the Pearson's coefficient will fail because it is not able to capture the non-linear relationship between the two distributions. # load dataset df_anscombe = sns . load_dataset ( 'anscombe' ) df_anscombe . dataset . unique () array(['I', 'II', 'III', 'IV'], dtype=object) def get_case ( df : pd . DataFrame , case : str = 'I' ): return df [ df [ 'dataset' ] == case ] def plot_cases ( df : pd . DataFrame , case : str = 'I' , save = True , plot_type = 'reg' ): df = get_case ( df , case ) plt . figure ( figsize = ( 4 , 4 )) if plot_type == 'reg' : pts = sns . regplot ( x = \"x\" , y = \"y\" , data = df , ) elif plot_type == 'joint' : pts = sns . jointplot ( x = \"x\" , y = \"y\" , data = df , kind = \"regplot\" , ) elif plot_type == 'density' : pts = sns . jointplot ( x = \"x\" , y = \"y\" , data = df , kind = \"kde\" , ) else : raise ValueError ( '' ) plt . xlabel ( \"\" ) plt . ylabel ( \"\" ) plt . xticks ([]) plt . yticks ([]) # plt.axis('off') plt . tight_layout () if save is not None : plt . savefig ( SAVE_PATH + f 'demo_case { case } _ { plot_type } .png' , dpi = 200 , transparent = True ) return None plot_cases ( df_anscombe , 'III' , plot_type = 'reg' ) This is a very simple case where we have a linear relationship between the datasets. The regression plot above shows a linear line that is fit between the two distributions. We can also see the marginal distributions (the histograms) for X and Y. As you can see, they are definitely similar. But now, we are going to look at a way to summarize this information.","title":"Data"},{"location":"notebooks/explore/variation_of_information/#mathematics","text":"There are a few important quantities to consider when we need to represent the statistics and compare two datasets. Variance Covariance Correlation Root Mean Squared","title":"Mathematics"},{"location":"notebooks/explore/variation_of_information/#covariance","text":"The covariance is a measure to determine how much two variances change. The covariance between X and Y is given by: C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) C(X,Y)=\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu_x)(y_i - \\mu_i) where N N is the number of elements in both datasets. Notice how this formula assumes that the number of samples for X and Y are equivalent. This measure is unbounded as it can have a value between -\\infty -\\infty and \\infty \\infty . Let's look at an example of how to calculate this below. # covariance formula def cov ( X , Y ): n_samples = X . shape [ 0 ] # get mean X_mu = X . mean () Y_mu = Y . mean () cov_xy = 0 # loop through the data points for ix in range ( n_samples ): cov_xy += ( X . values [ ix ] - X_mu ) * ( Y . values [ ix ] - Y_mu ) return cov_xy / n_samples # extract the data X = get_case ( df_anscombe , 'I' )[ 'x' ] Y = get_case ( df_anscombe , 'I' )[ 'y' ] # get covariance cov_xy = cov ( X , Y ) print ( cov_xy ) 5.000909090909091 X . values [:, None ] . reshape ( - 1 , 1 ) . shape (11, 1) That number is fairly meaningless now. But we can compare the covariance number of this versus the other cases.","title":"Covariance"},{"location":"notebooks/explore/variation_of_information/#refactor","text":"We can remove the loop by doing a matrix multiplication. C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) C(X,Y)=\\frac{1}{N} (X-X_\\mu)^\\top (Y-Y_\\mu) where X,Y \\in \\mathbb{R}^{N\\times 1} X,Y \\in \\mathbb{R}^{N\\times 1} np . dot ( X [:, None ] . T - X . mean (), Y [:, None ] - Y . mean ()) / X . shape [ 0 ] # covariance formula def cov ( X , Y ): n_samples = X . shape [ 0 ] # get mean X_mu = X . mean () Y_mu = Y . mean () # remove mean from data X -= X_mu Y -= Y_mu # Ensure 2d X = np . atleast_2d ( X ) . reshape ( - 1 , 1 ) Y = np . atleast_2d ( Y ) . reshape ( - 1 , 1 ) # calculate the covariance cov_xy = X . T @ Y return ( cov_xy / n_samples ) . item () def test_anscombe ( func , save_name = None ): fig , axs = plt . subplots ( nrows = 2 , ncols = 2 , figsize = ( 7 , 5 )) for iax , icase in zip ( axs . ravel (), [ 'I' , 'II' , 'III' , 'IV' ]): # data X = get_case ( df_anscombe , icase )[ 'x' ] Y = get_case ( df_anscombe , icase )[ 'y' ] output = func ( X . values , Y . values ) iax . scatter ( X . values , Y . values , label = f \"Case { icase } : $C$= { output : .2f } \" ) iax . legend () # iax.legend(f\"Case: {icase}\") # get covariance # print(f\"Case {icase}: {cov_xy.item()}\") plt . tight_layout () if save_name is not None : plt . savefig ( SAVE_PATH + f \"demo_ { save_name } .png\" ) plt . show () test_anscombe ( cov , 'cov' )","title":"Refactor"},{"location":"notebooks/explore/variation_of_information/#multi-variate-multi-dimensional","text":"np . random . seed ( 123 ) X = np . random . randn ( 20 , 2 ) Y = 0.5 * X # calculate covariance matrix cov = np . cov ( X . squeeze (), Y . squeeze ()) print ( X . shape , Y . shape , cov . shape ) (20, 2) (20, 2) (40, 40) cov . shape (40, 40) def cov_hs_features ( X , Y ): # calculate covariance matrix cov_xy = np . cov ( X , Y ) # summarize information cov_sum = np . linalg . norm ( cov , ord = 'fro' ) return cov_sum # ||X.T @ Y||_F - feature space lhs = np . linalg . norm ( X . T @ Y , ord = 'fro' ) ** 2 print ( lhs ) # ||XX.T @ YY.T||_F - sample space mhs = np . trace ( X @ X . T @ Y @ Y . T ) print ( mhs ) 379.05326003069405 379.053260030694 # ||X.T @ Y||_F - feature space lhs = np . linalg . norm ( np . cov ( X , Y ), ord = 'fro' ) ** 2 print ( lhs ) # ||XX.T @ YY.T||_F - sample space mhs = np . trace ( X @ X . T @ Y @ Y . T ) print ( mhs ) 913.0831824181817 379.053260030694 # RHS raw = np . trace ( X @ Y . T ) / np . sqrt ( np . trace ( X @ X . T ) * np . trace ( Y @ Y . T )) print ( raw ) # MHS 1.0","title":"Multi-Variate (Multi-Dimensional)"},{"location":"notebooks/explore/variation_of_information/#formula-1","text":"\\frac{tr(XY^\\top}{\\sqrt{tr(XX^\\top)tr(YY^T)}} \\frac{tr(XY^\\top}{\\sqrt{tr(XX^\\top)tr(YY^T)}} <span><span class=\"MathJax_Preview\">\\frac{tr(XY^\\top}{\\sqrt{tr(XX^\\top)tr(YY^T)}}</span><script type=\"math/tex\">\\frac{tr(XY^\\top}{\\sqrt{tr(XX^\\top)tr(YY^T)}} # raw formula raw = np . trace ( X @ Y . T ) / np . sqrt ( np . trace ( X @ X . T ) * np . trace ( Y @ Y . T )) print ( raw ) # numerator numer1 = np . trace ( X @ Y . T ) ** 2 numer2 = np . linalg . norm ( X @ Y . T ) print ( numer1 , numer2 ) 1.0 229.5889532845504 15.152193018984098","title":"Formula 1"},{"location":"notebooks/explore/variation_of_information/#formula-ii","text":"\\frac{tr(XX\\top YY^\\top)}{\\sqrt{tr(XX^\\top XX^\\top)tr(YY^\\top YY^\\top)}} \\frac{tr(XX\\top YY^\\top)}{\\sqrt{tr(XX^\\top XX^\\top)tr(YY^\\top YY^\\top)}} <span><span class=\"MathJax_Preview\">\\frac{tr(XX\\top YY^\\top)}{\\sqrt{tr(XX^\\top XX^\\top)tr(YY^\\top YY^\\top)}}</span><script type=\"math/tex\">\\frac{tr(XX\\top YY^\\top)}{\\sqrt{tr(XX^\\top XX^\\top)tr(YY^\\top YY^\\top)}} # formula 2 S = X @ X . T T = Y @ Y . T raw = np . trace ( S . T @ T ) / np . sqrt ( np . trace ( S . T @ S ) * np . trace ( T . T @ T )) print ( raw ) # numerator numer1 = np . trace ( S . T @ T ) numer2 = np . linalg . norm ( S . T @ T ) print ( numer1 , numer2 ) # denominator denom1 = np . sqrt ( np . trace ( S . T @ S ) * np . trace ( T . T @ T )) denom2 = np . sqrt ( np . linalg . norm ( S . T @ S ) * np . linalg . norm ( T . T @ T )) print ( denom1 , denom2 ) 0.9999999999999999 229.5889532845504 229.58895328455043 229.58895328455043 229.58895328455043","title":"Formula II"},{"location":"notebooks/explore/variation_of_information/#proposed","text":"\\frac{tr(X^\\top Y)}{\\sqrt{tr(X^\\top X)tr(Y^\\top Y)}} \\frac{tr(X^\\top Y)}{\\sqrt{tr(X^\\top X)tr(Y^\\top Y)}} <span><span class=\"MathJax_Preview\">\\frac{tr(X^\\top Y)}{\\sqrt{tr(X^\\top X)tr(Y^\\top Y)}}</span><script type=\"math/tex\">\\frac{tr(X^\\top Y)}{\\sqrt{tr(X^\\top X)tr(Y^\\top Y)}} # proposed raw = np . trace ( X . T @ Y ) / np . sqrt ( np . trace ( X . T @ X ) * np . trace ( Y . T @ Y )) print ( raw ) # numerator numer1 = np . trace ( X . T @ Y ) numer2 = np . linalg . norm ( X . T @ Y ) print ( numer1 , numer2 ) 1.0 15.152193018984097 15.152193018984097 cov_feat_norm = cov_hs_features ( X , Y ) print ( cov_feat_norm ) X_ft_norm = cov_hs_features ( X , X ) Y_ft_norm = cov_hs_features ( Y , Y ) corr_feat_norm = cov_feat_norm / ( X_ft_norm * Y_ft_norm ) print ( corr_feat_norm ) 1.1016198535241324 0.9077541556653632 np . inner ( np . cov ( X , X . T ), np . cov ( Y , Y . T )) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-352-625ad9b4cf86> in <module> ----> 1 np . inner ( np . cov ( X , X . T ) , np . cov ( Y , Y . T ) ) <__array_function__ internals> in cov (*args, **kwargs) ~/.conda/envs/it4dnn/lib/python3.6/site-packages/numpy/lib/function_base.py in cov (m, y, rowvar, bias, ddof, fweights, aweights) 2388 if not rowvar and y . shape [ 0 ] != 1 : 2389 y = y . T -> 2390 X = np . concatenate ( ( X , y ) , axis = 0 ) 2391 2392 if ddof is None : <__array_function__ internals> in concatenate (*args, **kwargs) ValueError : all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 1 and the array at index 1 has size 20 def cov_hs_samples ( X , Y ): # calculate samples covariance matrix K_x = np . cov ( X . T ) K_y = np . cov ( Y . T ) # summarize return np . sum ( K_x * K_y ) cov_samp_norm = cov_hs_samples ( X , Y ) print ( cov_samp_norm ) X_samp_norm = cov_hs_samples ( X , X ) Y_samp_norm = cov_hs_samples ( Y , Y ) corr_samp_norm = cov_samp_norm / np . sqrt ( X_samp_norm * Y_samp_norm ) print ( corr_samp_norm ) 0.0014930782114528126 1.0 cov_norm = cov_hs_features ( X , Y ) print ( cov_norm ) 1.1016198535241324 def get_linear_hsic ( X , Y ): hsic_model = HSIC ( kernel = 'linear' , scorer = 'hsic' , bias = True ) hsic_model . fit ( X , Y ); hsic_score = hsic_model . score ( X ) return hsic_score def get_linear_cka ( X , Y ): hsic_model = HSIC ( kernel = 'linear' , scorer = 'tka' ) hsic_model . fit ( X , Y ); hsic_score = hsic_model . score ( X ) return hsic_score cka_score = get_linear_cka ( X , Y ) print ( cka_score ) 1.0000000000000002 hsic_score = get_linear_hsic ( X , Y ) print ( hsic_score ) 0.0060638424362907456 # Samples Covariance Trace np . trace ( np . cov ( X . T ) @ np . cov ( Y . T )) 0.00671893898757977 # Feature Covariance Trace np . linalg . norm ( np . cov ( X , Y ), ord = 'fro' ) 0.20492283589774365 np . linalg . norm ( X . T @ Y , ord = 'fro' ) 13.366924136623567 def corr_hs ( X , Y ): # calculate summarize covariance matrix cov_sum = cov_hs ( X , Y ) # summarize X_sum = cov_hs ( X , X ) Y_sum = cov_hs ( Y , Y ) # calculate correlation return cov_sum / np . sqrt ( X_sum * Y_sum ) corr_sum = corr_hs ( X , Y ) print ( corr_sum ) 1.0 # calculate empirical covariance cov = X . T @ Y assert cov . shape == ( X . shape [ 1 ], Y . shape [ 1 ]) cov array([[2.55059567, 2.2232904 , 1.81886892, 2.1282874 , 2.6284517 ], [2.2232904 , 3.78740257, 2.82952369, 2.78454246, 2.93383308], [1.81886892, 2.82952369, 2.83388253, 2.62597812, 2.54855784], [2.1282874 , 2.78454246, 2.62597812, 2.92247425, 2.66389554], [2.6284517 , 2.93383308, 2.54855784, 2.66389554, 3.48777985]]) So, we see that the covariance doesn't seem to change very much between datasets.","title":"Proposed"},{"location":"notebooks/explore/variation_of_information/#correlation","text":"This is the normalized version of the covariance measured mentioned above. This is done by dividing the covariance by the product of the standard deviation of the two samples X and Y. So the forumaltion is: \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} \\rho(X, Y) = \\frac{C(X,Y)}{\\sigma_x \\sigma_y} With this normalization, we now have a measure that is bounded between -1 and 1. This makes it much more interpretable and also invariant to isotropic scaling, \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) \\rho(X,Y)=\\rho(\\alpha X, \\beta Y) where \\alpha, \\beta \\in \\mathbb{R}^{+} \\alpha, \\beta \\in \\mathbb{R}^{+} def corr ( X , Y ): # get standard deviation X_std , Y_std = X . std (), Y . std () # calculate the correlation cov_xy = cov ( X , Y ) # calculate the correlation return ( cov_xy / ( X_std * Y_std )) . item () corr_xy = corr ( X , Y ) print ( corr_xy ) 0.7422004694043999 Now that it is bounded between -1 and 1, this value let's us know that this value is equivalent to being close to 1. So fairly similar. test_anscombe ( corr , 'corr' ) So at this point, this is a bit of a red flag. All of the \\rho \\rho values are the same but we can see very clearly that there are some key differences between the distributions. The covariance nor the correlation measure gave us useful information.","title":"Correlation"},{"location":"notebooks/explore/variation_of_information/#root-mean-squared","text":"This is a popular measure for measuring the errors between two datasets. More or less, it is a covariance measure that penalizes higher deviations between the datasets. # covariance formula def rmse ( X , Y ): n_samples = X . shape [ 0 ] # get mean X_mu = X . mean () Y_mu = Y . mean () # remove mean from data X -= X_mu Y -= Y_mu # calculate the squared covariance cov_xy = np . average (( X - Y ) ** 2 , axis = 0 ) return np . sqrt (( cov_xy )) rmse_xy = rmse ( X , Y ) print ( rmse_xy ) 1.936554834777258","title":"Root Mean Squared"},{"location":"notebooks/explore/variation_of_information/#refactor_1","text":"The scikit-learn library has a built-in mean_sqared_error function which you can call and then use the np.sqrt on the output. from sklearn.metrics import mean_squared_error def rmse ( X , Y ): # calculate the squared covariance rmse_xy = mean_squared_error ( X , Y ) return np . sqrt ( rmse_xy ) rmse_xy = rmse ( X , Y ) print ( rmse_xy ) 1.936554834777258 test_anscombe ( rmse , 'rmse' )","title":"Refactor"},{"location":"notebooks/explore/variation_of_information/#hsic","text":"def get_linear_hsic ( X , Y ): hsic_model = HSIC ( kernel = 'linear' , scorer = 'hsic' , bias = True ) hsic_model . fit ( X [:, None ], Y [:, None ]); hsic_score = hsic_model . score ( X [:, None ]) return hsic_score hsic_score = get_linear_hsic ( X , Y ) print ( hsic_score ) 24.972734710743808 test_anscombe ( get_linear_hsic , 'hsic_lin' )","title":"HSIC"},{"location":"notebooks/explore/variation_of_information/#rbf-kernel","text":"def get_rbf_hsic ( X , Y ): hsic_model = HSIC ( kernel = 'rbf' , scorer = 'hsic' ) hsic_model . fit ( X [:, None ], Y [:, None ]); hsic_score = hsic_model . score ( X [:, None ]) return hsic_score test_anscombe ( get_rbf_hsic , 'hsic_rbf' )","title":"RBF Kernel"},{"location":"notebooks/explore/variation_of_information/#kernel-alignment","text":"","title":"Kernel Alignment"},{"location":"notebooks/explore/variation_of_information/#linear","text":"def get_linear_ka ( X , Y ): hsic_model = HSIC ( kernel = 'linear' , scorer = 'tka' ) hsic_model . fit ( X [:, None ], Y [:, None ]); hsic_score = hsic_model . score ( X [:, None ]) return hsic_score test_anscombe ( get_linear_ka , 'cka_lin' )","title":"Linear"},{"location":"notebooks/explore/variation_of_information/#rbf-kernel_1","text":"def get_rbf_ka ( X , Y ): hsic_model = HSIC ( kernel = 'rbf' , scorer = 'tka' ) hsic_model . fit ( X [:, None ], Y [:, None ]); hsic_score = hsic_model . score ( X [:, None ]) return hsic_score test_anscombe ( get_rbf_ka , 'ka_rbf' )","title":"RBF Kernel"},{"location":"notebooks/explore/variation_of_information/#mutual-information","text":"In this section, I will be doing the same thing as before except this time I will use the equivalent Information Theory Measures. In principle, they should be better at capturing non-linear relationships and I will be able to add different representations using spatial-temporal information.","title":"Mutual Information"},{"location":"notebooks/explore/variation_of_information/#entropy","text":"This is the simplest and it is analogous to the standard deviation \\sigma \\sigma . Entropy is defined by H(X) = - \\int_{X} f(x) \\log f(x) dx H(X) = - \\int_{X} f(x) \\log f(x) dx This is the expected amount of uncertainty present in a given distributin function f(X) f(X) . It captures the amount of surprise within a distribution. So if there are a large number of low probability events, then the expected uncertainty will be higher. Whereas distributions with fairly equally likely events will have low entropy values as there are not many surprise events, e.g. Uniform. kde = smi . nonparametric . KDEUnivariate ( Y ) kde . fit () print ( kde . entropy ) plt . plot ( kde . support , kde . density ) 1.9670005831544313 [<matplotlib.lines.Line2D at 0x7faec83543c8>] import scipy.stats def entropy ( data , method = 'counts' ): if method == 'counts' : _ , pdata = np . unique ( data , return_counts = True ) entropy = scipy . stats . entropy ( pdata ) elif method == 'kde' : kde = smi . nonparametric . KDEUnivariate ( data ) kde . fit () entropy = kde . entropy else : raise ValueError ( 'Unrecognized method.' ) return entropy Hx = entropy ( X , 'counts' ) Hy = entropy ( Y , 'counts' ) print ( Hx , Hy ) 2.3978952727983707 2.3978952727983707","title":"Entropy"},{"location":"notebooks/explore/variation_of_information/#mutual-information_1","text":"Given two distributions X and Y, we can calculate the mutual information as I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy I(X,Y) = \\int_X\\int_Y p(x,y) \\log \\frac{p(x,y)}{p_x(x)p_y(y)}dxdy where p(x,y) p(x,y) is the joint probability and p_x(x), p_y(y) p_x(x), p_y(y) are the marginal probabilities of X X and Y Y respectively. We can also express the mutual information as a function of the Entropy H(X) H(X) I(X,Y)=H(X) + H(Y) - H(X,Y) I(X,Y)=H(X) + H(Y) - H(X,Y) <span><span class=\"MathJax_Preview\">I(X,Y)=H(X) + H(Y) - H(X,Y)</span><script type=\"math/tex\">I(X,Y)=H(X) + H(Y) - H(X,Y) def mutual_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) return Hx + Hy - Hxy Hxy = entropy ( pd . concat (( X , Y ))) mi_xy = mutual_info ( X . values , Y . values ) print ( mi_xy ) 2.203120400100416 test_anscombe ( mutual_info , 'kde' ) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/sandbox/nonparametric/kernels.py:204: RuntimeWarning: divide by zero encountered in double_scalars w = 1. / (h * n) * np.sum(self((xs-x)/h), axis=0) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/sandbox/nonparametric/kernels.py:204: RuntimeWarning: divide by zero encountered in true_divide w = 1. / (h * n) * np.sum(self((xs-x)/h), axis=0) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/sandbox/nonparametric/kernels.py:204: RuntimeWarning: invalid value encountered in multiply w = 1. / (h * n) * np.sum(self((xs-x)/h), axis=0) /home/emmanuel/.conda/envs/it4dnn/lib/python3.6/site-packages/statsmodels/nonparametric/kde.py:232: IntegrationWarning: The occurrence of roundoff error is detected, which prevents the requested tolerance from being achieved. The error may be underestimated. return -integrate.quad(entr, a, b, args=(endog,))[0] def norm_mutual_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) # mutual information mi_xy = Hx + Hy - Hxy return ( mi_xy / ( np . sqrt ( Hx * Hy ))) test_anscombe ( norm_mutual_info , 'nkde' ) def red_mutual_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) # mutual information mi_xy = Hx + Hy - Hxy return ( 2 * mi_xy / ( Hx + Hy )) test_anscombe ( red_mutual_info , 'rkde' )","title":"Mutual Information"},{"location":"notebooks/explore/variation_of_information/#variation-of-information","text":"$$ \\begin{aligned} VI(X,Y) &= H(X) + H(Y) - 2I(X,Y) \\ &= I(X,X) + I(Y,Y) - 2I(X,Y) \\end{aligned}$$ def variation_info ( X , Y , method = 'kde' ): Hx = entropy ( X , method ) Hy = entropy ( Y , method ) Hxy = entropy ( np . concatenate (( X , Y )), method ) # mutual information mi_xy = Hx + Hy - Hxy # variation of information vi_xy = Hx + Hy - 2 * mi_xy return vi_xy test_anscombe ( variation_info , 'vikde' )","title":"Variation of Information"},{"location":"notebooks/explore/variation_of_information/#rvi-based-diagram","text":"Analagous to the Taylor Diagram, we can summarize the ITMs in a way that was easy to interpret. It used the relationship between the entropy, the mutual information and the normalized mutual information via the triangle inequality. Assuming we can draw a diagram using the law of cosines; c^2 = a^2 + b^2 - 2ab \\cos \\phi c^2 = a^2 + b^2 - 2ab \\cos \\phi we can write this in terms of \\sigma \\sigma , \\rho \\rho and RMSE RMSE as we have expressed above. \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} \\begin{aligned} \\text{RVI}^2 &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\\\ &= H(X) + H(Y) - 2 \\sqrt{H(X)H(Y)} \\rho \\end{aligned} where The sides are as follows: a = \\sigma_{\\text{obs}} a = \\sigma_{\\text{obs}} - the entropy of the observed data b = \\sigma_{\\text{sim}} b = \\sigma_{\\text{sim}} - the entropy of the simulated data \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} \\rho = \\frac{I(X,Y)}{\\sqrt{H(X)H(Y)}} - the normalized mutual information RMSE RMSE - the variation of information between the two datasets h_a = entropy ( X , 'counts' ) h_b = entropy ( Y , 'kde' ) print ( 'H(X),H(Y):' , h_a , h_b ) # joint entropy h_ab = entropy ( pd . concat (( X , Y )), 'kde' ) print ( 'H(X,Y):' , h_ab ) # mutual information mi_ab = h_a + h_b - h_ab print ( 'MI(X,Y):' , mi_ab ) # normalized mutual information nmi_ab = mi_ab / np . sqrt ( h_a * h_b ) print ( 'NMI(X,Y):' , nmi_ab ) # scaled mutual info smi_ab = mi_ab * ( h_ab / ( h_a * h_b )) print ( 'SMI(X,Y):' , smi_ab ) # cos rho term c_ab = 2 * smi_ab - 1 print ( 'C_XY:' , c_ab ) # vi vi = h_a + h_b - 2 * np . sqrt ( h_a * h_b ) * nmi_ab print ( 'VI(X,Y):' , vi ) H(X),H(Y): 2.3978952727983707 1.967000583154429 H(X,Y): 2.51573500302577 MI(X,Y): 1.8491608529270298 NMI(X,Y): 0.8514464531077769 SMI(X,Y): 0.986290574938243 C_XY: 0.972581149876486 VI(X,Y): 0.6665741500987403 def vi_coeffs ( X , Y , method = 'counts' ): # entropy observations h_a = entropy ( X , method ) # entropy simulated h_b = entropy ( Y , method ) # joint entropy h_ab = entropy ( pd . concat (( X , Y )), method ) # mutual information mi_ab = h_a + h_b - h_ab # normalized mutual information nmi_ab = mi_ab / np . sqrt ( h_a * h_b ) # scaled mutual information smi_ab = 2 * mi_ab * ( h_ab / ( h_a * h_b )) - 1 # vi vi_ab = h_a + h_b - 2 * np . sqrt (( h_a * h_b )) * nmi_ab # save coefficients data = { 'h_a' : h_a , 'h_b' : h_b , 'nmi' : nmi_ab , 'smi' : smi_ab , 'theta' : np . arccos ( nmi_ab ), 'vi' : vi_ab } return data # Model I X = get_case ( df_anscombe , 'I' )[ 'x' ] Y = get_case ( df_anscombe , 'I' )[ 'y' ] data1 = vi_coeffs ( X , Y , 'kde' ) print ( data1 ) # Model II X = get_case ( df_anscombe , 'II' )[ 'x' ] Y = get_case ( df_anscombe , 'II' )[ 'y' ] data2 = vi_coeffs ( X , Y , 'kde' ) print ( data2 ) # Model III X = get_case ( df_anscombe , 'III' )[ 'x' ] Y = get_case ( df_anscombe , 'III' )[ 'y' ] data3 = vi_coeffs ( X , Y , 'kde' ) print ( data3 ) # # Model IV # X = get_case(df_anscombe, 'IV')['x'] # Y = get_case(df_anscombe, 'IV')['y'] # data4 = vi_coeffs(X, Y) # print(data4) {'h_a': 2.7518548199717574, 'h_b': 2.2080793522856723, 'nmi': 0.9751276249886281, 'smi': 1.0224173292681424, 'theta': 0.2235002023733858, 'vi': 0.15251984951237407} {'h_a': 2.7518548199717574, 'h_b': 2.1269598407974937, 'nmi': 0.9842327930994981, 'smi': 1.0321990011233386, 'theta': 0.1778134760779669, 'vi': 0.11647649545914085} {'h_a': 2.7518548199717574, 'h_b': 1.967000583154429, 'nmi': 0.9469416711631927, 'smi': 1.0478734393463243, 'theta': 0.32721332665482533, 'vi': 0.31261460292535403} import matplotlib.pyplot as plt import numpy as np theta = np . linspace ( 0 , np . pi ) r = np . sin ( theta ) fig = plt . figure ( figsize = ( 7 , 5 )) ax = fig . add_subplot ( 111 , polar = True ) m = ax . scatter ( 0 , data1 [ 'h_a' ], s = 200 , alpha = 0.75 , label = 'Data' , zorder = 0 ) m1 = ax . scatter ( data1 [ 'theta' ], data1 [ 'h_b' ], s = 150 , alpha = 0.75 , marker = 'x' , label = 'Model I' ) m1 = ax . scatter ( data2 [ 'theta' ], data2 [ 'h_b' ], s = 150 , alpha = 0.75 , marker = 'o' , label = 'Model II' ) m1 = ax . scatter ( data3 [ 'theta' ], data3 [ 'h_b' ], s = 150 , alpha = 0.75 , marker = '.' , label = 'Model III' ) # m1 = ax.scatter(theta4, b4, s=100, alpha=0.75, marker='o', label='Model II') # ax.plot(0) ax . set_ylim ([ 0 , 3 ]) # ax.set_xticks([0.1, 0.2, 0.3, 0.9]) # ax.set_xticklabels([1.0, 0.9, 0.8, 0.6, 0.3, 0.2, 0.1]) # m1 = ax.scatter(theta1, a, s=50, alpha=0.75) # m1 = ax.scatter(theta1, a, s=50, alpha=0.75) c = ax . plot ( theta , data1 [ 'h_a' ] * np . ones ( theta . shape ), color = 'black' , linestyle = 'dashed' , alpha = 0.75 ) ax . set_xlabel ( 'Entropy' , labelpad = 20 ) ax . set_ylabel ( 'Entropy' , labelpad = 20 ) plt . legend () ax . set_thetamin ( 0 ) ax . set_thetamax ( 90 ) plt . tight_layout () plt . savefig ( SAVE_PATH + 'demo_vi.png' ) plt . show ()","title":"RVI-Based Diagram"},{"location":"notebooks/spatial_temporal/1.0_load_esdc/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Part I - Loading the ESDC \u00b6 In this tutorial, we will walk through how one can access the ESDC. Fortunately, we have 2 ways: locally and through the internet. I have already personally downloading the cubes to our in-house server. But for reproducibility purposes, the cubes can be accessed externally as well. So we will walk-through the two ways that this is possible. from xcube.core.dsio import open_cube import pathlib import xarray as xr DATA_PATH = pathlib . Path ( \"/media/disk/databases/ESDC/\" ) Method I - Online \u00b6 For this first example, we will load the cubes from the server. Then we can view the cubes. # load cube from bit bucket cube_from_s3_bucket = open_cube ( \"https://obs.eu-de.otc.t-systems.com/obs-esdc-v2.0.0/esdc-8d-0.25deg-1x720x1440-2.0.0.zarr\" ) cube_from_s3_bucket Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: bnds : 2 lat : 720 lon : 1440 time : 1702 Coordinates: (6) lat (lat) float32 89.875 89.625 ... -89.625 -89.875 array([ 89.875, 89.625, 89.375, ..., -89.375, -89.625, -89.875], dtype=float32) lat_bnds (lat, bnds) float32 dask.array<chunksize=(720, 2), meta=np.ndarray> Array Chunk Bytes 5.76 kB 5.76 kB Shape (720, 2) (720, 2) Count 2 Tasks 1 Chunks Type float32 numpy.ndarray 2 720 lon (lon) float32 -179.875 -179.625 ... 179.875 array([-179.875, -179.625, -179.375, ..., 179.375, 179.625, 179.875], dtype=float32) lon_bnds (lon, bnds) float32 dask.array<chunksize=(1440, 2), meta=np.ndarray> Array Chunk Bytes 11.52 kB 11.52 kB Shape (1440, 2) (1440, 2) Count 2 Tasks 1 Chunks Type float32 numpy.ndarray 2 1440 time (time) datetime64[ns] 1980-01-05 ... 2016-12-30 bounds : time_bnds long_name : time standard_name : time array(['1980-01-05T00:00:00.000000000', '1980-01-13T00:00:00.000000000', '1980-01-21T00:00:00.000000000', ..., '2016-12-14T00:00:00.000000000', '2016-12-22T00:00:00.000000000', '2016-12-30T00:00:00.000000000'], dtype='datetime64[ns]') time_bnds (time, bnds) datetime64[ns] dask.array<chunksize=(1702, 2), meta=np.ndarray> Array Chunk Bytes 27.23 kB 27.23 kB Shape (1702, 2) (1702, 2) Count 2 Tasks 1 Chunks Type datetime64[ns] numpy.ndarray 2 1702 Data variables: (79) Rg (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 2 esa_cci_path : nan long_name : Downwelling shortwave radiation orig_attrs : {'long_name': 'Downwelling shortwave radiation', 'project_name': 'BESS', 'references': 'Ryu, Y.*, Jiang, C., Kobayashi, H., & Detto, M. (2018). MODIS-derived global land products of shortwave radiation and diffuse and total photosynthetically active radiation at 5 km resolution from 2000. Remote Sensing of Environment, 204, 812-825', 'source_name': 'surface_downwelling_shortwave_flux_in_air', 'standard_name': 'surface_downwelling_shortwave_flux_in_air', 'units': 'W m-2', 'url': 'http://environment.snu.ac.kr/bess_rad/'} orig_version : 15.10.2017 project_name : BESS time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-03-01 units : W m-2 url : http://environment.snu.ac.kr/bess_rad/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_1600 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 25 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 1600 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 1600 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 units : 1 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_550 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 26 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 550 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 550 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 units : 1 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_670 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 27 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 670 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 670 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 units : 1 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_870 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 28 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 870 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 870 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 units : 1 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 air_temperature_2m (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 4 esa_cci_path : nan long_name : 2 Metre Air Temperature orig_attrs : {'comment': 'Air temperature at 2m from the ERA5 reanalysis product.', 'long_name': '2 metre air temperature', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'air_temperature_2m', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 units : K url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 analysed_sst (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 44 esa_cci_path : /neodc/esacci/sst/data/lt/Analysis/L4/v01.1/ long_name : Analysed Sea Surface Temperature orig_attrs : {'Conventions': 'CF-1.5, Unidata Observation Dataset v1.0', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'acknowledgment': 'Funded by ESA', 'cdm_data_type': 'grid', 'comment': 'WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value', 'creator_email': 'science.leader@esa-sst-cci.org', 'creator_name': 'ESA SST CCI', 'creator_processing_institution': 'These data were produced at the Met Office as part of the ESA SST CCI project.', 'creator_url': 'http://www.esa-sst-cci.org', 'date_created': '20130309T132046Z', 'easternmost_longitude': 180.00001525878906, 'file_quality_level': 3, 'gds_version_id': '2.0', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': 0.05000000074505806, 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': 0.05000000074505806, 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': -0.20000000298023224, 'geospatial_vertical_min': -0.20000000298023224, 'history': 'Created using OSTIA reanalysis system v2.0', 'id': 'OSTIA-ESACCI-L4-v01.1', 'institution': 'ESACCI', 'keywords': 'Oceans > Ocean Temperature > Sea Surface Temperature', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'license': 'GHRSST protocol describes data use as free and open', 'long_name': 'analysed sea surface temperature', 'metadata_link': 'http://www.esa-cci.org', 'naming_authority': 'org.ghrsst', 'netcdf_version_id': '4.1.3', 'northernmost_latitude': 90.0, 'orig_attrs': {}, 'platform': 'ERS-<1,2>, Envisat, NOAA-<12,14,15,16,17,18>, MetOpA', 'processing_level': 'L4', 'product_version': '1.1', 'project': 'Climate Change Initiative - European Space Agency', 'publisher_email': 'science.leader@esa-sst-cci.org', 'publisher_name': 'ESACCI', 'publisher_url': 'http://www.esa-sst-cci.org', 'references': 'http://www.esa-sst-cci.org', 'sensor': 'ATSR, AATSR, AVHRR_GAC', 'source': 'ATSR<1,2>-ESACCI-L3U-v1.0, AATSR-ESACCI-L3U-v1.0, AVHRR<12,14,15,16,17,18>_G-ESACCI-L2P-v1.0, AVHRRMTA-ESACCI-L2P-v1.0, EUMETSAT_OSI-SAF-ICE-v1.1, EUMETSAT_OSI-SAF-ICE-v2.2', 'source_dir': '/neodc/esacci/sst/data/lt/Analysis/L4/v01.1', 'source_name': 'analysed_sst', 'source_version': 'v01.1', 'southernmost_latitude': -90.0, 'spatial_resolution': '0.05 degree', 'standard_name': 'sea_water_temperature', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention', 'start_time': '20100101T000000Z', 'stop_time': '20100101T235959Z', 'summary': 'OSTIA L4 product from the ESA SST CCI project, produced using OSTIA reanalysis system v2.0. Ice field corrected in v1.1 (v1.0 had ice from day-1). Static ice field between 20080101-20080229 and 20080501-20080521 also fixed in v1.1', 'time_coverage_duration': 'P1D', 'time_coverage_end': '20100101T235959Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '20100101T000000Z', 'title': 'ESA SST CCI OSTIA L4 product', 'tracking_id': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'units': 'kelvin', 'url': 'http://www.esa-sst-cci.org', 'uuid': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'valid_max': 4500.0, 'valid_min': -300.0, 'westernmost_longitude': -180.0} orig_version : v01.1 project_name : ESA Sea Surface Temperature Climate Change Initiative (ESA SST CCI) time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 1991-09-02 units : kelvin url : http://www.esa-sst-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 bare_soil_evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 55 esa_cci_path : nan long_name : Bare Soil Evaporation orig_attrs : {'long_name': 'Bare Soil Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Eb', 'standard_name': 'bare_soil_water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm/day url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 black_sky_albedo (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 65 esa_cci_path : nan long_name : Black Sky Albedo for Visible Wavebands orig_attrs : {'comment': 'Black sky albedo derived from the GlobAlbedo CCI project dataset', 'long_name': 'Black Sky Albedo for Visible Wavebands', 'orig_attrs': {}, 'project_name': 'GlobAlbedo', 'references': 'Muller, Jan-Peter, et al. \"The ESA GLOBALBEDO project for mapping the Earth\u2019s land surface albedo for 15 years from European sensors.\" Geophysical Research Abstracts. Vol. 13. 2012.', 'source_name': 'DHR_VIS', 'standard_name': 'surface_albedo_black_sky', 'units': '-', 'url': 'http://www.globalbedo.org/'} orig_version : nan project_name : GlobAlbedo time_coverage_end : 2012-01-05 time_coverage_resolution : P8D time_coverage_start : 1998-01-05 units : - url : http://www.globalbedo.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 black_sky_albedo_avhrr (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 76 esa_cci_path : nan long_name : Directional Hemisphere Reflectance albedo - VIS band orig_attrs : {'comment': 'Black sky albedo derived from the QA4ECV Albedo Product', 'long_name': 'Directional Hemisphere Reflectance albedo - VIS band', 'orig_attrs': {}, 'project_name': 'QA4ECV - European Union Framework Program 7', 'source_name': 'DHR_VIS', 'standard_name': 'surface_albedo_black_sky', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV - European Union Framework Program 7 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 units : 1 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 burnt_area (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 54 esa_cci_path : nan long_name : Monthly Burnt Area orig_attrs : {'comment': 'Burnt Area based on the GFED4 fire product.', 'long_name': 'Monthly Burnt Area', 'orig_attrs': {}, 'project_name': 'GFED4', 'references': 'Giglio, Louis, James T. Randerson, and Guido R. Werf. \"Analysis of daily, monthly, and annual burned area using the fourth\u2010generation global fire emissions database (GFED4).\" Journal of Geophysical Research: Biogeosciences 118.1 (2013): 317-328.', 'source_name': 'BurntArea', 'standard_name': 'burnt_area', 'units': 'hectares', 'url': 'http://www.globalfiredata.org/'} orig_version : gfed4 project_name : GFED4 time_coverage_end : 2014-03-02 time_coverage_resolution : P8D time_coverage_start : 1995-01-05 units : hectares url : http://www.globalfiredata.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 c_emissions (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 53 esa_cci_path : nan long_name : Carbon Dioxide Emissions Due to Natural Fires orig_attrs : {'comment': 'Carbon emissions by fires based on the GFED4 fire product.', 'long_name': 'Carbon dioxide emissions due to natural fires expressed as carbon flux.', 'orig_attrs': {}, 'project_name': 'GFED4', 'references': 'Giglio, Louis, James T. Randerson, and Guido R. Werf. \"Analysis of daily, monthly, and annual burned area using the fourth\u2010generation global fire emissions database (GFED4).\" Journal of Geophysical Research: Biogeosciences 118.1 (2013): 317-328.', 'source_name': 'Emission', 'standard_name': 'surface_upward_mass_flux_of_carbon_dioxide_expressed_as_carbon_due_to_emission_from_fires', 'units': 'g C m-2 month-1', 'url': 'http://www.globalfiredata.org/'} orig_version : gfed4 project_name : GFED4 time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : g C m-2 month-1 url : http://www.globalfiredata.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cee (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 30 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Effective Emissivity at 10.8 um orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud effective emissivity at 10.8 um', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cee', 'spatial_resolution': '0.50 degree', 'standard_name': 'cee', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 1.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : 1 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cer (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 31 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Effective Radius orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud effective radius', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cer', 'spatial_resolution': '0.50 degree', 'standard_name': 'cer', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'um', 'url': 'http://www.dwd.de', 'valid_max': 200.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : um url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cfc (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 32 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud fraction orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud fraction', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cfc', 'spatial_resolution': '0.50 degree', 'standard_name': 'cfc', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 1.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : 1 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 chlor_a (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 29 esa_cci_path : /neodc/esacci/ocean_colour/data/v3.1-release/geographic/netcdf/chlor_a/daily/v3.1 long_name : Chlorophyll-a Concentration in Seawater orig_attrs : {'Conventions': 'CF-1.6', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'ancillary_variables': 'chlor_a_log10_rmsd chlor_a_log10_bias', 'cdm_data_type': 'Grid', 'comment': 'See summary attribute', 'creation_date': '20160822T065128Z', 'creator_email': 'help@esa-oceancolour-cci.org', 'creator_name': 'Plymouth Marine Laboratory', 'creator_url': 'http://esa-oceancolour-cci.org', 'date_created': '20160822T065128Z', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': '.04166666666666666666', 'geospatial_lat_units': 'decimal degrees north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': '.04166666666666666666', 'geospatial_lon_units': 'decimal degrees east', 'geospatial_vertical_max': 0.0, 'geospatial_vertical_min': 0.0, 'grid_mapping': 'crs', 'history': 'Source data were: NASA OBPG SeaWiFS level2 R2014.0 LAC and GAC [A/C via l2gen], NASA OBPG VIIRS L2 R2014.0.1 (identical to R2014.0.2) [A/C via l2gen], NASA OBPG MODIS Aqua level 1A [A/C: l2gen equivalent to R2014.0.1 + Polymer 3.5] and ESA MERIS L1B (3rd reprocessing inc OCL correction) [Polymer v3.5]; Derived products were mainly produced with functions validated from the current NASA SeaDAS release and some custom implementations. Uncertainty generation determined by the fuzzy classifier scheme of Tim Moore (2009) and Thomas Jackson et al (2017)', 'id': 'ESACCI-OC-L3S-CHLOR_A-MERGED-1D_DAILY_4km_GEO_PML_OCx-20120101-fv3.1.nc', 'institution': 'Plymouth Marine Laboratory', 'keywords': 'satellite,observation,ocean,ocean colour', 'keywords_vocabulary': 'none', 'license': 'ESA CCI Data Policy: free and open access. When referencing, please use: Ocean Colour Climate Change Initiative dataset, Version <Version Number>, European Space Agency, available online at http://www.esa-oceancolour-cci.org. We would also appreciate being notified of publications so that we can list them on the project website at http://www.esa-oceancolour-cci.org/?q=publications', 'long_name': \"Chlorophyll-a concentration in seawater (not log-transformed), generated by SeaDAS using a blended combination of OCI (OC4v6 + Hu's CI), OC3 and OC5, depending on water class memberships\", 'naming_authority': 'uk.ac.pml', 'netcdf_file_type': 'NETCDF4_CLASSIC', 'number_of_optical_water_types': '14', 'orig_attrs': {}, 'parameter_vocab_uri': 'http://vocab.nerc.ac.uk/collection/P04/current/', 'platform': 'Orbview-2,Aqua,Envisat,Suomi-NPP', 'processing_level': 'Level-3', 'product_version': '3.1', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-oceancolour-cci.org/', 'sensor': 'SeaWiFS,MODIS,MERIS,VIIRS', 'source': 'NASA SeaWiFS L2 R2014.0 LAC and GAC, MODIS-Aqua L1A, MERIS L1B 3rd reprocessing inc OCL corrections, NASA VIIRS L2 R2014.0.1 (data identical to R2014.0.2)', 'source_dir': '/neodc/esacci/ocean_colour/data/v3.1-release/geographic/netcdf/chlor_a/daily/v3.1/', 'source_name': 'chlor_a', 'source_version': 'v3.1', 'spatial_resolution': '4km nominal at equator', 'standard_name': 'mass_concentration_of_chlorophyll_a_in_sea_water', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Conventions Version 1.6', 'start_date': '01-JAN-2012 00:00:00.000000', 'stop_date': '01-JAN-2012 23:59:00.000000', 'summary': \"Data products generated by the Ocean Colour component of the European Space Agency Climate Change Initiative project. These files are daily composites of merged sensor (MERIS, MODIS Aqua, SeaWiFS LAC & GAC, VIIRS) products. MODIS Aqua and MERIS were band-shifted and bias-corrected to SeaWiFS bands and values using a temporally and spatially varying scheme based on the overlap years of 2003-2007. VIIRS was band-shifted and bias-corrected in a second stage against the MODIS Rrs that had already been corrected to SeaWiFS levels, for the overlap period 2012-2013. VIIRS and SeaWiFS Rrs were derived from standard NASA L2 products; MERIS and MODIS from a combination of NASA's l2gen (for basic sensor geometry corrections, etc) and HYGEOS Polymer v3.5 (for atmospheric correction). The Rrs were binned to a sinusoidal 4km level-3 grid, and later to 4km geographic projection, by Brockmann Consult's BEAM. Derived products were generally computed with the standard SeaDAS algorithms. QAA IOPs were derived using the standard SeaDAS algorithm but with a modified backscattering table to match that used in the bandshifting. The final chlorophyll is a combination of OC4, Hu's CI and OC5, depending on the water class memberships. Uncertainty estimates were added using the fuzzy water classifier and uncertainty estimation algorithm of Tim Moore as documented in Jackson et al (2017).\", 'time_coverage_duration': 'P1D', 'time_coverage_end': '201201012359Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '201201010000Z', 'title': 'ESA CCI Ocean Colour Product', 'tracking_id': '4e0985e0-f157-40f6-b0f1-0a2bb0261f12', 'units': 'milligram m-3', 'units_nonstandard': 'mg m^-3', 'url': 'http://esa-oceancolour-cci.org'} orig_version : v3.1 project_name : ESA CCI Ocean Colour Product time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1997-09-02 units : milligram m-3 url : http://esa-oceancolour-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cot (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 35 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Optical Thickness orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud optical thickness', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cot', 'spatial_resolution': '0.50 degree', 'standard_name': 'cot', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 320.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : 1 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 country_mask (time, lat, lon) float64 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> orig_attrs : {'ds_method': 'MODE', 'orig_attrs': {}, 'source_name': 'country_mask', 'standard_name': 'country_mask', 'units': '-'} units : - Array Chunk Bytes 14.12 GB 8.29 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float64 numpy.ndarray 1440 720 1702 cph (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 39 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Fraction of Liquid Water Clouds orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'fraction of liquid water clouds', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cph', 'spatial_resolution': '0.50 degree', 'standard_name': 'cph', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 1.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : 1 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cth (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 36 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Top Height orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud top height', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cth', 'spatial_resolution': '0.50 degree', 'standard_name': 'cth', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'km', 'url': 'http://www.dwd.de', 'valid_max': 20.0, 'valid_min': -1.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : km url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 ctp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 37 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Top Pressure orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud top pressure', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'ctp', 'spatial_resolution': '0.50 degree', 'standard_name': 'ctp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'hPa', 'url': 'http://www.dwd.de', 'valid_max': 1200.0, 'valid_min': 50.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : hPa url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 ctt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 38 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Top Temperature orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud top temperature', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'ctt', 'spatial_resolution': '0.50 degree', 'standard_name': 'ctt', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'K', 'url': 'http://www.dwd.de', 'valid_max': 320.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : K url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 56 esa_cci_path : nan long_name : Evaporation orig_attrs : {'long_name': 'Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'E', 'standard_name': 'water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm/day url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 evaporative_stress (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 57 esa_cci_path : nan long_name : Evaporative Stress Factor orig_attrs : {'long_name': 'Evaporative Stress Factor', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'S', 'standard_name': 'evaporative_stress_factor', 'units': '', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fapar_tip (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 74 esa_cci_path : nan long_name : Fraction of Absorbed PAR orig_attrs : {'long_name': 'Fraction of Absorbed Photosynthetically Active Radiation', 'orig_attrs': {}, 'project_name': 'QA4ECV', 'source_name': 'fapar', 'standard_name': 'fapar', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 units : 1 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fat_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 21 esa_cci_path : nan long_name : Tropospheric Ozone Column (Fixed Altitude) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Altitude definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'fat_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fat_p (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 17 esa_cci_path : nan long_name : Tropopause Air Pressure for the Fixed Altitude Tropopause orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropopause_air_pressure for the Fixed Altitude Tropopause', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'fat_p', 'standard_name': 'tropopause_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : hPa url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 flt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 19 esa_cci_path : nan long_name : Tropospheric Ozone Column orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Layers definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'flt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 flt_p (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 18 esa_cci_path : nan long_name : Tropopause Air Pressure for the Fixed Layer Tropopause orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropopause_air_pressure for the fixed layer tropopause', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'flt_p', 'standard_name': 'tropopause_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : hPa url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fractional_snow_cover (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 68 esa_cci_path : nan long_name : Surface Fraction Covered by Snow orig_attrs : {'comment': 'Grid cell fractional snow cover based on the Globsnow CCI product.', 'long_name': 'Surface fraction covered by snow.', 'orig_attrs': {}, 'project_name': 'GlobSnow', 'references': 'Luojus, Kari, et al. \"ESA DUE Globsnow-Global Snow Database for Climate Research.\" ESA Special Publication. Vol. 686. 2010.', 'source_name': 'MFSC', 'standard_name': 'surface_snow_area_fraction', 'units': 'percent', 'url': 'http://www.globsnow.info/'} orig_version : v2.0 project_name : GlobSnow time_coverage_end : 2013-01-05 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : percent url : http://www.globsnow.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_fat_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 22 esa_cci_path : nan long_name : Tropospheric Ozone Column (Fixed Altitude) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Altitude definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_fat_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_flt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 23 esa_cci_path : nan long_name : Tropospheric Ozone Column (Fixed Layers) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Layers definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_flt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_lrt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 20 esa_cci_path : nan long_name : Tropospheric Ozone Column ( Lapse Rate) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on lapse rate definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_lrt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_msr_flt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 8 esa_cci_path : nan long_name : Residual MSR-FLT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-FLT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_msr_flt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_msr_lrt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 10 esa_cci_path : nan long_name : Residual MSR-LRT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-LRT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_msr_lrt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : gC m-2 day-1 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 interception_loss (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 58 esa_cci_path : nan long_name : Interception Loss orig_attrs : {'long_name': 'Interception Loss', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Ei', 'standard_name': 'interception_loss', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm/day url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 iwp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 33 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Ice Water Path orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud ice water path', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'iwp', 'spatial_resolution': '0.50 degree', 'standard_name': 'iwp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'g/m2', 'url': 'http://www.dwd.de', 'valid_max': 32000.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : g/m2 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 land_surface_temperature (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 69 esa_cci_path : nan long_name : Land Surface Temperature orig_attrs : {'comment': 'Advanced Along Track Scanning Radiometer pixel land surface temperature product', 'long_name': 'Land Surface Temperature', 'orig_attrs': {}, 'project_name': 'GlobTemperature', 'references': 'Jim\u00e9nez, C., et al. \"Inversion of AMSR\u2010E observations for land surface temperature estimation: 1. Methodology and evaluation with station temperature.\" Journal of Geophysical Research: Atmospheres 122.6 (2017): 3330-3347.', 'source_name': 'LST', 'standard_name': 'surface_temperature', 'units': 'K', 'url': 'http://data.globtemperature.info/'} orig_version : nan project_name : GlobTemperature time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2002-05-21 units : K url : http://data.globtemperature.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 latent_energy (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 48 esa_cci_path : nan long_name : Latent Energy orig_attrs : {'comment': 'Latent heat flux from the surface.', 'long_name': 'Latent Energy', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'LE', 'standard_name': 'surface_upward_latent_heat_flux', 'units': 'W m-2', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : W m-2 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 leaf_area_index (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 73 esa_cci_path : nan long_name : Effective Leaf Area Index orig_attrs : {'long_name': 'Effective Leaf Area Index', 'orig_attrs': {}, 'project_name': 'QA4ECV', 'source_name': 'Lai', 'standard_name': 'leaf_area_index', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 units : 1 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 lrt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 24 esa_cci_path : nan long_name : Tropospheric Ozone Column (Lapse Rate) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on lapse rate definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'lrt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 lrt_p (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 16 esa_cci_path : nan long_name : Tropopause Air Pressure (Lapse Rate) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropopause_air_pressure for the lapse rate tropopause', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'lrt_p', 'standard_name': 'tropopause_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : hPa url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 lwp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 34 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Liquid Water Path orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud liquid water path', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'lwp', 'spatial_resolution': '0.50 degree', 'standard_name': 'lwp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'g/m2', 'url': 'http://www.dwd.de', 'valid_max': 32000.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : g/m2 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 mask (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 46 esa_cci_path : /neodc/esacci/sst/data/lt/Analysis/L4/v01.1/ long_name : Sea/Land/Lake/Ice Field Composite Mask orig_attrs : {'Conventions': 'CF-1.5, Unidata Observation Dataset v1.0', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'acknowledgment': 'Funded by ESA', 'cdm_data_type': 'grid', 'comment': 'WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value', 'creator_email': 'science.leader@esa-sst-cci.org', 'creator_name': 'ESA SST CCI', 'creator_processing_institution': 'These data were produced at the Met Office as part of the ESA SST CCI project.', 'creator_url': 'http://www.esa-sst-cci.org', 'date_created': '20130309T132046Z', 'easternmost_longitude': 180.00001525878906, 'file_quality_level': 3, 'gds_version_id': '2.0', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': 0.05000000074505806, 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': 0.05000000074505806, 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': -0.20000000298023224, 'geospatial_vertical_min': -0.20000000298023224, 'history': 'Created using OSTIA reanalysis system v2.0', 'id': 'OSTIA-ESACCI-L4-v01.1', 'institution': 'ESACCI', 'keywords': 'Oceans > Ocean Temperature > Sea Surface Temperature', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'license': 'GHRSST protocol describes data use as free and open', 'long_name': 'sea/land/lake/ice field composite mask', 'metadata_link': 'http://www.esa-cci.org', 'naming_authority': 'org.ghrsst', 'netcdf_version_id': '4.1.3', 'northernmost_latitude': 90.0, 'orig_attrs': {}, 'platform': 'ERS-<1,2>, Envisat, NOAA-<12,14,15,16,17,18>, MetOpA', 'processing_level': 'L4', 'product_version': '1.1', 'project': 'Climate Change Initiative - European Space Agency', 'publisher_email': 'science.leader@esa-sst-cci.org', 'publisher_name': 'ESACCI', 'publisher_url': 'http://www.esa-sst-cci.org', 'references': 'http://www.esa-sst-cci.org', 'sensor': 'ATSR, AATSR, AVHRR_GAC', 'source': 'ATSR<1,2>-ESACCI-L3U-v1.0, AATSR-ESACCI-L3U-v1.0, AVHRR<12,14,15,16,17,18>_G-ESACCI-L2P-v1.0, AVHRRMTA-ESACCI-L2P-v1.0, EUMETSAT_OSI-SAF-ICE-v1.1, EUMETSAT_OSI-SAF-ICE-v2.2', 'source_dir': '/neodc/esacci/sst/data/lt/Analysis/L4/v01.1', 'source_name': 'mask', 'source_version': 'v01.1', 'southernmost_latitude': -90.0, 'spatial_resolution': '0.05 degree', 'standard_name': 'mask', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention', 'start_time': '20100101T000000Z', 'stop_time': '20100101T235959Z', 'summary': 'OSTIA L4 product from the ESA SST CCI project, produced using OSTIA reanalysis system v2.0. Ice field corrected in v1.1 (v1.0 had ice from day-1). Static ice field between 20080101-20080229 and 20080501-20080521 also fixed in v1.1', 'time_coverage_duration': 'P1D', 'time_coverage_end': '20100101T235959Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '20100101T000000Z', 'title': 'ESA SST CCI OSTIA L4 product', 'tracking_id': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'url': 'http://www.esa-sst-cci.org', 'uuid': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'valid_max': 31.0, 'valid_min': 1.0, 'westernmost_longitude': -180.0} orig_version : v01.1 project_name : ESA Sea Surface Temperature Climate Change Initiative (ESA SST CCI) time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 1991-09-02 url : http://www.esa-sst-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 max_air_temperature_2m (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 6 esa_cci_path : nan long_name : Maximum 2 Metre Air Temperature orig_attrs : {'comment': 'Air temperature at 2m from the ERA5 reanalysis product.', 'long_name': 'Maximum 2 metre air temperature', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'max_air_temperature_2m', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 units : K url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 min_air_temperature_2m (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 7 esa_cci_path : nan long_name : Minimum 2 Metre Air Temperature orig_attrs : {'comment': 'Air temperature at 2m from the ERA5 reanalysis product.', 'long_name': 'Minimum 2 metre air temperature', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'min_air_temperature_2m', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 units : K url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 msr_flt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 9 esa_cci_path : nan long_name : Residual MSR-FLT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-FLT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'msr_flt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 msr_lrt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 11 esa_cci_path : nan long_name : Residual MSR-LRT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-LRT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'msr_lrt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 net_ecosystem_exchange (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 49 esa_cci_path : nan long_name : Net Ecosystem Exchange orig_attrs : {'comment': 'Net carbon exchange between the ecosystem and the atmopshere.', 'long_name': 'Net Ecosystem Exchange', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'NEE', 'standard_name': 'net_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : gC m-2 day-1 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 net_radiation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 50 esa_cci_path : nan long_name : Net Radiation orig_attrs : {'comment': 'Net radiation to the surface', 'long_name': 'Net Radiation', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'Rn', 'standard_name': 'surface_net_radiation_flux', 'units': 'W m-2', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : W m-2 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 open_water_evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 59 esa_cci_path : nan long_name : Open-Water Evaporation orig_attrs : {'long_name': 'Open-water Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Ew', 'standard_name': 'water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm/day url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 ozone (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 72 esa_cci_path : /neodc/esacci/ozone/data/total_columns/l3/merged/v0100/ long_name : Mean Total Ozone Column in dobson units orig_attrs : {'comment': 'Atmospheric ozone based on the Ozone CCI data.', 'long_name': 'Mean total ozone column in dobson units', 'orig_attrs': {}, 'project_name': 'Ozone CCI', 'references': 'Laeng, A., et al. \"The ozone climate change initiative: Comparison of four Level-2 processors for the Michelson Interferometer for Passive Atmospheric Sounding (MIPAS).\" Remote Sensing of Environment 162 (2015): 316-343.', 'source_name': 'atmosphere_mole_content_of_ozone', 'standard_name': 'atmosphere_mole_content_of_ozone', 'units': 'DU', 'url': 'http://www.esa-ozone-cci.org/'} orig_version : v0100 project_name : Ozone CCI time_coverage_end : 2011-06-30 time_coverage_resolution : P8D time_coverage_start : 1996-03-09 units : DU url : http://www.esa-ozone-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 par (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 3 esa_cci_path : nan long_name : Photosynthetically Active Radiation orig_attrs : {'long_name': 'Photosynthetically active radiation', 'orig_attrs': {}, 'project_name': 'BESS', 'references': 'Ryu, Y.*, Jiang, C., Kobayashi, H., & Detto, M. (2018). MODIS-derived global land products of shortwave radiation and diffuse and total photosynthetically active radiation at 5 km resolution from 2000. Remote Sensing of Environment, 204, 812-825', 'source_name': 'surface_downwelling_photosynthetic_radiative_flux_in_air', 'standard_name': 'surface_downwelling_photosynthetic_radiative_flux_in_air', 'units': 'W m-2', 'url': 'http://environment.snu.ac.kr/bess_rad/'} orig_version : 15.10.2017 project_name : BESS time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-03-01 units : W m-2 url : http://environment.snu.ac.kr/bess_rad/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 pardiff (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 1 esa_cci_path : nan long_name : Diffuse Photosynthetically Active Radiation orig_attrs : {'long_name': 'Diffuse Photosynthetically active radiation', 'orig_attrs': {}, 'project_name': 'BESS', 'references': 'Ryu, Y.*, Jiang, C., Kobayashi, H., & Detto, M. (2018). MODIS-derived global land products of shortwave radiation and diffuse and total photosynthetically active radiation at 5 km resolution from 2000. Remote Sensing of Environment, 204, 812-825', 'source_name': 'surface_diffuse_downwelling_photosynthetic_radiative_flux_in_air', 'standard_name': 'surface_diffuse_downwelling_photosynthetic_radiative_flux_in_air', 'units': 'W m-2', 'url': 'http://environment.snu.ac.kr/bess_rad/'} orig_version : 15.10.2017 project_name : BESS time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-03-01 units : W m-2 url : http://environment.snu.ac.kr/bess_rad/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 potential_evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 60 esa_cci_path : nan long_name : Potential Evaporation orig_attrs : {'long_name': 'Potential Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Ep', 'standard_name': 'potential_water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm/day url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 precipitation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 71 esa_cci_path : nan long_name : Precipitation orig_attrs : {'comment': 'Precipitation based on the GPCP dataset.', 'long_name': 'Precip - RealTime [RT] (see documentation for more information)', 'orig_attrs': {}, 'project_name': 'GPCP', 'references': 'Adler, Robert F., et al. \"The version-2 global precipitation climatology project (GPCP) monthly precipitation analysis (1979-present).\" Journal of hydrometeorology 4.6 (2003): 1147-1167.', 'source_name': 'Precip', 'standard_name': 'precipitation_flux', 'units': 'mm/day', 'url': 'http://precip.gsfc.nasa.gov/'} orig_version : nan project_name : GPCP time_coverage_end : 2015-01-05 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 units : mm/day url : http://precip.gsfc.nasa.gov/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 precipitation_era5 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 5 esa_cci_path : nan long_name : ERA5 Precipitation orig_attrs : {'comment': 'Total precipitation from the ERA5 reanalysis product.', 'long_name': 'ERA 5 Precipitation', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'precipitation', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 units : K url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 psurf (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 12 esa_cci_path : nan long_name : Surface Air Pressure orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'surface_air_pressure', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'psurf', 'standard_name': 'surface_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : hPa url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 root_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 61 esa_cci_path : nan long_name : Root-Zone Soil Moisture orig_attrs : {'long_name': 'Root-Zone Soil Moisture', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'SMroot', 'standard_name': 'soil_moisture_content', 'units': 'm3/m3', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : m3/m3 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 sea_ice_fraction (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 45 esa_cci_path : /neodc/esacci/sst/data/lt/Analysis/L4/v01.1/ long_name : Sea Ice Area Fraction orig_attrs : {'Conventions': 'CF-1.5, Unidata Observation Dataset v1.0', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'acknowledgment': 'Funded by ESA', 'cdm_data_type': 'grid', 'comment': 'WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value', 'creator_email': 'science.leader@esa-sst-cci.org', 'creator_name': 'ESA SST CCI', 'creator_processing_institution': 'These data were produced at the Met Office as part of the ESA SST CCI project.', 'creator_url': 'http://www.esa-sst-cci.org', 'date_created': '20130309T132046Z', 'easternmost_longitude': 180.00001525878906, 'file_quality_level': 3, 'gds_version_id': '2.0', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': 0.05000000074505806, 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': 0.05000000074505806, 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': -0.20000000298023224, 'geospatial_vertical_min': -0.20000000298023224, 'history': 'Created using OSTIA reanalysis system v2.0', 'id': 'OSTIA-ESACCI-L4-v01.1', 'institution': 'ESACCI', 'keywords': 'Oceans > Ocean Temperature > Sea Surface Temperature', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'license': 'GHRSST protocol describes data use as free and open', 'long_name': 'sea ice area fraction', 'metadata_link': 'http://www.esa-cci.org', 'naming_authority': 'org.ghrsst', 'netcdf_version_id': '4.1.3', 'northernmost_latitude': 90.0, 'platform': 'ERS-<1,2>, Envisat, NOAA-<12,14,15,16,17,18>, MetOpA', 'processing_level': 'L4', 'product_version': '1.1', 'project': 'Climate Change Initiative - European Space Agency', 'publisher_email': 'science.leader@esa-sst-cci.org', 'publisher_name': 'ESACCI', 'publisher_url': 'http://www.esa-sst-cci.org', 'references': 'http://www.esa-sst-cci.org', 'sensor': 'ATSR, AATSR, AVHRR_GAC', 'source': 'ATSR<1,2>-ESACCI-L3U-v1.0, AATSR-ESACCI-L3U-v1.0, AVHRR<12,14,15,16,17,18>_G-ESACCI-L2P-v1.0, AVHRRMTA-ESACCI-L2P-v1.0, EUMETSAT_OSI-SAF-ICE-v1.1, EUMETSAT_OSI-SAF-ICE-v2.2', 'source_dir': '/neodc/esacci/sst/data/lt/Analysis/L4/v01.1', 'source_name': 'sea_ice_fraction', 'source_version': 'v01.1', 'southernmost_latitude': -90.0, 'spatial_resolution': '0.05 degree', 'standard_name': 'sea_ice_area_fraction', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention', 'summary': 'OSTIA L4 product from the ESA SST CCI project, produced using OSTIA reanalysis system v2.0. Ice field corrected in v1.1 (v1.0 had ice from day-1). Static ice field between 20080101-20080229 and 20080501-20080521 also fixed in v1.1', 'time_coverage_duration': 'P1D', 'time_coverage_end': '20100101T235959Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '20100101T000000Z', 'title': 'ESA SST CCI OSTIA L4 product', 'tracking_id': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'units': '1', 'url': 'http://www.esa-sst-cci.org', 'uuid': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'valid_max': 100.0, 'valid_min': 0.0, 'westernmost_longitude': -180.0} orig_version : v01.1 project_name : ESA Sea Surface Temperature Climate Change Initiative (ESA SST CCI) time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 1991-09-02 units : 1 url : http://www.esa-sst-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 sensible_heat (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 51 esa_cci_path : nan long_name : Sensible Heat orig_attrs : {'comment': 'Sensible heat flux from the surface', 'long_name': 'Sensible Heat', 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'H', 'standard_name': 'surface_upward_sensible_heat_flux', 'units': 'W m-2', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : W m-2 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 snow_sublimation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 62 esa_cci_path : nan long_name : Snow Sublimation orig_attrs : {'long_name': 'Snow Sublimation', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Es', 'standard_name': 'snow_sublimation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm/day url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 snow_water_equivalent (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 67 esa_cci_path : nan long_name : Daily Snow Water Equivalent orig_attrs : {'certain_values': '-2 == mountains, -1 == water bodies, 0 == either SWE, or missing data in the southern hemisphere', 'comment': 'Grid cell fractional snow cover based on the Globsnow CCI product.', 'long_name': 'Daily Snow Water Equivalent', 'project_name': 'GlobSnow', 'references': 'Luojus, Kari, et al. \"ESA DUE Globsnow-Global Snow Database for Climate Research.\" ESA Special Publication. Vol. 686. 2010.', 'source_name': 'SWE', 'units': 'mm', 'url': 'http://www.globsnow.info/'} orig_version : v2.0 project_name : GlobSnow time_coverage_end : 2012-12-30 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 units : mm url : http://www.globsnow.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 soil_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 78 esa_cci_path : /neodc/esacci/soil_moisture/data/daily_files/COMBINED/v04.2/ long_name : Soil Moisture orig_attrs : {'comment': 'Soil moisture based on the SOilmoisture CCI project', 'long_name': 'Soil Moisture', 'project_name': 'SoilMoisture CCI', 'references': 'Liu, Y.Y., Parinussa, R.M., Dorigo, W.A., De Jeu, R.A.M., Wagner, W., McCabe, M.F., Evans, J.P., and van Dijk, A.I.J.M. (2012): Trend-preserving blending of passive and active microwave soil moisture retrievals; Liu, Y.Y., Parinussa, R.M., Dorigo, W.A., De Jeu, R.A.M., Wagner, W., van Dijk, A.I.J.M., McCabe, M.F., & Evans, J.P. (2011): Developing an improved soil moisture dataset by blending passive and active microwave satellite based retrievals. Hydrology and Earth System Sciences, 15, 425-436.', 'source_name': 'SoilMoisture', 'standard_name': 'soil_moisture_content', 'units': 'm3', 'url': 'http://www.esa-soilmoisture-cci.org'} orig_version : v04.2 project_name : SoilMoisture CCI time_coverage_end : 2014-01-29 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 units : m3 url : http://www.esa-soilmoisture-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 srex_mask (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 77 esa_cci_path : nan long_name : Mask for SREX Regions orig_attrs : {'ds_method': 'MODE', 'long_name': 'Mask for SREX regions', 'source_name': 'layer', 'standard_name': 'srex_mask', 'units': '-'} orig_version : nan project_name : regionmask - SREX Regions time_coverage_end : 1980-01-05 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 units : - url : https://regionmask.readthedocs.io/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 stemp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 40 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Surface Temperature orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'surface temperature', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'stemp', 'spatial_resolution': '0.50 degree', 'standard_name': 'stemp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'K', 'url': 'http://www.dwd.de', 'valid_max': 320.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : K url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 surface_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 63 esa_cci_path : nan long_name : Surface Soil Moisture orig_attrs : {'long_name': 'Surface Soil Moisture', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'SMsurf', 'standard_name': 'soil_moisture_content', 'units': 'mm3/mm3', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm3/mm3 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 terrestrial_ecosystem_respiration (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 52 esa_cci_path : nan long_name : Terrestrial Ecosystem Respiration orig_attrs : {'comment': 'Total carbon release of the ecosystem through respiration.', 'long_name': 'Terrestrial Ecosystem Respiration', 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'TERall', 'standard_name': 'ecosystem_respiration_carbon_flux', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2012-12-30 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : gC m-2 day-1 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 totcol_assim (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 13 esa_cci_path : nan long_name : Total Ozone Column (Assimilated TM5 data) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'total ozone column derived from assimilated TM5 data', 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'totcol_assim', 'standard_name': 'atmosphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 totcol_free (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 14 esa_cci_path : nan long_name : Total Ozone Column (Assimilated TM5 data) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'total ozone column derived from assimilated TM5 data', 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'totcol_free', 'standard_name': 'atmosphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 totcol_msr (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 15 esa_cci_path : nan long_name : Total Ozone Column (MSR data) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'total ozone column derived from MSR data', 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'totcol_msr', 'standard_name': 'atmosphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 transpiration (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 64 esa_cci_path : nan long_name : Transpiration orig_attrs : {'long_name': 'Transpiration', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Et', 'standard_name': 'transpiration_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm/day url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 water_mask (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 43 esa_cci_path : /neodc/esacci/land_cover/data/water_bodies/v4.0/ long_name : Terrestrial or Water Pixel Classification orig_attrs : {'long_name': 'Terrestrial or water pixel classification', 'project_name': 'Climate Change Initiative - European Space Agency', 'source_name': 'wb_class', 'standard_name': 'land_cover_lccs', 'units': '-', 'url': 'http://www.esa-landcover-cci.org'} orig_version : v4.0 project_name : ESA Land Cover Climate Change Initiative (Land_Cover_cci) time_coverage_end : 1980-01-05 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 units : - url : http://www.esa-landcover-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 water_vapour (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 70 esa_cci_path : nan long_name : Total Column Water Vapour orig_attrs : {'comment': 'Total column water vapour based on the GlobVapour CCI product.', 'long_name': 'Total Column Water Vapour', 'project_name': 'GlobVapour', 'references': 'Schneider, Nadine, et al. \"ESA DUE GlobVapour water vapor products: Validation.\" AIP Conference Proceedings. Vol. 1531. No. 1. 2013.', 'source_name': 'tcwv_res', 'standard_name': 'atmosphere_mass_content_of_water_vapor', 'units': 'kg m-2', 'url': 'http://www.globvapour.info/'} orig_version : nan project_name : GlobVapour time_coverage_end : 2008-12-30 time_coverage_resolution : P8D time_coverage_start : 1996-01-05 units : kg m-2 url : http://www.globvapour.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 white_sky_albedo (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 66 esa_cci_path : nan long_name : White Sky Albedo for Visible Wavebands orig_attrs : {'comment': 'White sky albedo derived from the GlobAlbedo CCI project dataset', 'long_name': 'White Sky Albedo for Visible Wavebands', 'project_name': 'GlobAlbedo', 'references': 'Muller, Jan-Peter, et al. \"The ESA GLOBALBEDO project for mapping the Earth\u2019s land surface albedo for 15 years from European sensors.\" Geophysical Research Abstracts. Vol. 13. 2012.', 'source_name': 'BHR_VIS', 'standard_name': 'surface_albedo_white_sky', 'units': '-', 'url': 'http://www.globalbedo.org/'} orig_version : nan project_name : GlobAlbedo time_coverage_end : 2012-01-05 time_coverage_resolution : P8D time_coverage_start : 1998-01-05 units : - url : http://www.globalbedo.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 white_sky_albedo_avhrr (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 75 esa_cci_path : nan long_name : Bi-Hemisphere Reflectance Albedo - VIS band orig_attrs : {'comment': 'White sky albedo derived from the QA4ECV Albedo Product', 'long_name': 'Bi-Hemisphere Reflectance albedo - VIS band', 'project_name': 'QA4ECV - European Union Framework Program 7', 'source_name': 'BHR_VIS', 'standard_name': 'surface_albedo_white_sky', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV - European Union Framework Program 7 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 units : 1 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 xch4 (time, lat, lon) float64 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 42 esa_cci_path : /neodc/esacci/ghg/data/obs4mips/crdp_3/CO2/v100/ long_name : Column Average Dry-air Mole Fraction Methane orig_attrs : {'Conventions': 'CF-1.6', 'associated_files': 'obs4mips_co2_crdp3_v100.sav', 'cell_methods': 'time: mean', 'comment': 'Satellite retrieved column-average dry-air mole fraction of atmospheric carbon dioxide (XCO2)', 'contact': 'maximilian.reuter@iup.physik.uni-bremen.de', 'creation_date': '20160303T111125Z', 'data_structure': 'grid', 'frequency': 'mon', 'institute_id': 'IUP', 'institution': 'Institute of Environmental Physics, University of Bremen', 'long_name': 'column-average dry-air mole fraction of atmospheric carbon dioxide', 'mip_specs': 'CMIP5', 'product': 'observations', 'project_id': 'obs4MIPs', 'project_name': 'Ozone CCI', 'realm': 'atmos', 'references': 'Laeng, A., et al. \"The ozone climate change initiative: Comparison of four Level-2 processors for the Michelson Interferometer for Passive Atmospheric Sounding (MIPAS).\" Remote Sensing of Environment 162 (2015): 316-343.', 'source': 'ESA GHG CCI XCO2 CRDP3', 'source_id': 'XCO2_CRDP3', 'source_name': 'xch4', 'source_type': 'satellite_retrieval', 'standard_name': 'dry_atmosphere_mole_fraction_of_carbon_dioxide', 'tracking_id': '60972082-05c2-4a04-947a-99042c642c68', 'units': '1', 'url': 'http://www.esa-ghg-cci.org/'} orig_version : v100 project_name : ESA Greenhouse Gases Climate Change Initiative (GHG_cci) time_coverage_end : 2014-12-15 time_coverage_resolution : P8D time_coverage_start : 2003-01-13 units : 1 url : http://www.esa-ghg-cci.org/ Array Chunk Bytes 14.12 GB 8.29 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float64 numpy.ndarray 1440 720 1702 xco2 (time, lat, lon) float64 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 41 esa_cci_path : /neodc/esacci/ghg/data/obs4mips/crdp_3/CO2/v100/ long_name : Column Average Dry-air Mole Fraction Carbon Dioxide orig_attrs : {'Conventions': 'CF-1.6', 'associated_files': 'obs4mips_co2_crdp3_v100.sav', 'cell_methods': 'time: mean', 'comment': 'Satellite retrieved column-average dry-air mole fraction of atmospheric carbon dioxide (XCO2)', 'contact': 'maximilian.reuter@iup.physik.uni-bremen.de', 'creation_date': '20160303T111125Z', 'data_structure': 'grid', 'frequency': 'mon', 'institute_id': 'IUP', 'institution': 'Institute of Environmental Physics, University of Bremen', 'long_name': 'column-average dry-air mole fraction of atmospheric carbon dioxide', 'mip_specs': 'CMIP5', 'product': 'observations', 'project_id': 'obs4MIPs', 'project_name': 'Ozone CCI', 'realm': 'atmos', 'references': 'Laeng, A., et al. \"The ozone climate change initiative: Comparison of four Level-2 processors for the Michelson Interferometer for Passive Atmospheric Sounding (MIPAS).\" Remote Sensing of Environment 162 (2015): 316-343.', 'source': 'ESA GHG CCI XCO2 CRDP3', 'source_id': 'XCO2_CRDP3', 'source_name': 'xco2', 'source_type': 'satellite_retrieval', 'standard_name': 'dry_atmosphere_mole_fraction_of_carbon_dioxide', 'tracking_id': '60972082-05c2-4a04-947a-99042c642c68', 'units': '1', 'url': 'http://www.esa-ghg-cci.org/'} orig_version : v100 project_name : ESA Greenhouse Gases Climate Change Initiative (GHG_cci) time_coverage_end : 2014-12-15 time_coverage_resolution : P8D time_coverage_start : 2003-01-13 units : 1 url : http://www.esa-ghg-cci.org/ Array Chunk Bytes 14.12 GB 8.29 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float64 numpy.ndarray 1440 720 1702 Attributes: (35) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 17.12.2018 geospatial_lat_max : 89.75 geospatial_lat_min : -89.75 geospatial_lon_max : 179.75 geospatial_lon_min : -179.75 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube And now we can access the variables that we require only, e.g. gross_primary_productivity and soil_moisture . variables = [ 'gross_primary_productivity' , 'soil_moisture' ] cubes = cube_from_s3_bucket [ variables ] cubes Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: lat : 720 lon : 1440 time : 1702 Coordinates: (3) lat (lat) float32 89.875 89.625 ... -89.625 -89.875 array([ 89.875, 89.625, 89.375, ..., -89.375, -89.625, -89.875], dtype=float32) time (time) datetime64[ns] 1980-01-05 ... 2016-12-30 bounds : time_bnds long_name : time standard_name : time array(['1980-01-05T00:00:00.000000000', '1980-01-13T00:00:00.000000000', '1980-01-21T00:00:00.000000000', ..., '2016-12-14T00:00:00.000000000', '2016-12-22T00:00:00.000000000', '2016-12-30T00:00:00.000000000'], dtype='datetime64[ns]') lon (lon) float32 -179.875 -179.625 ... 179.875 array([-179.875, -179.625, -179.375, ..., 179.375, 179.625, 179.875], dtype=float32) Data variables: (2) gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : gC m-2 day-1 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 soil_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 78 esa_cci_path : /neodc/esacci/soil_moisture/data/daily_files/COMBINED/v04.2/ long_name : Soil Moisture orig_attrs : {'comment': 'Soil moisture based on the SOilmoisture CCI project', 'long_name': 'Soil Moisture', 'project_name': 'SoilMoisture CCI', 'references': 'Liu, Y.Y., Parinussa, R.M., Dorigo, W.A., De Jeu, R.A.M., Wagner, W., McCabe, M.F., Evans, J.P., and van Dijk, A.I.J.M. (2012): Trend-preserving blending of passive and active microwave soil moisture retrievals; Liu, Y.Y., Parinussa, R.M., Dorigo, W.A., De Jeu, R.A.M., Wagner, W., van Dijk, A.I.J.M., McCabe, M.F., & Evans, J.P. (2011): Developing an improved soil moisture dataset by blending passive and active microwave satellite based retrievals. Hydrology and Earth System Sciences, 15, 425-436.', 'source_name': 'SoilMoisture', 'standard_name': 'soil_moisture_content', 'units': 'm3', 'url': 'http://www.esa-soilmoisture-cci.org'} orig_version : v04.2 project_name : SoilMoisture CCI time_coverage_end : 2014-01-29 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 units : m3 url : http://www.esa-soilmoisture-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 Attributes: (35) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 17.12.2018 geospatial_lat_max : 89.75 geospatial_lat_min : -89.75 geospatial_lon_max : 179.75 geospatial_lon_min : -179.75 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube Method II - Personal Server \u00b6 Like I said before, I have personally downloaded the cubes on our server so I can access them in house. First let's look at the available cubes. ! ls $ DATA_PATH Cube_2019highColombiaCube_184x120x120.zarr Cube_2019highColombiaCube_1x3360x2760.zarr esdc-8d-0.083deg-184x270x270-2.0.0.zarr esdc-8d-0.083deg-1x2160x4320-2.0.0.zarr esdc-8d-0.25deg-184x90x90-2.0.0.zarr esdc-8d-0.25deg-1x720x1440-2.0.0.zarr We're going to open the datacube with 0.25 degree resolution that is optimized for spatial computations. # get filename filename = DATA_PATH . joinpath ( \"esdc-8d-0.25deg-1x720x1440-2.0.0.zarr\" ) # open datacube datacube = xr . open_zarr ( str ( filename )) datacube Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: bnds : 2 lat : 720 lon : 1440 time : 1702 Coordinates: (6) lat (lat) float32 89.875 89.625 ... -89.625 -89.875 array([ 89.875, 89.625, 89.375, ..., -89.375, -89.625, -89.875], dtype=float32) lat_bnds (lat, bnds) float32 dask.array<chunksize=(720, 2), meta=np.ndarray> Array Chunk Bytes 5.76 kB 5.76 kB Shape (720, 2) (720, 2) Count 2 Tasks 1 Chunks Type float32 numpy.ndarray 2 720 lon (lon) float32 -179.875 -179.625 ... 179.875 array([-179.875, -179.625, -179.375, ..., 179.375, 179.625, 179.875], dtype=float32) lon_bnds (lon, bnds) float32 dask.array<chunksize=(1440, 2), meta=np.ndarray> Array Chunk Bytes 11.52 kB 11.52 kB Shape (1440, 2) (1440, 2) Count 2 Tasks 1 Chunks Type float32 numpy.ndarray 2 1440 time (time) datetime64[ns] 1980-01-05 ... 2016-12-30 bounds : time_bnds long_name : time standard_name : time array(['1980-01-05T00:00:00.000000000', '1980-01-13T00:00:00.000000000', '1980-01-21T00:00:00.000000000', ..., '2016-12-14T00:00:00.000000000', '2016-12-22T00:00:00.000000000', '2016-12-30T00:00:00.000000000'], dtype='datetime64[ns]') time_bnds (time, bnds) datetime64[ns] dask.array<chunksize=(1702, 2), meta=np.ndarray> Array Chunk Bytes 27.23 kB 27.23 kB Shape (1702, 2) (1702, 2) Count 2 Tasks 1 Chunks Type datetime64[ns] numpy.ndarray 2 1702 Data variables: (79) Rg (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 2 esa_cci_path : nan long_name : Downwelling shortwave radiation orig_attrs : {'long_name': 'Downwelling shortwave radiation', 'project_name': 'BESS', 'references': 'Ryu, Y.*, Jiang, C., Kobayashi, H., & Detto, M. (2018). MODIS-derived global land products of shortwave radiation and diffuse and total photosynthetically active radiation at 5 km resolution from 2000. Remote Sensing of Environment, 204, 812-825', 'source_name': 'surface_downwelling_shortwave_flux_in_air', 'standard_name': 'surface_downwelling_shortwave_flux_in_air', 'units': 'W m-2', 'url': 'http://environment.snu.ac.kr/bess_rad/'} orig_version : 15.10.2017 project_name : BESS time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-03-01 url : http://environment.snu.ac.kr/bess_rad/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_1600 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 25 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 1600 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 1600 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_550 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 26 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 550 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 550 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_670 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 27 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 670 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 670 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_870 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 28 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 870 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 870 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 air_temperature_2m (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 4 esa_cci_path : nan long_name : 2 Metre Air Temperature orig_attrs : {'comment': 'Air temperature at 2m from the ERA5 reanalysis product.', 'long_name': '2 metre air temperature', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'air_temperature_2m', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 analysed_sst (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 44 esa_cci_path : /neodc/esacci/sst/data/lt/Analysis/L4/v01.1/ long_name : Analysed Sea Surface Temperature orig_attrs : {'Conventions': 'CF-1.5, Unidata Observation Dataset v1.0', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'acknowledgment': 'Funded by ESA', 'cdm_data_type': 'grid', 'comment': 'WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value', 'creator_email': 'science.leader@esa-sst-cci.org', 'creator_name': 'ESA SST CCI', 'creator_processing_institution': 'These data were produced at the Met Office as part of the ESA SST CCI project.', 'creator_url': 'http://www.esa-sst-cci.org', 'date_created': '20130309T132046Z', 'easternmost_longitude': 180.00001525878906, 'file_quality_level': 3, 'gds_version_id': '2.0', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': 0.05000000074505806, 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': 0.05000000074505806, 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': -0.20000000298023224, 'geospatial_vertical_min': -0.20000000298023224, 'history': 'Created using OSTIA reanalysis system v2.0', 'id': 'OSTIA-ESACCI-L4-v01.1', 'institution': 'ESACCI', 'keywords': 'Oceans > Ocean Temperature > Sea Surface Temperature', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'license': 'GHRSST protocol describes data use as free and open', 'long_name': 'analysed sea surface temperature', 'metadata_link': 'http://www.esa-cci.org', 'naming_authority': 'org.ghrsst', 'netcdf_version_id': '4.1.3', 'northernmost_latitude': 90.0, 'orig_attrs': {}, 'platform': 'ERS-<1,2>, Envisat, NOAA-<12,14,15,16,17,18>, MetOpA', 'processing_level': 'L4', 'product_version': '1.1', 'project': 'Climate Change Initiative - European Space Agency', 'publisher_email': 'science.leader@esa-sst-cci.org', 'publisher_name': 'ESACCI', 'publisher_url': 'http://www.esa-sst-cci.org', 'references': 'http://www.esa-sst-cci.org', 'sensor': 'ATSR, AATSR, AVHRR_GAC', 'source': 'ATSR<1,2>-ESACCI-L3U-v1.0, AATSR-ESACCI-L3U-v1.0, AVHRR<12,14,15,16,17,18>_G-ESACCI-L2P-v1.0, AVHRRMTA-ESACCI-L2P-v1.0, EUMETSAT_OSI-SAF-ICE-v1.1, EUMETSAT_OSI-SAF-ICE-v2.2', 'source_dir': '/neodc/esacci/sst/data/lt/Analysis/L4/v01.1', 'source_name': 'analysed_sst', 'source_version': 'v01.1', 'southernmost_latitude': -90.0, 'spatial_resolution': '0.05 degree', 'standard_name': 'sea_water_temperature', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention', 'start_time': '20100101T000000Z', 'stop_time': '20100101T235959Z', 'summary': 'OSTIA L4 product from the ESA SST CCI project, produced using OSTIA reanalysis system v2.0. Ice field corrected in v1.1 (v1.0 had ice from day-1). Static ice field between 20080101-20080229 and 20080501-20080521 also fixed in v1.1', 'time_coverage_duration': 'P1D', 'time_coverage_end': '20100101T235959Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '20100101T000000Z', 'title': 'ESA SST CCI OSTIA L4 product', 'tracking_id': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'units': 'kelvin', 'url': 'http://www.esa-sst-cci.org', 'uuid': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'valid_max': 4500.0, 'valid_min': -300.0, 'westernmost_longitude': -180.0} orig_version : v01.1 project_name : ESA Sea Surface Temperature Climate Change Initiative (ESA SST CCI) time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 1991-09-02 url : http://www.esa-sst-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 bare_soil_evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 55 esa_cci_path : nan long_name : Bare Soil Evaporation orig_attrs : {'long_name': 'Bare Soil Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Eb', 'standard_name': 'bare_soil_water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 black_sky_albedo (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 65 esa_cci_path : nan long_name : Black Sky Albedo for Visible Wavebands orig_attrs : {'comment': 'Black sky albedo derived from the GlobAlbedo CCI project dataset', 'long_name': 'Black Sky Albedo for Visible Wavebands', 'orig_attrs': {}, 'project_name': 'GlobAlbedo', 'references': 'Muller, Jan-Peter, et al. \"The ESA GLOBALBEDO project for mapping the Earth\u2019s land surface albedo for 15 years from European sensors.\" Geophysical Research Abstracts. Vol. 13. 2012.', 'source_name': 'DHR_VIS', 'standard_name': 'surface_albedo_black_sky', 'units': '-', 'url': 'http://www.globalbedo.org/'} orig_version : nan project_name : GlobAlbedo time_coverage_end : 2012-01-05 time_coverage_resolution : P8D time_coverage_start : 1998-01-05 url : http://www.globalbedo.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 black_sky_albedo_avhrr (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 76 esa_cci_path : nan long_name : Directional Hemisphere Reflectance albedo - VIS band orig_attrs : {'comment': 'Black sky albedo derived from the QA4ECV Albedo Product', 'long_name': 'Directional Hemisphere Reflectance albedo - VIS band', 'orig_attrs': {}, 'project_name': 'QA4ECV - European Union Framework Program 7', 'source_name': 'DHR_VIS', 'standard_name': 'surface_albedo_black_sky', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV - European Union Framework Program 7 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 burnt_area (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 54 esa_cci_path : nan long_name : Monthly Burnt Area orig_attrs : {'comment': 'Burnt Area based on the GFED4 fire product.', 'long_name': 'Monthly Burnt Area', 'orig_attrs': {}, 'project_name': 'GFED4', 'references': 'Giglio, Louis, James T. Randerson, and Guido R. Werf. \"Analysis of daily, monthly, and annual burned area using the fourth\u2010generation global fire emissions database (GFED4).\" Journal of Geophysical Research: Biogeosciences 118.1 (2013): 317-328.', 'source_name': 'BurntArea', 'standard_name': 'burnt_area', 'units': 'hectares', 'url': 'http://www.globalfiredata.org/'} orig_version : gfed4 project_name : GFED4 time_coverage_end : 2014-03-02 time_coverage_resolution : P8D time_coverage_start : 1995-01-05 url : http://www.globalfiredata.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 c_emissions (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 53 esa_cci_path : nan long_name : Carbon Dioxide Emissions Due to Natural Fires orig_attrs : {'comment': 'Carbon emissions by fires based on the GFED4 fire product.', 'long_name': 'Carbon dioxide emissions due to natural fires expressed as carbon flux.', 'orig_attrs': {}, 'project_name': 'GFED4', 'references': 'Giglio, Louis, James T. Randerson, and Guido R. Werf. \"Analysis of daily, monthly, and annual burned area using the fourth\u2010generation global fire emissions database (GFED4).\" Journal of Geophysical Research: Biogeosciences 118.1 (2013): 317-328.', 'source_name': 'Emission', 'standard_name': 'surface_upward_mass_flux_of_carbon_dioxide_expressed_as_carbon_due_to_emission_from_fires', 'units': 'g C m-2 month-1', 'url': 'http://www.globalfiredata.org/'} orig_version : gfed4 project_name : GFED4 time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.globalfiredata.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cee (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 30 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Effective Emissivity at 10.8 um orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud effective emissivity at 10.8 um', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cee', 'spatial_resolution': '0.50 degree', 'standard_name': 'cee', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 1.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cer (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 31 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Effective Radius orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud effective radius', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cer', 'spatial_resolution': '0.50 degree', 'standard_name': 'cer', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'um', 'url': 'http://www.dwd.de', 'valid_max': 200.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cfc (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 32 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud fraction orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud fraction', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cfc', 'spatial_resolution': '0.50 degree', 'standard_name': 'cfc', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 1.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 chlor_a (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 29 esa_cci_path : /neodc/esacci/ocean_colour/data/v3.1-release/geographic/netcdf/chlor_a/daily/v3.1 long_name : Chlorophyll-a Concentration in Seawater orig_attrs : {'Conventions': 'CF-1.6', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'ancillary_variables': 'chlor_a_log10_rmsd chlor_a_log10_bias', 'cdm_data_type': 'Grid', 'comment': 'See summary attribute', 'creation_date': '20160822T065128Z', 'creator_email': 'help@esa-oceancolour-cci.org', 'creator_name': 'Plymouth Marine Laboratory', 'creator_url': 'http://esa-oceancolour-cci.org', 'date_created': '20160822T065128Z', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': '.04166666666666666666', 'geospatial_lat_units': 'decimal degrees north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': '.04166666666666666666', 'geospatial_lon_units': 'decimal degrees east', 'geospatial_vertical_max': 0.0, 'geospatial_vertical_min': 0.0, 'grid_mapping': 'crs', 'history': 'Source data were: NASA OBPG SeaWiFS level2 R2014.0 LAC and GAC [A/C via l2gen], NASA OBPG VIIRS L2 R2014.0.1 (identical to R2014.0.2) [A/C via l2gen], NASA OBPG MODIS Aqua level 1A [A/C: l2gen equivalent to R2014.0.1 + Polymer 3.5] and ESA MERIS L1B (3rd reprocessing inc OCL correction) [Polymer v3.5]; Derived products were mainly produced with functions validated from the current NASA SeaDAS release and some custom implementations. Uncertainty generation determined by the fuzzy classifier scheme of Tim Moore (2009) and Thomas Jackson et al (2017)', 'id': 'ESACCI-OC-L3S-CHLOR_A-MERGED-1D_DAILY_4km_GEO_PML_OCx-20120101-fv3.1.nc', 'institution': 'Plymouth Marine Laboratory', 'keywords': 'satellite,observation,ocean,ocean colour', 'keywords_vocabulary': 'none', 'license': 'ESA CCI Data Policy: free and open access. When referencing, please use: Ocean Colour Climate Change Initiative dataset, Version <Version Number>, European Space Agency, available online at http://www.esa-oceancolour-cci.org. We would also appreciate being notified of publications so that we can list them on the project website at http://www.esa-oceancolour-cci.org/?q=publications', 'long_name': \"Chlorophyll-a concentration in seawater (not log-transformed), generated by SeaDAS using a blended combination of OCI (OC4v6 + Hu's CI), OC3 and OC5, depending on water class memberships\", 'naming_authority': 'uk.ac.pml', 'netcdf_file_type': 'NETCDF4_CLASSIC', 'number_of_optical_water_types': '14', 'orig_attrs': {}, 'parameter_vocab_uri': 'http://vocab.nerc.ac.uk/collection/P04/current/', 'platform': 'Orbview-2,Aqua,Envisat,Suomi-NPP', 'processing_level': 'Level-3', 'product_version': '3.1', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-oceancolour-cci.org/', 'sensor': 'SeaWiFS,MODIS,MERIS,VIIRS', 'source': 'NASA SeaWiFS L2 R2014.0 LAC and GAC, MODIS-Aqua L1A, MERIS L1B 3rd reprocessing inc OCL corrections, NASA VIIRS L2 R2014.0.1 (data identical to R2014.0.2)', 'source_dir': '/neodc/esacci/ocean_colour/data/v3.1-release/geographic/netcdf/chlor_a/daily/v3.1/', 'source_name': 'chlor_a', 'source_version': 'v3.1', 'spatial_resolution': '4km nominal at equator', 'standard_name': 'mass_concentration_of_chlorophyll_a_in_sea_water', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Conventions Version 1.6', 'start_date': '01-JAN-2012 00:00:00.000000', 'stop_date': '01-JAN-2012 23:59:00.000000', 'summary': \"Data products generated by the Ocean Colour component of the European Space Agency Climate Change Initiative project. These files are daily composites of merged sensor (MERIS, MODIS Aqua, SeaWiFS LAC & GAC, VIIRS) products. MODIS Aqua and MERIS were band-shifted and bias-corrected to SeaWiFS bands and values using a temporally and spatially varying scheme based on the overlap years of 2003-2007. VIIRS was band-shifted and bias-corrected in a second stage against the MODIS Rrs that had already been corrected to SeaWiFS levels, for the overlap period 2012-2013. VIIRS and SeaWiFS Rrs were derived from standard NASA L2 products; MERIS and MODIS from a combination of NASA's l2gen (for basic sensor geometry corrections, etc) and HYGEOS Polymer v3.5 (for atmospheric correction). The Rrs were binned to a sinusoidal 4km level-3 grid, and later to 4km geographic projection, by Brockmann Consult's BEAM. Derived products were generally computed with the standard SeaDAS algorithms. QAA IOPs were derived using the standard SeaDAS algorithm but with a modified backscattering table to match that used in the bandshifting. The final chlorophyll is a combination of OC4, Hu's CI and OC5, depending on the water class memberships. Uncertainty estimates were added using the fuzzy water classifier and uncertainty estimation algorithm of Tim Moore as documented in Jackson et al (2017).\", 'time_coverage_duration': 'P1D', 'time_coverage_end': '201201012359Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '201201010000Z', 'title': 'ESA CCI Ocean Colour Product', 'tracking_id': '4e0985e0-f157-40f6-b0f1-0a2bb0261f12', 'units': 'milligram m-3', 'units_nonstandard': 'mg m^-3', 'url': 'http://esa-oceancolour-cci.org'} orig_version : v3.1 project_name : ESA CCI Ocean Colour Product time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1997-09-02 url : http://esa-oceancolour-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cot (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 35 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Optical Thickness orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud optical thickness', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cot', 'spatial_resolution': '0.50 degree', 'standard_name': 'cot', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 320.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 country_mask (time, lat, lon) float64 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> orig_attrs : {'ds_method': 'MODE', 'orig_attrs': {}, 'source_name': 'country_mask', 'standard_name': 'country_mask', 'units': '-'} Array Chunk Bytes 14.12 GB 8.29 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float64 numpy.ndarray 1440 720 1702 cph (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 39 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Fraction of Liquid Water Clouds orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'fraction of liquid water clouds', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cph', 'spatial_resolution': '0.50 degree', 'standard_name': 'cph', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 1.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cth (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 36 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Top Height orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud top height', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cth', 'spatial_resolution': '0.50 degree', 'standard_name': 'cth', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'km', 'url': 'http://www.dwd.de', 'valid_max': 20.0, 'valid_min': -1.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 ctp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 37 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Top Pressure orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud top pressure', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'ctp', 'spatial_resolution': '0.50 degree', 'standard_name': 'ctp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'hPa', 'url': 'http://www.dwd.de', 'valid_max': 1200.0, 'valid_min': 50.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 ctt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 38 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Top Temperature orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud top temperature', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'ctt', 'spatial_resolution': '0.50 degree', 'standard_name': 'ctt', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'K', 'url': 'http://www.dwd.de', 'valid_max': 320.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 56 esa_cci_path : nan long_name : Evaporation orig_attrs : {'long_name': 'Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'E', 'standard_name': 'water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 evaporative_stress (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 57 esa_cci_path : nan long_name : Evaporative Stress Factor orig_attrs : {'long_name': 'Evaporative Stress Factor', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'S', 'standard_name': 'evaporative_stress_factor', 'units': '', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fapar_tip (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 74 esa_cci_path : nan long_name : Fraction of Absorbed PAR orig_attrs : {'long_name': 'Fraction of Absorbed Photosynthetically Active Radiation', 'orig_attrs': {}, 'project_name': 'QA4ECV', 'source_name': 'fapar', 'standard_name': 'fapar', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fat_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 21 esa_cci_path : nan long_name : Tropospheric Ozone Column (Fixed Altitude) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Altitude definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'fat_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fat_p (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 17 esa_cci_path : nan long_name : Tropopause Air Pressure for the Fixed Altitude Tropopause orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropopause_air_pressure for the Fixed Altitude Tropopause', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'fat_p', 'standard_name': 'tropopause_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 flt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 19 esa_cci_path : nan long_name : Tropospheric Ozone Column orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Layers definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'flt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 flt_p (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 18 esa_cci_path : nan long_name : Tropopause Air Pressure for the Fixed Layer Tropopause orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropopause_air_pressure for the fixed layer tropopause', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'flt_p', 'standard_name': 'tropopause_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fractional_snow_cover (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 68 esa_cci_path : nan long_name : Surface Fraction Covered by Snow orig_attrs : {'comment': 'Grid cell fractional snow cover based on the Globsnow CCI product.', 'long_name': 'Surface fraction covered by snow.', 'orig_attrs': {}, 'project_name': 'GlobSnow', 'references': 'Luojus, Kari, et al. \"ESA DUE Globsnow-Global Snow Database for Climate Research.\" ESA Special Publication. Vol. 686. 2010.', 'source_name': 'MFSC', 'standard_name': 'surface_snow_area_fraction', 'units': 'percent', 'url': 'http://www.globsnow.info/'} orig_version : v2.0 project_name : GlobSnow time_coverage_end : 2013-01-05 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.globsnow.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_fat_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 22 esa_cci_path : nan long_name : Tropospheric Ozone Column (Fixed Altitude) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Altitude definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_fat_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_flt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 23 esa_cci_path : nan long_name : Tropospheric Ozone Column (Fixed Layers) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Layers definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_flt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_lrt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 20 esa_cci_path : nan long_name : Tropospheric Ozone Column ( Lapse Rate) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on lapse rate definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_lrt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_msr_flt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 8 esa_cci_path : nan long_name : Residual MSR-FLT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-FLT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_msr_flt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_msr_lrt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 10 esa_cci_path : nan long_name : Residual MSR-LRT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-LRT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_msr_lrt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 interception_loss (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 58 esa_cci_path : nan long_name : Interception Loss orig_attrs : {'long_name': 'Interception Loss', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Ei', 'standard_name': 'interception_loss', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 iwp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 33 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Ice Water Path orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud ice water path', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'iwp', 'spatial_resolution': '0.50 degree', 'standard_name': 'iwp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'g/m2', 'url': 'http://www.dwd.de', 'valid_max': 32000.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 land_surface_temperature (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 69 esa_cci_path : nan long_name : Land Surface Temperature orig_attrs : {'comment': 'Advanced Along Track Scanning Radiometer pixel land surface temperature product', 'long_name': 'Land Surface Temperature', 'orig_attrs': {}, 'project_name': 'GlobTemperature', 'references': 'Jim\u00e9nez, C., et al. \"Inversion of AMSR\u2010E observations for land surface temperature estimation: 1. Methodology and evaluation with station temperature.\" Journal of Geophysical Research: Atmospheres 122.6 (2017): 3330-3347.', 'source_name': 'LST', 'standard_name': 'surface_temperature', 'units': 'K', 'url': 'http://data.globtemperature.info/'} orig_version : nan project_name : GlobTemperature time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2002-05-21 url : http://data.globtemperature.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 latent_energy (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 48 esa_cci_path : nan long_name : Latent Energy orig_attrs : {'comment': 'Latent heat flux from the surface.', 'long_name': 'Latent Energy', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'LE', 'standard_name': 'surface_upward_latent_heat_flux', 'units': 'W m-2', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 leaf_area_index (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 73 esa_cci_path : nan long_name : Effective Leaf Area Index orig_attrs : {'long_name': 'Effective Leaf Area Index', 'orig_attrs': {}, 'project_name': 'QA4ECV', 'source_name': 'Lai', 'standard_name': 'leaf_area_index', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 lrt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 24 esa_cci_path : nan long_name : Tropospheric Ozone Column (Lapse Rate) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on lapse rate definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'lrt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 lrt_p (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 16 esa_cci_path : nan long_name : Tropopause Air Pressure (Lapse Rate) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropopause_air_pressure for the lapse rate tropopause', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'lrt_p', 'standard_name': 'tropopause_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 lwp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 34 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Liquid Water Path orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud liquid water path', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'lwp', 'spatial_resolution': '0.50 degree', 'standard_name': 'lwp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'g/m2', 'url': 'http://www.dwd.de', 'valid_max': 32000.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 mask (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 46 esa_cci_path : /neodc/esacci/sst/data/lt/Analysis/L4/v01.1/ long_name : Sea/Land/Lake/Ice Field Composite Mask orig_attrs : {'Conventions': 'CF-1.5, Unidata Observation Dataset v1.0', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'acknowledgment': 'Funded by ESA', 'cdm_data_type': 'grid', 'comment': 'WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value', 'creator_email': 'science.leader@esa-sst-cci.org', 'creator_name': 'ESA SST CCI', 'creator_processing_institution': 'These data were produced at the Met Office as part of the ESA SST CCI project.', 'creator_url': 'http://www.esa-sst-cci.org', 'date_created': '20130309T132046Z', 'easternmost_longitude': 180.00001525878906, 'file_quality_level': 3, 'gds_version_id': '2.0', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': 0.05000000074505806, 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': 0.05000000074505806, 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': -0.20000000298023224, 'geospatial_vertical_min': -0.20000000298023224, 'history': 'Created using OSTIA reanalysis system v2.0', 'id': 'OSTIA-ESACCI-L4-v01.1', 'institution': 'ESACCI', 'keywords': 'Oceans > Ocean Temperature > Sea Surface Temperature', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'license': 'GHRSST protocol describes data use as free and open', 'long_name': 'sea/land/lake/ice field composite mask', 'metadata_link': 'http://www.esa-cci.org', 'naming_authority': 'org.ghrsst', 'netcdf_version_id': '4.1.3', 'northernmost_latitude': 90.0, 'orig_attrs': {}, 'platform': 'ERS-<1,2>, Envisat, NOAA-<12,14,15,16,17,18>, MetOpA', 'processing_level': 'L4', 'product_version': '1.1', 'project': 'Climate Change Initiative - European Space Agency', 'publisher_email': 'science.leader@esa-sst-cci.org', 'publisher_name': 'ESACCI', 'publisher_url': 'http://www.esa-sst-cci.org', 'references': 'http://www.esa-sst-cci.org', 'sensor': 'ATSR, AATSR, AVHRR_GAC', 'source': 'ATSR<1,2>-ESACCI-L3U-v1.0, AATSR-ESACCI-L3U-v1.0, AVHRR<12,14,15,16,17,18>_G-ESACCI-L2P-v1.0, AVHRRMTA-ESACCI-L2P-v1.0, EUMETSAT_OSI-SAF-ICE-v1.1, EUMETSAT_OSI-SAF-ICE-v2.2', 'source_dir': '/neodc/esacci/sst/data/lt/Analysis/L4/v01.1', 'source_name': 'mask', 'source_version': 'v01.1', 'southernmost_latitude': -90.0, 'spatial_resolution': '0.05 degree', 'standard_name': 'mask', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention', 'start_time': '20100101T000000Z', 'stop_time': '20100101T235959Z', 'summary': 'OSTIA L4 product from the ESA SST CCI project, produced using OSTIA reanalysis system v2.0. Ice field corrected in v1.1 (v1.0 had ice from day-1). Static ice field between 20080101-20080229 and 20080501-20080521 also fixed in v1.1', 'time_coverage_duration': 'P1D', 'time_coverage_end': '20100101T235959Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '20100101T000000Z', 'title': 'ESA SST CCI OSTIA L4 product', 'tracking_id': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'url': 'http://www.esa-sst-cci.org', 'uuid': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'valid_max': 31.0, 'valid_min': 1.0, 'westernmost_longitude': -180.0} orig_version : v01.1 project_name : ESA Sea Surface Temperature Climate Change Initiative (ESA SST CCI) time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 1991-09-02 url : http://www.esa-sst-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 max_air_temperature_2m (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 6 esa_cci_path : nan long_name : Maximum 2 Metre Air Temperature orig_attrs : {'comment': 'Air temperature at 2m from the ERA5 reanalysis product.', 'long_name': 'Maximum 2 metre air temperature', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'max_air_temperature_2m', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 min_air_temperature_2m (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 7 esa_cci_path : nan long_name : Minimum 2 Metre Air Temperature orig_attrs : {'comment': 'Air temperature at 2m from the ERA5 reanalysis product.', 'long_name': 'Minimum 2 metre air temperature', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'min_air_temperature_2m', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 msr_flt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 9 esa_cci_path : nan long_name : Residual MSR-FLT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-FLT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'msr_flt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 msr_lrt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 11 esa_cci_path : nan long_name : Residual MSR-LRT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-LRT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'msr_lrt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 net_ecosystem_exchange (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 49 esa_cci_path : nan long_name : Net Ecosystem Exchange orig_attrs : {'comment': 'Net carbon exchange between the ecosystem and the atmopshere.', 'long_name': 'Net Ecosystem Exchange', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'NEE', 'standard_name': 'net_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 net_radiation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 50 esa_cci_path : nan long_name : Net Radiation orig_attrs : {'comment': 'Net radiation to the surface', 'long_name': 'Net Radiation', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'Rn', 'standard_name': 'surface_net_radiation_flux', 'units': 'W m-2', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 open_water_evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 59 esa_cci_path : nan long_name : Open-Water Evaporation orig_attrs : {'long_name': 'Open-water Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Ew', 'standard_name': 'water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 ozone (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 72 esa_cci_path : /neodc/esacci/ozone/data/total_columns/l3/merged/v0100/ long_name : Mean Total Ozone Column in dobson units orig_attrs : {'comment': 'Atmospheric ozone based on the Ozone CCI data.', 'long_name': 'Mean total ozone column in dobson units', 'orig_attrs': {}, 'project_name': 'Ozone CCI', 'references': 'Laeng, A., et al. \"The ozone climate change initiative: Comparison of four Level-2 processors for the Michelson Interferometer for Passive Atmospheric Sounding (MIPAS).\" Remote Sensing of Environment 162 (2015): 316-343.', 'source_name': 'atmosphere_mole_content_of_ozone', 'standard_name': 'atmosphere_mole_content_of_ozone', 'units': 'DU', 'url': 'http://www.esa-ozone-cci.org/'} orig_version : v0100 project_name : Ozone CCI time_coverage_end : 2011-06-30 time_coverage_resolution : P8D time_coverage_start : 1996-03-09 url : http://www.esa-ozone-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 par (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 3 esa_cci_path : nan long_name : Photosynthetically Active Radiation orig_attrs : {'long_name': 'Photosynthetically active radiation', 'orig_attrs': {}, 'project_name': 'BESS', 'references': 'Ryu, Y.*, Jiang, C., Kobayashi, H., & Detto, M. (2018). MODIS-derived global land products of shortwave radiation and diffuse and total photosynthetically active radiation at 5 km resolution from 2000. Remote Sensing of Environment, 204, 812-825', 'source_name': 'surface_downwelling_photosynthetic_radiative_flux_in_air', 'standard_name': 'surface_downwelling_photosynthetic_radiative_flux_in_air', 'units': 'W m-2', 'url': 'http://environment.snu.ac.kr/bess_rad/'} orig_version : 15.10.2017 project_name : BESS time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-03-01 url : http://environment.snu.ac.kr/bess_rad/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 pardiff (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 1 esa_cci_path : nan long_name : Diffuse Photosynthetically Active Radiation orig_attrs : {'long_name': 'Diffuse Photosynthetically active radiation', 'orig_attrs': {}, 'project_name': 'BESS', 'references': 'Ryu, Y.*, Jiang, C., Kobayashi, H., & Detto, M. (2018). MODIS-derived global land products of shortwave radiation and diffuse and total photosynthetically active radiation at 5 km resolution from 2000. Remote Sensing of Environment, 204, 812-825', 'source_name': 'surface_diffuse_downwelling_photosynthetic_radiative_flux_in_air', 'standard_name': 'surface_diffuse_downwelling_photosynthetic_radiative_flux_in_air', 'units': 'W m-2', 'url': 'http://environment.snu.ac.kr/bess_rad/'} orig_version : 15.10.2017 project_name : BESS time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-03-01 url : http://environment.snu.ac.kr/bess_rad/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 potential_evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 60 esa_cci_path : nan long_name : Potential Evaporation orig_attrs : {'long_name': 'Potential Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Ep', 'standard_name': 'potential_water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 precipitation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 71 esa_cci_path : nan long_name : Precipitation orig_attrs : {'comment': 'Precipitation based on the GPCP dataset.', 'long_name': 'Precip - RealTime [RT] (see documentation for more information)', 'orig_attrs': {}, 'project_name': 'GPCP', 'references': 'Adler, Robert F., et al. \"The version-2 global precipitation climatology project (GPCP) monthly precipitation analysis (1979-present).\" Journal of hydrometeorology 4.6 (2003): 1147-1167.', 'source_name': 'Precip', 'standard_name': 'precipitation_flux', 'units': 'mm/day', 'url': 'http://precip.gsfc.nasa.gov/'} orig_version : nan project_name : GPCP time_coverage_end : 2015-01-05 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 url : http://precip.gsfc.nasa.gov/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 precipitation_era5 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 5 esa_cci_path : nan long_name : ERA5 Precipitation orig_attrs : {'comment': 'Total precipitation from the ERA5 reanalysis product.', 'long_name': 'ERA 5 Precipitation', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'precipitation', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 psurf (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 12 esa_cci_path : nan long_name : Surface Air Pressure orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'surface_air_pressure', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'psurf', 'standard_name': 'surface_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 root_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 61 esa_cci_path : nan long_name : Root-Zone Soil Moisture orig_attrs : {'long_name': 'Root-Zone Soil Moisture', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'SMroot', 'standard_name': 'soil_moisture_content', 'units': 'm3/m3', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 sea_ice_fraction (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 45 esa_cci_path : /neodc/esacci/sst/data/lt/Analysis/L4/v01.1/ long_name : Sea Ice Area Fraction orig_attrs : {'Conventions': 'CF-1.5, Unidata Observation Dataset v1.0', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'acknowledgment': 'Funded by ESA', 'cdm_data_type': 'grid', 'comment': 'WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value', 'creator_email': 'science.leader@esa-sst-cci.org', 'creator_name': 'ESA SST CCI', 'creator_processing_institution': 'These data were produced at the Met Office as part of the ESA SST CCI project.', 'creator_url': 'http://www.esa-sst-cci.org', 'date_created': '20130309T132046Z', 'easternmost_longitude': 180.00001525878906, 'file_quality_level': 3, 'gds_version_id': '2.0', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': 0.05000000074505806, 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': 0.05000000074505806, 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': -0.20000000298023224, 'geospatial_vertical_min': -0.20000000298023224, 'history': 'Created using OSTIA reanalysis system v2.0', 'id': 'OSTIA-ESACCI-L4-v01.1', 'institution': 'ESACCI', 'keywords': 'Oceans > Ocean Temperature > Sea Surface Temperature', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'license': 'GHRSST protocol describes data use as free and open', 'long_name': 'sea ice area fraction', 'metadata_link': 'http://www.esa-cci.org', 'naming_authority': 'org.ghrsst', 'netcdf_version_id': '4.1.3', 'northernmost_latitude': 90.0, 'platform': 'ERS-<1,2>, Envisat, NOAA-<12,14,15,16,17,18>, MetOpA', 'processing_level': 'L4', 'product_version': '1.1', 'project': 'Climate Change Initiative - European Space Agency', 'publisher_email': 'science.leader@esa-sst-cci.org', 'publisher_name': 'ESACCI', 'publisher_url': 'http://www.esa-sst-cci.org', 'references': 'http://www.esa-sst-cci.org', 'sensor': 'ATSR, AATSR, AVHRR_GAC', 'source': 'ATSR<1,2>-ESACCI-L3U-v1.0, AATSR-ESACCI-L3U-v1.0, AVHRR<12,14,15,16,17,18>_G-ESACCI-L2P-v1.0, AVHRRMTA-ESACCI-L2P-v1.0, EUMETSAT_OSI-SAF-ICE-v1.1, EUMETSAT_OSI-SAF-ICE-v2.2', 'source_dir': '/neodc/esacci/sst/data/lt/Analysis/L4/v01.1', 'source_name': 'sea_ice_fraction', 'source_version': 'v01.1', 'southernmost_latitude': -90.0, 'spatial_resolution': '0.05 degree', 'standard_name': 'sea_ice_area_fraction', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention', 'summary': 'OSTIA L4 product from the ESA SST CCI project, produced using OSTIA reanalysis system v2.0. Ice field corrected in v1.1 (v1.0 had ice from day-1). Static ice field between 20080101-20080229 and 20080501-20080521 also fixed in v1.1', 'time_coverage_duration': 'P1D', 'time_coverage_end': '20100101T235959Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '20100101T000000Z', 'title': 'ESA SST CCI OSTIA L4 product', 'tracking_id': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'units': '1', 'url': 'http://www.esa-sst-cci.org', 'uuid': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'valid_max': 100.0, 'valid_min': 0.0, 'westernmost_longitude': -180.0} orig_version : v01.1 project_name : ESA Sea Surface Temperature Climate Change Initiative (ESA SST CCI) time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 1991-09-02 url : http://www.esa-sst-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 sensible_heat (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 51 esa_cci_path : nan long_name : Sensible Heat orig_attrs : {'comment': 'Sensible heat flux from the surface', 'long_name': 'Sensible Heat', 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'H', 'standard_name': 'surface_upward_sensible_heat_flux', 'units': 'W m-2', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 snow_sublimation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 62 esa_cci_path : nan long_name : Snow Sublimation orig_attrs : {'long_name': 'Snow Sublimation', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Es', 'standard_name': 'snow_sublimation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 snow_water_equivalent (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 67 esa_cci_path : nan long_name : Daily Snow Water Equivalent orig_attrs : {'certain_values': '-2 == mountains, -1 == water bodies, 0 == either SWE, or missing data in the southern hemisphere', 'comment': 'Grid cell fractional snow cover based on the Globsnow CCI product.', 'long_name': 'Daily Snow Water Equivalent', 'project_name': 'GlobSnow', 'references': 'Luojus, Kari, et al. \"ESA DUE Globsnow-Global Snow Database for Climate Research.\" ESA Special Publication. Vol. 686. 2010.', 'source_name': 'SWE', 'units': 'mm', 'url': 'http://www.globsnow.info/'} orig_version : v2.0 project_name : GlobSnow time_coverage_end : 2012-12-30 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 url : http://www.globsnow.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 soil_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 78 esa_cci_path : /neodc/esacci/soil_moisture/data/daily_files/COMBINED/v04.2/ long_name : Soil Moisture orig_attrs : {'comment': 'Soil moisture based on the SOilmoisture CCI project', 'long_name': 'Soil Moisture', 'project_name': 'SoilMoisture CCI', 'references': 'Liu, Y.Y., Parinussa, R.M., Dorigo, W.A., De Jeu, R.A.M., Wagner, W., McCabe, M.F., Evans, J.P., and van Dijk, A.I.J.M. (2012): Trend-preserving blending of passive and active microwave soil moisture retrievals; Liu, Y.Y., Parinussa, R.M., Dorigo, W.A., De Jeu, R.A.M., Wagner, W., van Dijk, A.I.J.M., McCabe, M.F., & Evans, J.P. (2011): Developing an improved soil moisture dataset by blending passive and active microwave satellite based retrievals. Hydrology and Earth System Sciences, 15, 425-436.', 'source_name': 'SoilMoisture', 'standard_name': 'soil_moisture_content', 'units': 'm3', 'url': 'http://www.esa-soilmoisture-cci.org'} orig_version : v04.2 project_name : SoilMoisture CCI time_coverage_end : 2014-01-29 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 url : http://www.esa-soilmoisture-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 srex_mask (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 77 esa_cci_path : nan long_name : Mask for SREX Regions orig_attrs : {'ds_method': 'MODE', 'long_name': 'Mask for SREX regions', 'source_name': 'layer', 'standard_name': 'srex_mask', 'units': '-'} orig_version : nan project_name : regionmask - SREX Regions time_coverage_end : 1980-01-05 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 url : https://regionmask.readthedocs.io/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 stemp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 40 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Surface Temperature orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'surface temperature', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'stemp', 'spatial_resolution': '0.50 degree', 'standard_name': 'stemp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'K', 'url': 'http://www.dwd.de', 'valid_max': 320.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 surface_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 63 esa_cci_path : nan long_name : Surface Soil Moisture orig_attrs : {'long_name': 'Surface Soil Moisture', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'SMsurf', 'standard_name': 'soil_moisture_content', 'units': 'mm3/mm3', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 terrestrial_ecosystem_respiration (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 52 esa_cci_path : nan long_name : Terrestrial Ecosystem Respiration orig_attrs : {'comment': 'Total carbon release of the ecosystem through respiration.', 'long_name': 'Terrestrial Ecosystem Respiration', 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'TERall', 'standard_name': 'ecosystem_respiration_carbon_flux', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2012-12-30 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 totcol_assim (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 13 esa_cci_path : nan long_name : Total Ozone Column (Assimilated TM5 data) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'total ozone column derived from assimilated TM5 data', 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'totcol_assim', 'standard_name': 'atmosphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 totcol_free (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 14 esa_cci_path : nan long_name : Total Ozone Column (Assimilated TM5 data) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'total ozone column derived from assimilated TM5 data', 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'totcol_free', 'standard_name': 'atmosphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 totcol_msr (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 15 esa_cci_path : nan long_name : Total Ozone Column (MSR data) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'total ozone column derived from MSR data', 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'totcol_msr', 'standard_name': 'atmosphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 transpiration (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 64 esa_cci_path : nan long_name : Transpiration orig_attrs : {'long_name': 'Transpiration', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Et', 'standard_name': 'transpiration_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 water_mask (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 43 esa_cci_path : /neodc/esacci/land_cover/data/water_bodies/v4.0/ long_name : Terrestrial or Water Pixel Classification orig_attrs : {'long_name': 'Terrestrial or water pixel classification', 'project_name': 'Climate Change Initiative - European Space Agency', 'source_name': 'wb_class', 'standard_name': 'land_cover_lccs', 'units': '-', 'url': 'http://www.esa-landcover-cci.org'} orig_version : v4.0 project_name : ESA Land Cover Climate Change Initiative (Land_Cover_cci) time_coverage_end : 1980-01-05 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 url : http://www.esa-landcover-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 water_vapour (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 70 esa_cci_path : nan long_name : Total Column Water Vapour orig_attrs : {'comment': 'Total column water vapour based on the GlobVapour CCI product.', 'long_name': 'Total Column Water Vapour', 'project_name': 'GlobVapour', 'references': 'Schneider, Nadine, et al. \"ESA DUE GlobVapour water vapor products: Validation.\" AIP Conference Proceedings. Vol. 1531. No. 1. 2013.', 'source_name': 'tcwv_res', 'standard_name': 'atmosphere_mass_content_of_water_vapor', 'units': 'kg m-2', 'url': 'http://www.globvapour.info/'} orig_version : nan project_name : GlobVapour time_coverage_end : 2008-12-30 time_coverage_resolution : P8D time_coverage_start : 1996-01-05 url : http://www.globvapour.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 white_sky_albedo (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 66 esa_cci_path : nan long_name : White Sky Albedo for Visible Wavebands orig_attrs : {'comment': 'White sky albedo derived from the GlobAlbedo CCI project dataset', 'long_name': 'White Sky Albedo for Visible Wavebands', 'project_name': 'GlobAlbedo', 'references': 'Muller, Jan-Peter, et al. \"The ESA GLOBALBEDO project for mapping the Earth\u2019s land surface albedo for 15 years from European sensors.\" Geophysical Research Abstracts. Vol. 13. 2012.', 'source_name': 'BHR_VIS', 'standard_name': 'surface_albedo_white_sky', 'units': '-', 'url': 'http://www.globalbedo.org/'} orig_version : nan project_name : GlobAlbedo time_coverage_end : 2012-01-05 time_coverage_resolution : P8D time_coverage_start : 1998-01-05 url : http://www.globalbedo.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 white_sky_albedo_avhrr (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 75 esa_cci_path : nan long_name : Bi-Hemisphere Reflectance Albedo - VIS band orig_attrs : {'comment': 'White sky albedo derived from the QA4ECV Albedo Product', 'long_name': 'Bi-Hemisphere Reflectance albedo - VIS band', 'project_name': 'QA4ECV - European Union Framework Program 7', 'source_name': 'BHR_VIS', 'standard_name': 'surface_albedo_white_sky', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV - European Union Framework Program 7 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 xch4 (time, lat, lon) float64 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 42 esa_cci_path : /neodc/esacci/ghg/data/obs4mips/crdp_3/CO2/v100/ long_name : Column Average Dry-air Mole Fraction Methane orig_attrs : {'Conventions': 'CF-1.6', 'associated_files': 'obs4mips_co2_crdp3_v100.sav', 'cell_methods': 'time: mean', 'comment': 'Satellite retrieved column-average dry-air mole fraction of atmospheric carbon dioxide (XCO2)', 'contact': 'maximilian.reuter@iup.physik.uni-bremen.de', 'creation_date': '20160303T111125Z', 'data_structure': 'grid', 'frequency': 'mon', 'institute_id': 'IUP', 'institution': 'Institute of Environmental Physics, University of Bremen', 'long_name': 'column-average dry-air mole fraction of atmospheric carbon dioxide', 'mip_specs': 'CMIP5', 'product': 'observations', 'project_id': 'obs4MIPs', 'project_name': 'Ozone CCI', 'realm': 'atmos', 'references': 'Laeng, A., et al. \"The ozone climate change initiative: Comparison of four Level-2 processors for the Michelson Interferometer for Passive Atmospheric Sounding (MIPAS).\" Remote Sensing of Environment 162 (2015): 316-343.', 'source': 'ESA GHG CCI XCO2 CRDP3', 'source_id': 'XCO2_CRDP3', 'source_name': 'xch4', 'source_type': 'satellite_retrieval', 'standard_name': 'dry_atmosphere_mole_fraction_of_carbon_dioxide', 'tracking_id': '60972082-05c2-4a04-947a-99042c642c68', 'units': '1', 'url': 'http://www.esa-ghg-cci.org/'} orig_version : v100 project_name : ESA Greenhouse Gases Climate Change Initiative (GHG_cci) time_coverage_end : 2014-12-15 time_coverage_resolution : P8D time_coverage_start : 2003-01-13 url : http://www.esa-ghg-cci.org/ Array Chunk Bytes 14.12 GB 8.29 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float64 numpy.ndarray 1440 720 1702 xco2 (time, lat, lon) float64 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 41 esa_cci_path : /neodc/esacci/ghg/data/obs4mips/crdp_3/CO2/v100/ long_name : Column Average Dry-air Mole Fraction Carbon Dioxide orig_attrs : {'Conventions': 'CF-1.6', 'associated_files': 'obs4mips_co2_crdp3_v100.sav', 'cell_methods': 'time: mean', 'comment': 'Satellite retrieved column-average dry-air mole fraction of atmospheric carbon dioxide (XCO2)', 'contact': 'maximilian.reuter@iup.physik.uni-bremen.de', 'creation_date': '20160303T111125Z', 'data_structure': 'grid', 'frequency': 'mon', 'institute_id': 'IUP', 'institution': 'Institute of Environmental Physics, University of Bremen', 'long_name': 'column-average dry-air mole fraction of atmospheric carbon dioxide', 'mip_specs': 'CMIP5', 'product': 'observations', 'project_id': 'obs4MIPs', 'project_name': 'Ozone CCI', 'realm': 'atmos', 'references': 'Laeng, A., et al. \"The ozone climate change initiative: Comparison of four Level-2 processors for the Michelson Interferometer for Passive Atmospheric Sounding (MIPAS).\" Remote Sensing of Environment 162 (2015): 316-343.', 'source': 'ESA GHG CCI XCO2 CRDP3', 'source_id': 'XCO2_CRDP3', 'source_name': 'xco2', 'source_type': 'satellite_retrieval', 'standard_name': 'dry_atmosphere_mole_fraction_of_carbon_dioxide', 'tracking_id': '60972082-05c2-4a04-947a-99042c642c68', 'units': '1', 'url': 'http://www.esa-ghg-cci.org/'} orig_version : v100 project_name : ESA Greenhouse Gases Climate Change Initiative (GHG_cci) time_coverage_end : 2014-12-15 time_coverage_resolution : P8D time_coverage_start : 2003-01-13 url : http://www.esa-ghg-cci.org/ Array Chunk Bytes 14.12 GB 8.29 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float64 numpy.ndarray 1440 720 1702 Attributes: (35) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 17.12.2018 geospatial_lat_max : 89.75 geospatial_lat_min : -89.75 geospatial_lon_max : 179.75 geospatial_lon_min : -179.75 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube # again subset only the ones we require variables = [ 'gross_primary_productivity' , 'soil_moisture' ] cubes = datacube [ variables ] cubes And we're done. That's it, there's nothing more to it.","title":"1.0 load esdc"},{"location":"notebooks/spatial_temporal/1.0_load_esdc/#part-i-loading-the-esdc","text":"In this tutorial, we will walk through how one can access the ESDC. Fortunately, we have 2 ways: locally and through the internet. I have already personally downloading the cubes to our in-house server. But for reproducibility purposes, the cubes can be accessed externally as well. So we will walk-through the two ways that this is possible. from xcube.core.dsio import open_cube import pathlib import xarray as xr DATA_PATH = pathlib . Path ( \"/media/disk/databases/ESDC/\" )","title":"Part I - Loading the ESDC"},{"location":"notebooks/spatial_temporal/1.0_load_esdc/#method-i-online","text":"For this first example, we will load the cubes from the server. Then we can view the cubes. # load cube from bit bucket cube_from_s3_bucket = open_cube ( \"https://obs.eu-de.otc.t-systems.com/obs-esdc-v2.0.0/esdc-8d-0.25deg-1x720x1440-2.0.0.zarr\" ) cube_from_s3_bucket Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: bnds : 2 lat : 720 lon : 1440 time : 1702 Coordinates: (6) lat (lat) float32 89.875 89.625 ... -89.625 -89.875 array([ 89.875, 89.625, 89.375, ..., -89.375, -89.625, -89.875], dtype=float32) lat_bnds (lat, bnds) float32 dask.array<chunksize=(720, 2), meta=np.ndarray> Array Chunk Bytes 5.76 kB 5.76 kB Shape (720, 2) (720, 2) Count 2 Tasks 1 Chunks Type float32 numpy.ndarray 2 720 lon (lon) float32 -179.875 -179.625 ... 179.875 array([-179.875, -179.625, -179.375, ..., 179.375, 179.625, 179.875], dtype=float32) lon_bnds (lon, bnds) float32 dask.array<chunksize=(1440, 2), meta=np.ndarray> Array Chunk Bytes 11.52 kB 11.52 kB Shape (1440, 2) (1440, 2) Count 2 Tasks 1 Chunks Type float32 numpy.ndarray 2 1440 time (time) datetime64[ns] 1980-01-05 ... 2016-12-30 bounds : time_bnds long_name : time standard_name : time array(['1980-01-05T00:00:00.000000000', '1980-01-13T00:00:00.000000000', '1980-01-21T00:00:00.000000000', ..., '2016-12-14T00:00:00.000000000', '2016-12-22T00:00:00.000000000', '2016-12-30T00:00:00.000000000'], dtype='datetime64[ns]') time_bnds (time, bnds) datetime64[ns] dask.array<chunksize=(1702, 2), meta=np.ndarray> Array Chunk Bytes 27.23 kB 27.23 kB Shape (1702, 2) (1702, 2) Count 2 Tasks 1 Chunks Type datetime64[ns] numpy.ndarray 2 1702 Data variables: (79) Rg (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 2 esa_cci_path : nan long_name : Downwelling shortwave radiation orig_attrs : {'long_name': 'Downwelling shortwave radiation', 'project_name': 'BESS', 'references': 'Ryu, Y.*, Jiang, C., Kobayashi, H., & Detto, M. (2018). MODIS-derived global land products of shortwave radiation and diffuse and total photosynthetically active radiation at 5 km resolution from 2000. Remote Sensing of Environment, 204, 812-825', 'source_name': 'surface_downwelling_shortwave_flux_in_air', 'standard_name': 'surface_downwelling_shortwave_flux_in_air', 'units': 'W m-2', 'url': 'http://environment.snu.ac.kr/bess_rad/'} orig_version : 15.10.2017 project_name : BESS time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-03-01 units : W m-2 url : http://environment.snu.ac.kr/bess_rad/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_1600 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 25 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 1600 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 1600 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 units : 1 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_550 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 26 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 550 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 550 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 units : 1 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_670 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 27 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 670 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 670 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 units : 1 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_870 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 28 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 870 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 870 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 units : 1 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 air_temperature_2m (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 4 esa_cci_path : nan long_name : 2 Metre Air Temperature orig_attrs : {'comment': 'Air temperature at 2m from the ERA5 reanalysis product.', 'long_name': '2 metre air temperature', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'air_temperature_2m', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 units : K url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 analysed_sst (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 44 esa_cci_path : /neodc/esacci/sst/data/lt/Analysis/L4/v01.1/ long_name : Analysed Sea Surface Temperature orig_attrs : {'Conventions': 'CF-1.5, Unidata Observation Dataset v1.0', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'acknowledgment': 'Funded by ESA', 'cdm_data_type': 'grid', 'comment': 'WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value', 'creator_email': 'science.leader@esa-sst-cci.org', 'creator_name': 'ESA SST CCI', 'creator_processing_institution': 'These data were produced at the Met Office as part of the ESA SST CCI project.', 'creator_url': 'http://www.esa-sst-cci.org', 'date_created': '20130309T132046Z', 'easternmost_longitude': 180.00001525878906, 'file_quality_level': 3, 'gds_version_id': '2.0', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': 0.05000000074505806, 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': 0.05000000074505806, 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': -0.20000000298023224, 'geospatial_vertical_min': -0.20000000298023224, 'history': 'Created using OSTIA reanalysis system v2.0', 'id': 'OSTIA-ESACCI-L4-v01.1', 'institution': 'ESACCI', 'keywords': 'Oceans > Ocean Temperature > Sea Surface Temperature', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'license': 'GHRSST protocol describes data use as free and open', 'long_name': 'analysed sea surface temperature', 'metadata_link': 'http://www.esa-cci.org', 'naming_authority': 'org.ghrsst', 'netcdf_version_id': '4.1.3', 'northernmost_latitude': 90.0, 'orig_attrs': {}, 'platform': 'ERS-<1,2>, Envisat, NOAA-<12,14,15,16,17,18>, MetOpA', 'processing_level': 'L4', 'product_version': '1.1', 'project': 'Climate Change Initiative - European Space Agency', 'publisher_email': 'science.leader@esa-sst-cci.org', 'publisher_name': 'ESACCI', 'publisher_url': 'http://www.esa-sst-cci.org', 'references': 'http://www.esa-sst-cci.org', 'sensor': 'ATSR, AATSR, AVHRR_GAC', 'source': 'ATSR<1,2>-ESACCI-L3U-v1.0, AATSR-ESACCI-L3U-v1.0, AVHRR<12,14,15,16,17,18>_G-ESACCI-L2P-v1.0, AVHRRMTA-ESACCI-L2P-v1.0, EUMETSAT_OSI-SAF-ICE-v1.1, EUMETSAT_OSI-SAF-ICE-v2.2', 'source_dir': '/neodc/esacci/sst/data/lt/Analysis/L4/v01.1', 'source_name': 'analysed_sst', 'source_version': 'v01.1', 'southernmost_latitude': -90.0, 'spatial_resolution': '0.05 degree', 'standard_name': 'sea_water_temperature', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention', 'start_time': '20100101T000000Z', 'stop_time': '20100101T235959Z', 'summary': 'OSTIA L4 product from the ESA SST CCI project, produced using OSTIA reanalysis system v2.0. Ice field corrected in v1.1 (v1.0 had ice from day-1). Static ice field between 20080101-20080229 and 20080501-20080521 also fixed in v1.1', 'time_coverage_duration': 'P1D', 'time_coverage_end': '20100101T235959Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '20100101T000000Z', 'title': 'ESA SST CCI OSTIA L4 product', 'tracking_id': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'units': 'kelvin', 'url': 'http://www.esa-sst-cci.org', 'uuid': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'valid_max': 4500.0, 'valid_min': -300.0, 'westernmost_longitude': -180.0} orig_version : v01.1 project_name : ESA Sea Surface Temperature Climate Change Initiative (ESA SST CCI) time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 1991-09-02 units : kelvin url : http://www.esa-sst-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 bare_soil_evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 55 esa_cci_path : nan long_name : Bare Soil Evaporation orig_attrs : {'long_name': 'Bare Soil Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Eb', 'standard_name': 'bare_soil_water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm/day url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 black_sky_albedo (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 65 esa_cci_path : nan long_name : Black Sky Albedo for Visible Wavebands orig_attrs : {'comment': 'Black sky albedo derived from the GlobAlbedo CCI project dataset', 'long_name': 'Black Sky Albedo for Visible Wavebands', 'orig_attrs': {}, 'project_name': 'GlobAlbedo', 'references': 'Muller, Jan-Peter, et al. \"The ESA GLOBALBEDO project for mapping the Earth\u2019s land surface albedo for 15 years from European sensors.\" Geophysical Research Abstracts. Vol. 13. 2012.', 'source_name': 'DHR_VIS', 'standard_name': 'surface_albedo_black_sky', 'units': '-', 'url': 'http://www.globalbedo.org/'} orig_version : nan project_name : GlobAlbedo time_coverage_end : 2012-01-05 time_coverage_resolution : P8D time_coverage_start : 1998-01-05 units : - url : http://www.globalbedo.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 black_sky_albedo_avhrr (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 76 esa_cci_path : nan long_name : Directional Hemisphere Reflectance albedo - VIS band orig_attrs : {'comment': 'Black sky albedo derived from the QA4ECV Albedo Product', 'long_name': 'Directional Hemisphere Reflectance albedo - VIS band', 'orig_attrs': {}, 'project_name': 'QA4ECV - European Union Framework Program 7', 'source_name': 'DHR_VIS', 'standard_name': 'surface_albedo_black_sky', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV - European Union Framework Program 7 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 units : 1 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 burnt_area (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 54 esa_cci_path : nan long_name : Monthly Burnt Area orig_attrs : {'comment': 'Burnt Area based on the GFED4 fire product.', 'long_name': 'Monthly Burnt Area', 'orig_attrs': {}, 'project_name': 'GFED4', 'references': 'Giglio, Louis, James T. Randerson, and Guido R. Werf. \"Analysis of daily, monthly, and annual burned area using the fourth\u2010generation global fire emissions database (GFED4).\" Journal of Geophysical Research: Biogeosciences 118.1 (2013): 317-328.', 'source_name': 'BurntArea', 'standard_name': 'burnt_area', 'units': 'hectares', 'url': 'http://www.globalfiredata.org/'} orig_version : gfed4 project_name : GFED4 time_coverage_end : 2014-03-02 time_coverage_resolution : P8D time_coverage_start : 1995-01-05 units : hectares url : http://www.globalfiredata.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 c_emissions (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 53 esa_cci_path : nan long_name : Carbon Dioxide Emissions Due to Natural Fires orig_attrs : {'comment': 'Carbon emissions by fires based on the GFED4 fire product.', 'long_name': 'Carbon dioxide emissions due to natural fires expressed as carbon flux.', 'orig_attrs': {}, 'project_name': 'GFED4', 'references': 'Giglio, Louis, James T. Randerson, and Guido R. Werf. \"Analysis of daily, monthly, and annual burned area using the fourth\u2010generation global fire emissions database (GFED4).\" Journal of Geophysical Research: Biogeosciences 118.1 (2013): 317-328.', 'source_name': 'Emission', 'standard_name': 'surface_upward_mass_flux_of_carbon_dioxide_expressed_as_carbon_due_to_emission_from_fires', 'units': 'g C m-2 month-1', 'url': 'http://www.globalfiredata.org/'} orig_version : gfed4 project_name : GFED4 time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : g C m-2 month-1 url : http://www.globalfiredata.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cee (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 30 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Effective Emissivity at 10.8 um orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud effective emissivity at 10.8 um', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cee', 'spatial_resolution': '0.50 degree', 'standard_name': 'cee', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 1.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : 1 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cer (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 31 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Effective Radius orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud effective radius', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cer', 'spatial_resolution': '0.50 degree', 'standard_name': 'cer', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'um', 'url': 'http://www.dwd.de', 'valid_max': 200.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : um url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cfc (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 32 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud fraction orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud fraction', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cfc', 'spatial_resolution': '0.50 degree', 'standard_name': 'cfc', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 1.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : 1 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 chlor_a (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 29 esa_cci_path : /neodc/esacci/ocean_colour/data/v3.1-release/geographic/netcdf/chlor_a/daily/v3.1 long_name : Chlorophyll-a Concentration in Seawater orig_attrs : {'Conventions': 'CF-1.6', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'ancillary_variables': 'chlor_a_log10_rmsd chlor_a_log10_bias', 'cdm_data_type': 'Grid', 'comment': 'See summary attribute', 'creation_date': '20160822T065128Z', 'creator_email': 'help@esa-oceancolour-cci.org', 'creator_name': 'Plymouth Marine Laboratory', 'creator_url': 'http://esa-oceancolour-cci.org', 'date_created': '20160822T065128Z', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': '.04166666666666666666', 'geospatial_lat_units': 'decimal degrees north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': '.04166666666666666666', 'geospatial_lon_units': 'decimal degrees east', 'geospatial_vertical_max': 0.0, 'geospatial_vertical_min': 0.0, 'grid_mapping': 'crs', 'history': 'Source data were: NASA OBPG SeaWiFS level2 R2014.0 LAC and GAC [A/C via l2gen], NASA OBPG VIIRS L2 R2014.0.1 (identical to R2014.0.2) [A/C via l2gen], NASA OBPG MODIS Aqua level 1A [A/C: l2gen equivalent to R2014.0.1 + Polymer 3.5] and ESA MERIS L1B (3rd reprocessing inc OCL correction) [Polymer v3.5]; Derived products were mainly produced with functions validated from the current NASA SeaDAS release and some custom implementations. Uncertainty generation determined by the fuzzy classifier scheme of Tim Moore (2009) and Thomas Jackson et al (2017)', 'id': 'ESACCI-OC-L3S-CHLOR_A-MERGED-1D_DAILY_4km_GEO_PML_OCx-20120101-fv3.1.nc', 'institution': 'Plymouth Marine Laboratory', 'keywords': 'satellite,observation,ocean,ocean colour', 'keywords_vocabulary': 'none', 'license': 'ESA CCI Data Policy: free and open access. When referencing, please use: Ocean Colour Climate Change Initiative dataset, Version <Version Number>, European Space Agency, available online at http://www.esa-oceancolour-cci.org. We would also appreciate being notified of publications so that we can list them on the project website at http://www.esa-oceancolour-cci.org/?q=publications', 'long_name': \"Chlorophyll-a concentration in seawater (not log-transformed), generated by SeaDAS using a blended combination of OCI (OC4v6 + Hu's CI), OC3 and OC5, depending on water class memberships\", 'naming_authority': 'uk.ac.pml', 'netcdf_file_type': 'NETCDF4_CLASSIC', 'number_of_optical_water_types': '14', 'orig_attrs': {}, 'parameter_vocab_uri': 'http://vocab.nerc.ac.uk/collection/P04/current/', 'platform': 'Orbview-2,Aqua,Envisat,Suomi-NPP', 'processing_level': 'Level-3', 'product_version': '3.1', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-oceancolour-cci.org/', 'sensor': 'SeaWiFS,MODIS,MERIS,VIIRS', 'source': 'NASA SeaWiFS L2 R2014.0 LAC and GAC, MODIS-Aqua L1A, MERIS L1B 3rd reprocessing inc OCL corrections, NASA VIIRS L2 R2014.0.1 (data identical to R2014.0.2)', 'source_dir': '/neodc/esacci/ocean_colour/data/v3.1-release/geographic/netcdf/chlor_a/daily/v3.1/', 'source_name': 'chlor_a', 'source_version': 'v3.1', 'spatial_resolution': '4km nominal at equator', 'standard_name': 'mass_concentration_of_chlorophyll_a_in_sea_water', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Conventions Version 1.6', 'start_date': '01-JAN-2012 00:00:00.000000', 'stop_date': '01-JAN-2012 23:59:00.000000', 'summary': \"Data products generated by the Ocean Colour component of the European Space Agency Climate Change Initiative project. These files are daily composites of merged sensor (MERIS, MODIS Aqua, SeaWiFS LAC & GAC, VIIRS) products. MODIS Aqua and MERIS were band-shifted and bias-corrected to SeaWiFS bands and values using a temporally and spatially varying scheme based on the overlap years of 2003-2007. VIIRS was band-shifted and bias-corrected in a second stage against the MODIS Rrs that had already been corrected to SeaWiFS levels, for the overlap period 2012-2013. VIIRS and SeaWiFS Rrs were derived from standard NASA L2 products; MERIS and MODIS from a combination of NASA's l2gen (for basic sensor geometry corrections, etc) and HYGEOS Polymer v3.5 (for atmospheric correction). The Rrs were binned to a sinusoidal 4km level-3 grid, and later to 4km geographic projection, by Brockmann Consult's BEAM. Derived products were generally computed with the standard SeaDAS algorithms. QAA IOPs were derived using the standard SeaDAS algorithm but with a modified backscattering table to match that used in the bandshifting. The final chlorophyll is a combination of OC4, Hu's CI and OC5, depending on the water class memberships. Uncertainty estimates were added using the fuzzy water classifier and uncertainty estimation algorithm of Tim Moore as documented in Jackson et al (2017).\", 'time_coverage_duration': 'P1D', 'time_coverage_end': '201201012359Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '201201010000Z', 'title': 'ESA CCI Ocean Colour Product', 'tracking_id': '4e0985e0-f157-40f6-b0f1-0a2bb0261f12', 'units': 'milligram m-3', 'units_nonstandard': 'mg m^-3', 'url': 'http://esa-oceancolour-cci.org'} orig_version : v3.1 project_name : ESA CCI Ocean Colour Product time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1997-09-02 units : milligram m-3 url : http://esa-oceancolour-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cot (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 35 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Optical Thickness orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud optical thickness', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cot', 'spatial_resolution': '0.50 degree', 'standard_name': 'cot', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 320.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : 1 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 country_mask (time, lat, lon) float64 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> orig_attrs : {'ds_method': 'MODE', 'orig_attrs': {}, 'source_name': 'country_mask', 'standard_name': 'country_mask', 'units': '-'} units : - Array Chunk Bytes 14.12 GB 8.29 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float64 numpy.ndarray 1440 720 1702 cph (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 39 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Fraction of Liquid Water Clouds orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'fraction of liquid water clouds', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cph', 'spatial_resolution': '0.50 degree', 'standard_name': 'cph', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 1.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : 1 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cth (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 36 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Top Height orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud top height', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cth', 'spatial_resolution': '0.50 degree', 'standard_name': 'cth', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'km', 'url': 'http://www.dwd.de', 'valid_max': 20.0, 'valid_min': -1.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : km url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 ctp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 37 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Top Pressure orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud top pressure', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'ctp', 'spatial_resolution': '0.50 degree', 'standard_name': 'ctp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'hPa', 'url': 'http://www.dwd.de', 'valid_max': 1200.0, 'valid_min': 50.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : hPa url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 ctt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 38 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Top Temperature orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud top temperature', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'ctt', 'spatial_resolution': '0.50 degree', 'standard_name': 'ctt', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'K', 'url': 'http://www.dwd.de', 'valid_max': 320.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : K url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 56 esa_cci_path : nan long_name : Evaporation orig_attrs : {'long_name': 'Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'E', 'standard_name': 'water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm/day url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 evaporative_stress (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 57 esa_cci_path : nan long_name : Evaporative Stress Factor orig_attrs : {'long_name': 'Evaporative Stress Factor', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'S', 'standard_name': 'evaporative_stress_factor', 'units': '', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fapar_tip (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 74 esa_cci_path : nan long_name : Fraction of Absorbed PAR orig_attrs : {'long_name': 'Fraction of Absorbed Photosynthetically Active Radiation', 'orig_attrs': {}, 'project_name': 'QA4ECV', 'source_name': 'fapar', 'standard_name': 'fapar', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 units : 1 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fat_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 21 esa_cci_path : nan long_name : Tropospheric Ozone Column (Fixed Altitude) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Altitude definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'fat_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fat_p (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 17 esa_cci_path : nan long_name : Tropopause Air Pressure for the Fixed Altitude Tropopause orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropopause_air_pressure for the Fixed Altitude Tropopause', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'fat_p', 'standard_name': 'tropopause_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : hPa url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 flt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 19 esa_cci_path : nan long_name : Tropospheric Ozone Column orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Layers definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'flt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 flt_p (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 18 esa_cci_path : nan long_name : Tropopause Air Pressure for the Fixed Layer Tropopause orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropopause_air_pressure for the fixed layer tropopause', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'flt_p', 'standard_name': 'tropopause_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : hPa url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fractional_snow_cover (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 68 esa_cci_path : nan long_name : Surface Fraction Covered by Snow orig_attrs : {'comment': 'Grid cell fractional snow cover based on the Globsnow CCI product.', 'long_name': 'Surface fraction covered by snow.', 'orig_attrs': {}, 'project_name': 'GlobSnow', 'references': 'Luojus, Kari, et al. \"ESA DUE Globsnow-Global Snow Database for Climate Research.\" ESA Special Publication. Vol. 686. 2010.', 'source_name': 'MFSC', 'standard_name': 'surface_snow_area_fraction', 'units': 'percent', 'url': 'http://www.globsnow.info/'} orig_version : v2.0 project_name : GlobSnow time_coverage_end : 2013-01-05 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : percent url : http://www.globsnow.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_fat_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 22 esa_cci_path : nan long_name : Tropospheric Ozone Column (Fixed Altitude) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Altitude definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_fat_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_flt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 23 esa_cci_path : nan long_name : Tropospheric Ozone Column (Fixed Layers) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Layers definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_flt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_lrt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 20 esa_cci_path : nan long_name : Tropospheric Ozone Column ( Lapse Rate) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on lapse rate definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_lrt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_msr_flt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 8 esa_cci_path : nan long_name : Residual MSR-FLT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-FLT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_msr_flt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_msr_lrt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 10 esa_cci_path : nan long_name : Residual MSR-LRT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-LRT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_msr_lrt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : gC m-2 day-1 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 interception_loss (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 58 esa_cci_path : nan long_name : Interception Loss orig_attrs : {'long_name': 'Interception Loss', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Ei', 'standard_name': 'interception_loss', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm/day url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 iwp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 33 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Ice Water Path orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud ice water path', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'iwp', 'spatial_resolution': '0.50 degree', 'standard_name': 'iwp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'g/m2', 'url': 'http://www.dwd.de', 'valid_max': 32000.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : g/m2 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 land_surface_temperature (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 69 esa_cci_path : nan long_name : Land Surface Temperature orig_attrs : {'comment': 'Advanced Along Track Scanning Radiometer pixel land surface temperature product', 'long_name': 'Land Surface Temperature', 'orig_attrs': {}, 'project_name': 'GlobTemperature', 'references': 'Jim\u00e9nez, C., et al. \"Inversion of AMSR\u2010E observations for land surface temperature estimation: 1. Methodology and evaluation with station temperature.\" Journal of Geophysical Research: Atmospheres 122.6 (2017): 3330-3347.', 'source_name': 'LST', 'standard_name': 'surface_temperature', 'units': 'K', 'url': 'http://data.globtemperature.info/'} orig_version : nan project_name : GlobTemperature time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2002-05-21 units : K url : http://data.globtemperature.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 latent_energy (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 48 esa_cci_path : nan long_name : Latent Energy orig_attrs : {'comment': 'Latent heat flux from the surface.', 'long_name': 'Latent Energy', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'LE', 'standard_name': 'surface_upward_latent_heat_flux', 'units': 'W m-2', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : W m-2 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 leaf_area_index (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 73 esa_cci_path : nan long_name : Effective Leaf Area Index orig_attrs : {'long_name': 'Effective Leaf Area Index', 'orig_attrs': {}, 'project_name': 'QA4ECV', 'source_name': 'Lai', 'standard_name': 'leaf_area_index', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 units : 1 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 lrt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 24 esa_cci_path : nan long_name : Tropospheric Ozone Column (Lapse Rate) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on lapse rate definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'lrt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 lrt_p (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 16 esa_cci_path : nan long_name : Tropopause Air Pressure (Lapse Rate) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropopause_air_pressure for the lapse rate tropopause', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'lrt_p', 'standard_name': 'tropopause_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : hPa url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 lwp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 34 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Liquid Water Path orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud liquid water path', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'lwp', 'spatial_resolution': '0.50 degree', 'standard_name': 'lwp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'g/m2', 'url': 'http://www.dwd.de', 'valid_max': 32000.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : g/m2 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 mask (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 46 esa_cci_path : /neodc/esacci/sst/data/lt/Analysis/L4/v01.1/ long_name : Sea/Land/Lake/Ice Field Composite Mask orig_attrs : {'Conventions': 'CF-1.5, Unidata Observation Dataset v1.0', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'acknowledgment': 'Funded by ESA', 'cdm_data_type': 'grid', 'comment': 'WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value', 'creator_email': 'science.leader@esa-sst-cci.org', 'creator_name': 'ESA SST CCI', 'creator_processing_institution': 'These data were produced at the Met Office as part of the ESA SST CCI project.', 'creator_url': 'http://www.esa-sst-cci.org', 'date_created': '20130309T132046Z', 'easternmost_longitude': 180.00001525878906, 'file_quality_level': 3, 'gds_version_id': '2.0', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': 0.05000000074505806, 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': 0.05000000074505806, 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': -0.20000000298023224, 'geospatial_vertical_min': -0.20000000298023224, 'history': 'Created using OSTIA reanalysis system v2.0', 'id': 'OSTIA-ESACCI-L4-v01.1', 'institution': 'ESACCI', 'keywords': 'Oceans > Ocean Temperature > Sea Surface Temperature', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'license': 'GHRSST protocol describes data use as free and open', 'long_name': 'sea/land/lake/ice field composite mask', 'metadata_link': 'http://www.esa-cci.org', 'naming_authority': 'org.ghrsst', 'netcdf_version_id': '4.1.3', 'northernmost_latitude': 90.0, 'orig_attrs': {}, 'platform': 'ERS-<1,2>, Envisat, NOAA-<12,14,15,16,17,18>, MetOpA', 'processing_level': 'L4', 'product_version': '1.1', 'project': 'Climate Change Initiative - European Space Agency', 'publisher_email': 'science.leader@esa-sst-cci.org', 'publisher_name': 'ESACCI', 'publisher_url': 'http://www.esa-sst-cci.org', 'references': 'http://www.esa-sst-cci.org', 'sensor': 'ATSR, AATSR, AVHRR_GAC', 'source': 'ATSR<1,2>-ESACCI-L3U-v1.0, AATSR-ESACCI-L3U-v1.0, AVHRR<12,14,15,16,17,18>_G-ESACCI-L2P-v1.0, AVHRRMTA-ESACCI-L2P-v1.0, EUMETSAT_OSI-SAF-ICE-v1.1, EUMETSAT_OSI-SAF-ICE-v2.2', 'source_dir': '/neodc/esacci/sst/data/lt/Analysis/L4/v01.1', 'source_name': 'mask', 'source_version': 'v01.1', 'southernmost_latitude': -90.0, 'spatial_resolution': '0.05 degree', 'standard_name': 'mask', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention', 'start_time': '20100101T000000Z', 'stop_time': '20100101T235959Z', 'summary': 'OSTIA L4 product from the ESA SST CCI project, produced using OSTIA reanalysis system v2.0. Ice field corrected in v1.1 (v1.0 had ice from day-1). Static ice field between 20080101-20080229 and 20080501-20080521 also fixed in v1.1', 'time_coverage_duration': 'P1D', 'time_coverage_end': '20100101T235959Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '20100101T000000Z', 'title': 'ESA SST CCI OSTIA L4 product', 'tracking_id': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'url': 'http://www.esa-sst-cci.org', 'uuid': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'valid_max': 31.0, 'valid_min': 1.0, 'westernmost_longitude': -180.0} orig_version : v01.1 project_name : ESA Sea Surface Temperature Climate Change Initiative (ESA SST CCI) time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 1991-09-02 url : http://www.esa-sst-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 max_air_temperature_2m (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 6 esa_cci_path : nan long_name : Maximum 2 Metre Air Temperature orig_attrs : {'comment': 'Air temperature at 2m from the ERA5 reanalysis product.', 'long_name': 'Maximum 2 metre air temperature', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'max_air_temperature_2m', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 units : K url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 min_air_temperature_2m (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 7 esa_cci_path : nan long_name : Minimum 2 Metre Air Temperature orig_attrs : {'comment': 'Air temperature at 2m from the ERA5 reanalysis product.', 'long_name': 'Minimum 2 metre air temperature', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'min_air_temperature_2m', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 units : K url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 msr_flt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 9 esa_cci_path : nan long_name : Residual MSR-FLT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-FLT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'msr_flt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 msr_lrt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 11 esa_cci_path : nan long_name : Residual MSR-LRT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-LRT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'msr_lrt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 net_ecosystem_exchange (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 49 esa_cci_path : nan long_name : Net Ecosystem Exchange orig_attrs : {'comment': 'Net carbon exchange between the ecosystem and the atmopshere.', 'long_name': 'Net Ecosystem Exchange', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'NEE', 'standard_name': 'net_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : gC m-2 day-1 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 net_radiation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 50 esa_cci_path : nan long_name : Net Radiation orig_attrs : {'comment': 'Net radiation to the surface', 'long_name': 'Net Radiation', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'Rn', 'standard_name': 'surface_net_radiation_flux', 'units': 'W m-2', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : W m-2 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 open_water_evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 59 esa_cci_path : nan long_name : Open-Water Evaporation orig_attrs : {'long_name': 'Open-water Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Ew', 'standard_name': 'water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm/day url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 ozone (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 72 esa_cci_path : /neodc/esacci/ozone/data/total_columns/l3/merged/v0100/ long_name : Mean Total Ozone Column in dobson units orig_attrs : {'comment': 'Atmospheric ozone based on the Ozone CCI data.', 'long_name': 'Mean total ozone column in dobson units', 'orig_attrs': {}, 'project_name': 'Ozone CCI', 'references': 'Laeng, A., et al. \"The ozone climate change initiative: Comparison of four Level-2 processors for the Michelson Interferometer for Passive Atmospheric Sounding (MIPAS).\" Remote Sensing of Environment 162 (2015): 316-343.', 'source_name': 'atmosphere_mole_content_of_ozone', 'standard_name': 'atmosphere_mole_content_of_ozone', 'units': 'DU', 'url': 'http://www.esa-ozone-cci.org/'} orig_version : v0100 project_name : Ozone CCI time_coverage_end : 2011-06-30 time_coverage_resolution : P8D time_coverage_start : 1996-03-09 units : DU url : http://www.esa-ozone-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 par (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 3 esa_cci_path : nan long_name : Photosynthetically Active Radiation orig_attrs : {'long_name': 'Photosynthetically active radiation', 'orig_attrs': {}, 'project_name': 'BESS', 'references': 'Ryu, Y.*, Jiang, C., Kobayashi, H., & Detto, M. (2018). MODIS-derived global land products of shortwave radiation and diffuse and total photosynthetically active radiation at 5 km resolution from 2000. Remote Sensing of Environment, 204, 812-825', 'source_name': 'surface_downwelling_photosynthetic_radiative_flux_in_air', 'standard_name': 'surface_downwelling_photosynthetic_radiative_flux_in_air', 'units': 'W m-2', 'url': 'http://environment.snu.ac.kr/bess_rad/'} orig_version : 15.10.2017 project_name : BESS time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-03-01 units : W m-2 url : http://environment.snu.ac.kr/bess_rad/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 pardiff (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 1 esa_cci_path : nan long_name : Diffuse Photosynthetically Active Radiation orig_attrs : {'long_name': 'Diffuse Photosynthetically active radiation', 'orig_attrs': {}, 'project_name': 'BESS', 'references': 'Ryu, Y.*, Jiang, C., Kobayashi, H., & Detto, M. (2018). MODIS-derived global land products of shortwave radiation and diffuse and total photosynthetically active radiation at 5 km resolution from 2000. Remote Sensing of Environment, 204, 812-825', 'source_name': 'surface_diffuse_downwelling_photosynthetic_radiative_flux_in_air', 'standard_name': 'surface_diffuse_downwelling_photosynthetic_radiative_flux_in_air', 'units': 'W m-2', 'url': 'http://environment.snu.ac.kr/bess_rad/'} orig_version : 15.10.2017 project_name : BESS time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-03-01 units : W m-2 url : http://environment.snu.ac.kr/bess_rad/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 potential_evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 60 esa_cci_path : nan long_name : Potential Evaporation orig_attrs : {'long_name': 'Potential Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Ep', 'standard_name': 'potential_water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm/day url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 precipitation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 71 esa_cci_path : nan long_name : Precipitation orig_attrs : {'comment': 'Precipitation based on the GPCP dataset.', 'long_name': 'Precip - RealTime [RT] (see documentation for more information)', 'orig_attrs': {}, 'project_name': 'GPCP', 'references': 'Adler, Robert F., et al. \"The version-2 global precipitation climatology project (GPCP) monthly precipitation analysis (1979-present).\" Journal of hydrometeorology 4.6 (2003): 1147-1167.', 'source_name': 'Precip', 'standard_name': 'precipitation_flux', 'units': 'mm/day', 'url': 'http://precip.gsfc.nasa.gov/'} orig_version : nan project_name : GPCP time_coverage_end : 2015-01-05 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 units : mm/day url : http://precip.gsfc.nasa.gov/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 precipitation_era5 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 5 esa_cci_path : nan long_name : ERA5 Precipitation orig_attrs : {'comment': 'Total precipitation from the ERA5 reanalysis product.', 'long_name': 'ERA 5 Precipitation', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'precipitation', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 units : K url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 psurf (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 12 esa_cci_path : nan long_name : Surface Air Pressure orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'surface_air_pressure', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'psurf', 'standard_name': 'surface_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : hPa url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 root_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 61 esa_cci_path : nan long_name : Root-Zone Soil Moisture orig_attrs : {'long_name': 'Root-Zone Soil Moisture', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'SMroot', 'standard_name': 'soil_moisture_content', 'units': 'm3/m3', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : m3/m3 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 sea_ice_fraction (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 45 esa_cci_path : /neodc/esacci/sst/data/lt/Analysis/L4/v01.1/ long_name : Sea Ice Area Fraction orig_attrs : {'Conventions': 'CF-1.5, Unidata Observation Dataset v1.0', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'acknowledgment': 'Funded by ESA', 'cdm_data_type': 'grid', 'comment': 'WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value', 'creator_email': 'science.leader@esa-sst-cci.org', 'creator_name': 'ESA SST CCI', 'creator_processing_institution': 'These data were produced at the Met Office as part of the ESA SST CCI project.', 'creator_url': 'http://www.esa-sst-cci.org', 'date_created': '20130309T132046Z', 'easternmost_longitude': 180.00001525878906, 'file_quality_level': 3, 'gds_version_id': '2.0', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': 0.05000000074505806, 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': 0.05000000074505806, 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': -0.20000000298023224, 'geospatial_vertical_min': -0.20000000298023224, 'history': 'Created using OSTIA reanalysis system v2.0', 'id': 'OSTIA-ESACCI-L4-v01.1', 'institution': 'ESACCI', 'keywords': 'Oceans > Ocean Temperature > Sea Surface Temperature', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'license': 'GHRSST protocol describes data use as free and open', 'long_name': 'sea ice area fraction', 'metadata_link': 'http://www.esa-cci.org', 'naming_authority': 'org.ghrsst', 'netcdf_version_id': '4.1.3', 'northernmost_latitude': 90.0, 'platform': 'ERS-<1,2>, Envisat, NOAA-<12,14,15,16,17,18>, MetOpA', 'processing_level': 'L4', 'product_version': '1.1', 'project': 'Climate Change Initiative - European Space Agency', 'publisher_email': 'science.leader@esa-sst-cci.org', 'publisher_name': 'ESACCI', 'publisher_url': 'http://www.esa-sst-cci.org', 'references': 'http://www.esa-sst-cci.org', 'sensor': 'ATSR, AATSR, AVHRR_GAC', 'source': 'ATSR<1,2>-ESACCI-L3U-v1.0, AATSR-ESACCI-L3U-v1.0, AVHRR<12,14,15,16,17,18>_G-ESACCI-L2P-v1.0, AVHRRMTA-ESACCI-L2P-v1.0, EUMETSAT_OSI-SAF-ICE-v1.1, EUMETSAT_OSI-SAF-ICE-v2.2', 'source_dir': '/neodc/esacci/sst/data/lt/Analysis/L4/v01.1', 'source_name': 'sea_ice_fraction', 'source_version': 'v01.1', 'southernmost_latitude': -90.0, 'spatial_resolution': '0.05 degree', 'standard_name': 'sea_ice_area_fraction', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention', 'summary': 'OSTIA L4 product from the ESA SST CCI project, produced using OSTIA reanalysis system v2.0. Ice field corrected in v1.1 (v1.0 had ice from day-1). Static ice field between 20080101-20080229 and 20080501-20080521 also fixed in v1.1', 'time_coverage_duration': 'P1D', 'time_coverage_end': '20100101T235959Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '20100101T000000Z', 'title': 'ESA SST CCI OSTIA L4 product', 'tracking_id': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'units': '1', 'url': 'http://www.esa-sst-cci.org', 'uuid': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'valid_max': 100.0, 'valid_min': 0.0, 'westernmost_longitude': -180.0} orig_version : v01.1 project_name : ESA Sea Surface Temperature Climate Change Initiative (ESA SST CCI) time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 1991-09-02 units : 1 url : http://www.esa-sst-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 sensible_heat (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 51 esa_cci_path : nan long_name : Sensible Heat orig_attrs : {'comment': 'Sensible heat flux from the surface', 'long_name': 'Sensible Heat', 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'H', 'standard_name': 'surface_upward_sensible_heat_flux', 'units': 'W m-2', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : W m-2 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 snow_sublimation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 62 esa_cci_path : nan long_name : Snow Sublimation orig_attrs : {'long_name': 'Snow Sublimation', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Es', 'standard_name': 'snow_sublimation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm/day url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 snow_water_equivalent (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 67 esa_cci_path : nan long_name : Daily Snow Water Equivalent orig_attrs : {'certain_values': '-2 == mountains, -1 == water bodies, 0 == either SWE, or missing data in the southern hemisphere', 'comment': 'Grid cell fractional snow cover based on the Globsnow CCI product.', 'long_name': 'Daily Snow Water Equivalent', 'project_name': 'GlobSnow', 'references': 'Luojus, Kari, et al. \"ESA DUE Globsnow-Global Snow Database for Climate Research.\" ESA Special Publication. Vol. 686. 2010.', 'source_name': 'SWE', 'units': 'mm', 'url': 'http://www.globsnow.info/'} orig_version : v2.0 project_name : GlobSnow time_coverage_end : 2012-12-30 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 units : mm url : http://www.globsnow.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 soil_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 78 esa_cci_path : /neodc/esacci/soil_moisture/data/daily_files/COMBINED/v04.2/ long_name : Soil Moisture orig_attrs : {'comment': 'Soil moisture based on the SOilmoisture CCI project', 'long_name': 'Soil Moisture', 'project_name': 'SoilMoisture CCI', 'references': 'Liu, Y.Y., Parinussa, R.M., Dorigo, W.A., De Jeu, R.A.M., Wagner, W., McCabe, M.F., Evans, J.P., and van Dijk, A.I.J.M. (2012): Trend-preserving blending of passive and active microwave soil moisture retrievals; Liu, Y.Y., Parinussa, R.M., Dorigo, W.A., De Jeu, R.A.M., Wagner, W., van Dijk, A.I.J.M., McCabe, M.F., & Evans, J.P. (2011): Developing an improved soil moisture dataset by blending passive and active microwave satellite based retrievals. Hydrology and Earth System Sciences, 15, 425-436.', 'source_name': 'SoilMoisture', 'standard_name': 'soil_moisture_content', 'units': 'm3', 'url': 'http://www.esa-soilmoisture-cci.org'} orig_version : v04.2 project_name : SoilMoisture CCI time_coverage_end : 2014-01-29 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 units : m3 url : http://www.esa-soilmoisture-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 srex_mask (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 77 esa_cci_path : nan long_name : Mask for SREX Regions orig_attrs : {'ds_method': 'MODE', 'long_name': 'Mask for SREX regions', 'source_name': 'layer', 'standard_name': 'srex_mask', 'units': '-'} orig_version : nan project_name : regionmask - SREX Regions time_coverage_end : 1980-01-05 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 units : - url : https://regionmask.readthedocs.io/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 stemp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 40 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Surface Temperature orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'surface temperature', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'stemp', 'spatial_resolution': '0.50 degree', 'standard_name': 'stemp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'K', 'url': 'http://www.dwd.de', 'valid_max': 320.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 units : K url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 surface_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 63 esa_cci_path : nan long_name : Surface Soil Moisture orig_attrs : {'long_name': 'Surface Soil Moisture', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'SMsurf', 'standard_name': 'soil_moisture_content', 'units': 'mm3/mm3', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm3/mm3 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 terrestrial_ecosystem_respiration (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 52 esa_cci_path : nan long_name : Terrestrial Ecosystem Respiration orig_attrs : {'comment': 'Total carbon release of the ecosystem through respiration.', 'long_name': 'Terrestrial Ecosystem Respiration', 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'TERall', 'standard_name': 'ecosystem_respiration_carbon_flux', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2012-12-30 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : gC m-2 day-1 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 totcol_assim (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 13 esa_cci_path : nan long_name : Total Ozone Column (Assimilated TM5 data) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'total ozone column derived from assimilated TM5 data', 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'totcol_assim', 'standard_name': 'atmosphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 totcol_free (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 14 esa_cci_path : nan long_name : Total Ozone Column (Assimilated TM5 data) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'total ozone column derived from assimilated TM5 data', 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'totcol_free', 'standard_name': 'atmosphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 totcol_msr (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 15 esa_cci_path : nan long_name : Total Ozone Column (MSR data) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'total ozone column derived from MSR data', 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'totcol_msr', 'standard_name': 'atmosphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 units : mol m-2 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 transpiration (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 64 esa_cci_path : nan long_name : Transpiration orig_attrs : {'long_name': 'Transpiration', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Et', 'standard_name': 'transpiration_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 units : mm/day url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 water_mask (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 43 esa_cci_path : /neodc/esacci/land_cover/data/water_bodies/v4.0/ long_name : Terrestrial or Water Pixel Classification orig_attrs : {'long_name': 'Terrestrial or water pixel classification', 'project_name': 'Climate Change Initiative - European Space Agency', 'source_name': 'wb_class', 'standard_name': 'land_cover_lccs', 'units': '-', 'url': 'http://www.esa-landcover-cci.org'} orig_version : v4.0 project_name : ESA Land Cover Climate Change Initiative (Land_Cover_cci) time_coverage_end : 1980-01-05 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 units : - url : http://www.esa-landcover-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 water_vapour (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 70 esa_cci_path : nan long_name : Total Column Water Vapour orig_attrs : {'comment': 'Total column water vapour based on the GlobVapour CCI product.', 'long_name': 'Total Column Water Vapour', 'project_name': 'GlobVapour', 'references': 'Schneider, Nadine, et al. \"ESA DUE GlobVapour water vapor products: Validation.\" AIP Conference Proceedings. Vol. 1531. No. 1. 2013.', 'source_name': 'tcwv_res', 'standard_name': 'atmosphere_mass_content_of_water_vapor', 'units': 'kg m-2', 'url': 'http://www.globvapour.info/'} orig_version : nan project_name : GlobVapour time_coverage_end : 2008-12-30 time_coverage_resolution : P8D time_coverage_start : 1996-01-05 units : kg m-2 url : http://www.globvapour.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 white_sky_albedo (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 66 esa_cci_path : nan long_name : White Sky Albedo for Visible Wavebands orig_attrs : {'comment': 'White sky albedo derived from the GlobAlbedo CCI project dataset', 'long_name': 'White Sky Albedo for Visible Wavebands', 'project_name': 'GlobAlbedo', 'references': 'Muller, Jan-Peter, et al. \"The ESA GLOBALBEDO project for mapping the Earth\u2019s land surface albedo for 15 years from European sensors.\" Geophysical Research Abstracts. Vol. 13. 2012.', 'source_name': 'BHR_VIS', 'standard_name': 'surface_albedo_white_sky', 'units': '-', 'url': 'http://www.globalbedo.org/'} orig_version : nan project_name : GlobAlbedo time_coverage_end : 2012-01-05 time_coverage_resolution : P8D time_coverage_start : 1998-01-05 units : - url : http://www.globalbedo.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 white_sky_albedo_avhrr (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 75 esa_cci_path : nan long_name : Bi-Hemisphere Reflectance Albedo - VIS band orig_attrs : {'comment': 'White sky albedo derived from the QA4ECV Albedo Product', 'long_name': 'Bi-Hemisphere Reflectance albedo - VIS band', 'project_name': 'QA4ECV - European Union Framework Program 7', 'source_name': 'BHR_VIS', 'standard_name': 'surface_albedo_white_sky', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV - European Union Framework Program 7 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 units : 1 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 xch4 (time, lat, lon) float64 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 42 esa_cci_path : /neodc/esacci/ghg/data/obs4mips/crdp_3/CO2/v100/ long_name : Column Average Dry-air Mole Fraction Methane orig_attrs : {'Conventions': 'CF-1.6', 'associated_files': 'obs4mips_co2_crdp3_v100.sav', 'cell_methods': 'time: mean', 'comment': 'Satellite retrieved column-average dry-air mole fraction of atmospheric carbon dioxide (XCO2)', 'contact': 'maximilian.reuter@iup.physik.uni-bremen.de', 'creation_date': '20160303T111125Z', 'data_structure': 'grid', 'frequency': 'mon', 'institute_id': 'IUP', 'institution': 'Institute of Environmental Physics, University of Bremen', 'long_name': 'column-average dry-air mole fraction of atmospheric carbon dioxide', 'mip_specs': 'CMIP5', 'product': 'observations', 'project_id': 'obs4MIPs', 'project_name': 'Ozone CCI', 'realm': 'atmos', 'references': 'Laeng, A., et al. \"The ozone climate change initiative: Comparison of four Level-2 processors for the Michelson Interferometer for Passive Atmospheric Sounding (MIPAS).\" Remote Sensing of Environment 162 (2015): 316-343.', 'source': 'ESA GHG CCI XCO2 CRDP3', 'source_id': 'XCO2_CRDP3', 'source_name': 'xch4', 'source_type': 'satellite_retrieval', 'standard_name': 'dry_atmosphere_mole_fraction_of_carbon_dioxide', 'tracking_id': '60972082-05c2-4a04-947a-99042c642c68', 'units': '1', 'url': 'http://www.esa-ghg-cci.org/'} orig_version : v100 project_name : ESA Greenhouse Gases Climate Change Initiative (GHG_cci) time_coverage_end : 2014-12-15 time_coverage_resolution : P8D time_coverage_start : 2003-01-13 units : 1 url : http://www.esa-ghg-cci.org/ Array Chunk Bytes 14.12 GB 8.29 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float64 numpy.ndarray 1440 720 1702 xco2 (time, lat, lon) float64 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 41 esa_cci_path : /neodc/esacci/ghg/data/obs4mips/crdp_3/CO2/v100/ long_name : Column Average Dry-air Mole Fraction Carbon Dioxide orig_attrs : {'Conventions': 'CF-1.6', 'associated_files': 'obs4mips_co2_crdp3_v100.sav', 'cell_methods': 'time: mean', 'comment': 'Satellite retrieved column-average dry-air mole fraction of atmospheric carbon dioxide (XCO2)', 'contact': 'maximilian.reuter@iup.physik.uni-bremen.de', 'creation_date': '20160303T111125Z', 'data_structure': 'grid', 'frequency': 'mon', 'institute_id': 'IUP', 'institution': 'Institute of Environmental Physics, University of Bremen', 'long_name': 'column-average dry-air mole fraction of atmospheric carbon dioxide', 'mip_specs': 'CMIP5', 'product': 'observations', 'project_id': 'obs4MIPs', 'project_name': 'Ozone CCI', 'realm': 'atmos', 'references': 'Laeng, A., et al. \"The ozone climate change initiative: Comparison of four Level-2 processors for the Michelson Interferometer for Passive Atmospheric Sounding (MIPAS).\" Remote Sensing of Environment 162 (2015): 316-343.', 'source': 'ESA GHG CCI XCO2 CRDP3', 'source_id': 'XCO2_CRDP3', 'source_name': 'xco2', 'source_type': 'satellite_retrieval', 'standard_name': 'dry_atmosphere_mole_fraction_of_carbon_dioxide', 'tracking_id': '60972082-05c2-4a04-947a-99042c642c68', 'units': '1', 'url': 'http://www.esa-ghg-cci.org/'} orig_version : v100 project_name : ESA Greenhouse Gases Climate Change Initiative (GHG_cci) time_coverage_end : 2014-12-15 time_coverage_resolution : P8D time_coverage_start : 2003-01-13 units : 1 url : http://www.esa-ghg-cci.org/ Array Chunk Bytes 14.12 GB 8.29 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float64 numpy.ndarray 1440 720 1702 Attributes: (35) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 17.12.2018 geospatial_lat_max : 89.75 geospatial_lat_min : -89.75 geospatial_lon_max : 179.75 geospatial_lon_min : -179.75 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube And now we can access the variables that we require only, e.g. gross_primary_productivity and soil_moisture . variables = [ 'gross_primary_productivity' , 'soil_moisture' ] cubes = cube_from_s3_bucket [ variables ] cubes Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: lat : 720 lon : 1440 time : 1702 Coordinates: (3) lat (lat) float32 89.875 89.625 ... -89.625 -89.875 array([ 89.875, 89.625, 89.375, ..., -89.375, -89.625, -89.875], dtype=float32) time (time) datetime64[ns] 1980-01-05 ... 2016-12-30 bounds : time_bnds long_name : time standard_name : time array(['1980-01-05T00:00:00.000000000', '1980-01-13T00:00:00.000000000', '1980-01-21T00:00:00.000000000', ..., '2016-12-14T00:00:00.000000000', '2016-12-22T00:00:00.000000000', '2016-12-30T00:00:00.000000000'], dtype='datetime64[ns]') lon (lon) float32 -179.875 -179.625 ... 179.875 array([-179.875, -179.625, -179.375, ..., 179.375, 179.625, 179.875], dtype=float32) Data variables: (2) gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 units : gC m-2 day-1 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 soil_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 78 esa_cci_path : /neodc/esacci/soil_moisture/data/daily_files/COMBINED/v04.2/ long_name : Soil Moisture orig_attrs : {'comment': 'Soil moisture based on the SOilmoisture CCI project', 'long_name': 'Soil Moisture', 'project_name': 'SoilMoisture CCI', 'references': 'Liu, Y.Y., Parinussa, R.M., Dorigo, W.A., De Jeu, R.A.M., Wagner, W., McCabe, M.F., Evans, J.P., and van Dijk, A.I.J.M. (2012): Trend-preserving blending of passive and active microwave soil moisture retrievals; Liu, Y.Y., Parinussa, R.M., Dorigo, W.A., De Jeu, R.A.M., Wagner, W., van Dijk, A.I.J.M., McCabe, M.F., & Evans, J.P. (2011): Developing an improved soil moisture dataset by blending passive and active microwave satellite based retrievals. Hydrology and Earth System Sciences, 15, 425-436.', 'source_name': 'SoilMoisture', 'standard_name': 'soil_moisture_content', 'units': 'm3', 'url': 'http://www.esa-soilmoisture-cci.org'} orig_version : v04.2 project_name : SoilMoisture CCI time_coverage_end : 2014-01-29 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 units : m3 url : http://www.esa-soilmoisture-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 Attributes: (35) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 17.12.2018 geospatial_lat_max : 89.75 geospatial_lat_min : -89.75 geospatial_lon_max : 179.75 geospatial_lon_min : -179.75 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube","title":"Method I - Online"},{"location":"notebooks/spatial_temporal/1.0_load_esdc/#method-ii-personal-server","text":"Like I said before, I have personally downloaded the cubes on our server so I can access them in house. First let's look at the available cubes. ! ls $ DATA_PATH Cube_2019highColombiaCube_184x120x120.zarr Cube_2019highColombiaCube_1x3360x2760.zarr esdc-8d-0.083deg-184x270x270-2.0.0.zarr esdc-8d-0.083deg-1x2160x4320-2.0.0.zarr esdc-8d-0.25deg-184x90x90-2.0.0.zarr esdc-8d-0.25deg-1x720x1440-2.0.0.zarr We're going to open the datacube with 0.25 degree resolution that is optimized for spatial computations. # get filename filename = DATA_PATH . joinpath ( \"esdc-8d-0.25deg-1x720x1440-2.0.0.zarr\" ) # open datacube datacube = xr . open_zarr ( str ( filename )) datacube Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: bnds : 2 lat : 720 lon : 1440 time : 1702 Coordinates: (6) lat (lat) float32 89.875 89.625 ... -89.625 -89.875 array([ 89.875, 89.625, 89.375, ..., -89.375, -89.625, -89.875], dtype=float32) lat_bnds (lat, bnds) float32 dask.array<chunksize=(720, 2), meta=np.ndarray> Array Chunk Bytes 5.76 kB 5.76 kB Shape (720, 2) (720, 2) Count 2 Tasks 1 Chunks Type float32 numpy.ndarray 2 720 lon (lon) float32 -179.875 -179.625 ... 179.875 array([-179.875, -179.625, -179.375, ..., 179.375, 179.625, 179.875], dtype=float32) lon_bnds (lon, bnds) float32 dask.array<chunksize=(1440, 2), meta=np.ndarray> Array Chunk Bytes 11.52 kB 11.52 kB Shape (1440, 2) (1440, 2) Count 2 Tasks 1 Chunks Type float32 numpy.ndarray 2 1440 time (time) datetime64[ns] 1980-01-05 ... 2016-12-30 bounds : time_bnds long_name : time standard_name : time array(['1980-01-05T00:00:00.000000000', '1980-01-13T00:00:00.000000000', '1980-01-21T00:00:00.000000000', ..., '2016-12-14T00:00:00.000000000', '2016-12-22T00:00:00.000000000', '2016-12-30T00:00:00.000000000'], dtype='datetime64[ns]') time_bnds (time, bnds) datetime64[ns] dask.array<chunksize=(1702, 2), meta=np.ndarray> Array Chunk Bytes 27.23 kB 27.23 kB Shape (1702, 2) (1702, 2) Count 2 Tasks 1 Chunks Type datetime64[ns] numpy.ndarray 2 1702 Data variables: (79) Rg (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 2 esa_cci_path : nan long_name : Downwelling shortwave radiation orig_attrs : {'long_name': 'Downwelling shortwave radiation', 'project_name': 'BESS', 'references': 'Ryu, Y.*, Jiang, C., Kobayashi, H., & Detto, M. (2018). MODIS-derived global land products of shortwave radiation and diffuse and total photosynthetically active radiation at 5 km resolution from 2000. Remote Sensing of Environment, 204, 812-825', 'source_name': 'surface_downwelling_shortwave_flux_in_air', 'standard_name': 'surface_downwelling_shortwave_flux_in_air', 'units': 'W m-2', 'url': 'http://environment.snu.ac.kr/bess_rad/'} orig_version : 15.10.2017 project_name : BESS time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-03-01 url : http://environment.snu.ac.kr/bess_rad/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_1600 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 25 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 1600 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 1600 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_550 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 26 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 550 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 550 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_670 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 27 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 670 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 670 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 aerosol_optical_thickness_870 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 28 esa_cci_path : /neodc/esacci/aerosol/data/AATSR_SU/L3/v4.21/DAILY/ long_name : Aerosol optical thickness at 870 nm orig_attrs : {'Conventions': 'CF-1.6', 'cdm_data_type': 'grid', 'coordinates': 'latitude longitude', 'creator_email': 'p.r.j.north@swansea.ac.uk, a.heckel@swansea.ac.uk', 'creator_name': 'Swansea University', 'creator_url': 'http:\\\\/\\\\/www.swan.ac.uk\\\\/staff\\\\/academic\\\\/environmentsociety\\\\/geography\\\\/northpeter\\\\/', 'date_created': '20151022T231808Z', 'geospatial_lat_max': '90.0', 'geospatial_lat_min': '-90.0', 'geospatial_lat_resolution': '1.0', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': '180.0', 'geospatial_lon_min': '-180.0', 'geospatial_lon_resolution': '1.0', 'geospatial_lon_units': 'degrees_east', 'history': 'Level 3 product from Swansea algorithm', 'id': '20020724141127-ESACCI-L3C_AEROSOL-AER_PRODUCTS-AATSR_ENVISAT-SU_DAILY-v4.21.nc', 'inputfilelist': 'ATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1 \\nATS_TOA_1PUUPA20020724_141127_000065272008_00024_02082_5805.N1', 'keywords': 'satellite,observation,atmosphere', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'lat': 180, 'license': 'ESA CCI Data Policy: free and open access', 'lon': 360, 'long_name': 'aerosol optical thickness at 870 nm', 'naming_authority': 'uk.ac.su.aatsraerosol', 'orig_attrs': {}, 'platform': 'ENVISAT', 'product_version': '4.21', 'project': 'Climate Change Initiative - European Space Agency', 'projection': 'equirectangular', 'references': 'http:\\\\/\\\\/www.esa-aerosol-cci.org', 'resolution': '1x1 degrees', 'sensor': 'AATSR', 'source': 'ATS_TOA_1P, V6.05', 'source_name': 'AAOD550_mean', 'standard_name': 'atmosphere_optical_thickness_due_to_ambient_aerosol', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains the level-3 daily mean aerosol properties products from AATSR satellite observations. Data are processed by Swansea algorithm', 'time': '1', 'time_coverage_end': '20020724T233825Z', 'time_coverage_start': '20020724T143513Z', 'title': 'AARDVARC CCI aerosol product level 3', 'tracking_id': 'a63f9cd2-1fed-4f9a-82fd-91f1c1b966b2', 'units': '1'} orig_version : v4.21 project_name : ESA Aerosol CCI time_coverage_end : 2012-04-10 time_coverage_resolution : P8D time_coverage_start : 2002-07-24 url : http://www.esa-aerosol-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 air_temperature_2m (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 4 esa_cci_path : nan long_name : 2 Metre Air Temperature orig_attrs : {'comment': 'Air temperature at 2m from the ERA5 reanalysis product.', 'long_name': '2 metre air temperature', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'air_temperature_2m', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 analysed_sst (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 44 esa_cci_path : /neodc/esacci/sst/data/lt/Analysis/L4/v01.1/ long_name : Analysed Sea Surface Temperature orig_attrs : {'Conventions': 'CF-1.5, Unidata Observation Dataset v1.0', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'acknowledgment': 'Funded by ESA', 'cdm_data_type': 'grid', 'comment': 'WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value', 'creator_email': 'science.leader@esa-sst-cci.org', 'creator_name': 'ESA SST CCI', 'creator_processing_institution': 'These data were produced at the Met Office as part of the ESA SST CCI project.', 'creator_url': 'http://www.esa-sst-cci.org', 'date_created': '20130309T132046Z', 'easternmost_longitude': 180.00001525878906, 'file_quality_level': 3, 'gds_version_id': '2.0', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': 0.05000000074505806, 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': 0.05000000074505806, 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': -0.20000000298023224, 'geospatial_vertical_min': -0.20000000298023224, 'history': 'Created using OSTIA reanalysis system v2.0', 'id': 'OSTIA-ESACCI-L4-v01.1', 'institution': 'ESACCI', 'keywords': 'Oceans > Ocean Temperature > Sea Surface Temperature', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'license': 'GHRSST protocol describes data use as free and open', 'long_name': 'analysed sea surface temperature', 'metadata_link': 'http://www.esa-cci.org', 'naming_authority': 'org.ghrsst', 'netcdf_version_id': '4.1.3', 'northernmost_latitude': 90.0, 'orig_attrs': {}, 'platform': 'ERS-<1,2>, Envisat, NOAA-<12,14,15,16,17,18>, MetOpA', 'processing_level': 'L4', 'product_version': '1.1', 'project': 'Climate Change Initiative - European Space Agency', 'publisher_email': 'science.leader@esa-sst-cci.org', 'publisher_name': 'ESACCI', 'publisher_url': 'http://www.esa-sst-cci.org', 'references': 'http://www.esa-sst-cci.org', 'sensor': 'ATSR, AATSR, AVHRR_GAC', 'source': 'ATSR<1,2>-ESACCI-L3U-v1.0, AATSR-ESACCI-L3U-v1.0, AVHRR<12,14,15,16,17,18>_G-ESACCI-L2P-v1.0, AVHRRMTA-ESACCI-L2P-v1.0, EUMETSAT_OSI-SAF-ICE-v1.1, EUMETSAT_OSI-SAF-ICE-v2.2', 'source_dir': '/neodc/esacci/sst/data/lt/Analysis/L4/v01.1', 'source_name': 'analysed_sst', 'source_version': 'v01.1', 'southernmost_latitude': -90.0, 'spatial_resolution': '0.05 degree', 'standard_name': 'sea_water_temperature', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention', 'start_time': '20100101T000000Z', 'stop_time': '20100101T235959Z', 'summary': 'OSTIA L4 product from the ESA SST CCI project, produced using OSTIA reanalysis system v2.0. Ice field corrected in v1.1 (v1.0 had ice from day-1). Static ice field between 20080101-20080229 and 20080501-20080521 also fixed in v1.1', 'time_coverage_duration': 'P1D', 'time_coverage_end': '20100101T235959Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '20100101T000000Z', 'title': 'ESA SST CCI OSTIA L4 product', 'tracking_id': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'units': 'kelvin', 'url': 'http://www.esa-sst-cci.org', 'uuid': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'valid_max': 4500.0, 'valid_min': -300.0, 'westernmost_longitude': -180.0} orig_version : v01.1 project_name : ESA Sea Surface Temperature Climate Change Initiative (ESA SST CCI) time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 1991-09-02 url : http://www.esa-sst-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 bare_soil_evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 55 esa_cci_path : nan long_name : Bare Soil Evaporation orig_attrs : {'long_name': 'Bare Soil Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Eb', 'standard_name': 'bare_soil_water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 black_sky_albedo (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 65 esa_cci_path : nan long_name : Black Sky Albedo for Visible Wavebands orig_attrs : {'comment': 'Black sky albedo derived from the GlobAlbedo CCI project dataset', 'long_name': 'Black Sky Albedo for Visible Wavebands', 'orig_attrs': {}, 'project_name': 'GlobAlbedo', 'references': 'Muller, Jan-Peter, et al. \"The ESA GLOBALBEDO project for mapping the Earth\u2019s land surface albedo for 15 years from European sensors.\" Geophysical Research Abstracts. Vol. 13. 2012.', 'source_name': 'DHR_VIS', 'standard_name': 'surface_albedo_black_sky', 'units': '-', 'url': 'http://www.globalbedo.org/'} orig_version : nan project_name : GlobAlbedo time_coverage_end : 2012-01-05 time_coverage_resolution : P8D time_coverage_start : 1998-01-05 url : http://www.globalbedo.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 black_sky_albedo_avhrr (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 76 esa_cci_path : nan long_name : Directional Hemisphere Reflectance albedo - VIS band orig_attrs : {'comment': 'Black sky albedo derived from the QA4ECV Albedo Product', 'long_name': 'Directional Hemisphere Reflectance albedo - VIS band', 'orig_attrs': {}, 'project_name': 'QA4ECV - European Union Framework Program 7', 'source_name': 'DHR_VIS', 'standard_name': 'surface_albedo_black_sky', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV - European Union Framework Program 7 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 burnt_area (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 54 esa_cci_path : nan long_name : Monthly Burnt Area orig_attrs : {'comment': 'Burnt Area based on the GFED4 fire product.', 'long_name': 'Monthly Burnt Area', 'orig_attrs': {}, 'project_name': 'GFED4', 'references': 'Giglio, Louis, James T. Randerson, and Guido R. Werf. \"Analysis of daily, monthly, and annual burned area using the fourth\u2010generation global fire emissions database (GFED4).\" Journal of Geophysical Research: Biogeosciences 118.1 (2013): 317-328.', 'source_name': 'BurntArea', 'standard_name': 'burnt_area', 'units': 'hectares', 'url': 'http://www.globalfiredata.org/'} orig_version : gfed4 project_name : GFED4 time_coverage_end : 2014-03-02 time_coverage_resolution : P8D time_coverage_start : 1995-01-05 url : http://www.globalfiredata.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 c_emissions (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 53 esa_cci_path : nan long_name : Carbon Dioxide Emissions Due to Natural Fires orig_attrs : {'comment': 'Carbon emissions by fires based on the GFED4 fire product.', 'long_name': 'Carbon dioxide emissions due to natural fires expressed as carbon flux.', 'orig_attrs': {}, 'project_name': 'GFED4', 'references': 'Giglio, Louis, James T. Randerson, and Guido R. Werf. \"Analysis of daily, monthly, and annual burned area using the fourth\u2010generation global fire emissions database (GFED4).\" Journal of Geophysical Research: Biogeosciences 118.1 (2013): 317-328.', 'source_name': 'Emission', 'standard_name': 'surface_upward_mass_flux_of_carbon_dioxide_expressed_as_carbon_due_to_emission_from_fires', 'units': 'g C m-2 month-1', 'url': 'http://www.globalfiredata.org/'} orig_version : gfed4 project_name : GFED4 time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.globalfiredata.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cee (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 30 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Effective Emissivity at 10.8 um orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud effective emissivity at 10.8 um', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cee', 'spatial_resolution': '0.50 degree', 'standard_name': 'cee', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 1.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cer (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 31 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Effective Radius orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud effective radius', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cer', 'spatial_resolution': '0.50 degree', 'standard_name': 'cer', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'um', 'url': 'http://www.dwd.de', 'valid_max': 200.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cfc (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 32 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud fraction orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud fraction', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cfc', 'spatial_resolution': '0.50 degree', 'standard_name': 'cfc', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 1.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 chlor_a (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 29 esa_cci_path : /neodc/esacci/ocean_colour/data/v3.1-release/geographic/netcdf/chlor_a/daily/v3.1 long_name : Chlorophyll-a Concentration in Seawater orig_attrs : {'Conventions': 'CF-1.6', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'ancillary_variables': 'chlor_a_log10_rmsd chlor_a_log10_bias', 'cdm_data_type': 'Grid', 'comment': 'See summary attribute', 'creation_date': '20160822T065128Z', 'creator_email': 'help@esa-oceancolour-cci.org', 'creator_name': 'Plymouth Marine Laboratory', 'creator_url': 'http://esa-oceancolour-cci.org', 'date_created': '20160822T065128Z', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': '.04166666666666666666', 'geospatial_lat_units': 'decimal degrees north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': '.04166666666666666666', 'geospatial_lon_units': 'decimal degrees east', 'geospatial_vertical_max': 0.0, 'geospatial_vertical_min': 0.0, 'grid_mapping': 'crs', 'history': 'Source data were: NASA OBPG SeaWiFS level2 R2014.0 LAC and GAC [A/C via l2gen], NASA OBPG VIIRS L2 R2014.0.1 (identical to R2014.0.2) [A/C via l2gen], NASA OBPG MODIS Aqua level 1A [A/C: l2gen equivalent to R2014.0.1 + Polymer 3.5] and ESA MERIS L1B (3rd reprocessing inc OCL correction) [Polymer v3.5]; Derived products were mainly produced with functions validated from the current NASA SeaDAS release and some custom implementations. Uncertainty generation determined by the fuzzy classifier scheme of Tim Moore (2009) and Thomas Jackson et al (2017)', 'id': 'ESACCI-OC-L3S-CHLOR_A-MERGED-1D_DAILY_4km_GEO_PML_OCx-20120101-fv3.1.nc', 'institution': 'Plymouth Marine Laboratory', 'keywords': 'satellite,observation,ocean,ocean colour', 'keywords_vocabulary': 'none', 'license': 'ESA CCI Data Policy: free and open access. When referencing, please use: Ocean Colour Climate Change Initiative dataset, Version <Version Number>, European Space Agency, available online at http://www.esa-oceancolour-cci.org. We would also appreciate being notified of publications so that we can list them on the project website at http://www.esa-oceancolour-cci.org/?q=publications', 'long_name': \"Chlorophyll-a concentration in seawater (not log-transformed), generated by SeaDAS using a blended combination of OCI (OC4v6 + Hu's CI), OC3 and OC5, depending on water class memberships\", 'naming_authority': 'uk.ac.pml', 'netcdf_file_type': 'NETCDF4_CLASSIC', 'number_of_optical_water_types': '14', 'orig_attrs': {}, 'parameter_vocab_uri': 'http://vocab.nerc.ac.uk/collection/P04/current/', 'platform': 'Orbview-2,Aqua,Envisat,Suomi-NPP', 'processing_level': 'Level-3', 'product_version': '3.1', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-oceancolour-cci.org/', 'sensor': 'SeaWiFS,MODIS,MERIS,VIIRS', 'source': 'NASA SeaWiFS L2 R2014.0 LAC and GAC, MODIS-Aqua L1A, MERIS L1B 3rd reprocessing inc OCL corrections, NASA VIIRS L2 R2014.0.1 (data identical to R2014.0.2)', 'source_dir': '/neodc/esacci/ocean_colour/data/v3.1-release/geographic/netcdf/chlor_a/daily/v3.1/', 'source_name': 'chlor_a', 'source_version': 'v3.1', 'spatial_resolution': '4km nominal at equator', 'standard_name': 'mass_concentration_of_chlorophyll_a_in_sea_water', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Conventions Version 1.6', 'start_date': '01-JAN-2012 00:00:00.000000', 'stop_date': '01-JAN-2012 23:59:00.000000', 'summary': \"Data products generated by the Ocean Colour component of the European Space Agency Climate Change Initiative project. These files are daily composites of merged sensor (MERIS, MODIS Aqua, SeaWiFS LAC & GAC, VIIRS) products. MODIS Aqua and MERIS were band-shifted and bias-corrected to SeaWiFS bands and values using a temporally and spatially varying scheme based on the overlap years of 2003-2007. VIIRS was band-shifted and bias-corrected in a second stage against the MODIS Rrs that had already been corrected to SeaWiFS levels, for the overlap period 2012-2013. VIIRS and SeaWiFS Rrs were derived from standard NASA L2 products; MERIS and MODIS from a combination of NASA's l2gen (for basic sensor geometry corrections, etc) and HYGEOS Polymer v3.5 (for atmospheric correction). The Rrs were binned to a sinusoidal 4km level-3 grid, and later to 4km geographic projection, by Brockmann Consult's BEAM. Derived products were generally computed with the standard SeaDAS algorithms. QAA IOPs were derived using the standard SeaDAS algorithm but with a modified backscattering table to match that used in the bandshifting. The final chlorophyll is a combination of OC4, Hu's CI and OC5, depending on the water class memberships. Uncertainty estimates were added using the fuzzy water classifier and uncertainty estimation algorithm of Tim Moore as documented in Jackson et al (2017).\", 'time_coverage_duration': 'P1D', 'time_coverage_end': '201201012359Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '201201010000Z', 'title': 'ESA CCI Ocean Colour Product', 'tracking_id': '4e0985e0-f157-40f6-b0f1-0a2bb0261f12', 'units': 'milligram m-3', 'units_nonstandard': 'mg m^-3', 'url': 'http://esa-oceancolour-cci.org'} orig_version : v3.1 project_name : ESA CCI Ocean Colour Product time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1997-09-02 url : http://esa-oceancolour-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cot (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 35 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Optical Thickness orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud optical thickness', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cot', 'spatial_resolution': '0.50 degree', 'standard_name': 'cot', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 320.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 country_mask (time, lat, lon) float64 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> orig_attrs : {'ds_method': 'MODE', 'orig_attrs': {}, 'source_name': 'country_mask', 'standard_name': 'country_mask', 'units': '-'} Array Chunk Bytes 14.12 GB 8.29 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float64 numpy.ndarray 1440 720 1702 cph (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 39 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Fraction of Liquid Water Clouds orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'fraction of liquid water clouds', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cph', 'spatial_resolution': '0.50 degree', 'standard_name': 'cph', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': '1', 'url': 'http://www.dwd.de', 'valid_max': 1.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 cth (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 36 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Top Height orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud top height', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'cth', 'spatial_resolution': '0.50 degree', 'standard_name': 'cth', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'km', 'url': 'http://www.dwd.de', 'valid_max': 20.0, 'valid_min': -1.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 ctp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 37 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Top Pressure orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud top pressure', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'ctp', 'spatial_resolution': '0.50 degree', 'standard_name': 'ctp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'hPa', 'url': 'http://www.dwd.de', 'valid_max': 1200.0, 'valid_min': 50.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 ctt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 38 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Top Temperature orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud top temperature', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'ctt', 'spatial_resolution': '0.50 degree', 'standard_name': 'ctt', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'K', 'url': 'http://www.dwd.de', 'valid_max': 320.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 56 esa_cci_path : nan long_name : Evaporation orig_attrs : {'long_name': 'Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'E', 'standard_name': 'water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 evaporative_stress (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 57 esa_cci_path : nan long_name : Evaporative Stress Factor orig_attrs : {'long_name': 'Evaporative Stress Factor', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'S', 'standard_name': 'evaporative_stress_factor', 'units': '', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fapar_tip (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 74 esa_cci_path : nan long_name : Fraction of Absorbed PAR orig_attrs : {'long_name': 'Fraction of Absorbed Photosynthetically Active Radiation', 'orig_attrs': {}, 'project_name': 'QA4ECV', 'source_name': 'fapar', 'standard_name': 'fapar', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fat_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 21 esa_cci_path : nan long_name : Tropospheric Ozone Column (Fixed Altitude) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Altitude definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'fat_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fat_p (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 17 esa_cci_path : nan long_name : Tropopause Air Pressure for the Fixed Altitude Tropopause orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropopause_air_pressure for the Fixed Altitude Tropopause', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'fat_p', 'standard_name': 'tropopause_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 flt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 19 esa_cci_path : nan long_name : Tropospheric Ozone Column orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Layers definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'flt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 flt_p (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 18 esa_cci_path : nan long_name : Tropopause Air Pressure for the Fixed Layer Tropopause orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropopause_air_pressure for the fixed layer tropopause', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'flt_p', 'standard_name': 'tropopause_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 fractional_snow_cover (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 68 esa_cci_path : nan long_name : Surface Fraction Covered by Snow orig_attrs : {'comment': 'Grid cell fractional snow cover based on the Globsnow CCI product.', 'long_name': 'Surface fraction covered by snow.', 'orig_attrs': {}, 'project_name': 'GlobSnow', 'references': 'Luojus, Kari, et al. \"ESA DUE Globsnow-Global Snow Database for Climate Research.\" ESA Special Publication. Vol. 686. 2010.', 'source_name': 'MFSC', 'standard_name': 'surface_snow_area_fraction', 'units': 'percent', 'url': 'http://www.globsnow.info/'} orig_version : v2.0 project_name : GlobSnow time_coverage_end : 2013-01-05 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.globsnow.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_fat_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 22 esa_cci_path : nan long_name : Tropospheric Ozone Column (Fixed Altitude) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Altitude definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_fat_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_flt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 23 esa_cci_path : nan long_name : Tropospheric Ozone Column (Fixed Layers) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on Fixed Layers definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_flt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_lrt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 20 esa_cci_path : nan long_name : Tropospheric Ozone Column ( Lapse Rate) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on lapse rate definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_lrt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_msr_flt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 8 esa_cci_path : nan long_name : Residual MSR-FLT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-FLT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_msr_flt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 free_msr_lrt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 10 esa_cci_path : nan long_name : Residual MSR-LRT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-LRT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'free_msr_lrt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 interception_loss (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 58 esa_cci_path : nan long_name : Interception Loss orig_attrs : {'long_name': 'Interception Loss', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Ei', 'standard_name': 'interception_loss', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 iwp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 33 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Ice Water Path orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud ice water path', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'iwp', 'spatial_resolution': '0.50 degree', 'standard_name': 'iwp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'g/m2', 'url': 'http://www.dwd.de', 'valid_max': 32000.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 land_surface_temperature (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 69 esa_cci_path : nan long_name : Land Surface Temperature orig_attrs : {'comment': 'Advanced Along Track Scanning Radiometer pixel land surface temperature product', 'long_name': 'Land Surface Temperature', 'orig_attrs': {}, 'project_name': 'GlobTemperature', 'references': 'Jim\u00e9nez, C., et al. \"Inversion of AMSR\u2010E observations for land surface temperature estimation: 1. Methodology and evaluation with station temperature.\" Journal of Geophysical Research: Atmospheres 122.6 (2017): 3330-3347.', 'source_name': 'LST', 'standard_name': 'surface_temperature', 'units': 'K', 'url': 'http://data.globtemperature.info/'} orig_version : nan project_name : GlobTemperature time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2002-05-21 url : http://data.globtemperature.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 latent_energy (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 48 esa_cci_path : nan long_name : Latent Energy orig_attrs : {'comment': 'Latent heat flux from the surface.', 'long_name': 'Latent Energy', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'LE', 'standard_name': 'surface_upward_latent_heat_flux', 'units': 'W m-2', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 leaf_area_index (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 73 esa_cci_path : nan long_name : Effective Leaf Area Index orig_attrs : {'long_name': 'Effective Leaf Area Index', 'orig_attrs': {}, 'project_name': 'QA4ECV', 'source_name': 'Lai', 'standard_name': 'leaf_area_index', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 lrt_c (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 24 esa_cci_path : nan long_name : Tropospheric Ozone Column (Lapse Rate) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropospheric ozone column (based on lapse rate definition) in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'lrt_c', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 lrt_p (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 16 esa_cci_path : nan long_name : Tropopause Air Pressure (Lapse Rate) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'tropopause_air_pressure for the lapse rate tropopause', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'lrt_p', 'standard_name': 'tropopause_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 lwp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 34 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Cloud Liquid Water Path orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'cloud liquid water path', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'orig_attrs': {}, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'lwp', 'spatial_resolution': '0.50 degree', 'standard_name': 'lwp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'g/m2', 'url': 'http://www.dwd.de', 'valid_max': 32000.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 mask (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 46 esa_cci_path : /neodc/esacci/sst/data/lt/Analysis/L4/v01.1/ long_name : Sea/Land/Lake/Ice Field Composite Mask orig_attrs : {'Conventions': 'CF-1.5, Unidata Observation Dataset v1.0', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'acknowledgment': 'Funded by ESA', 'cdm_data_type': 'grid', 'comment': 'WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value', 'creator_email': 'science.leader@esa-sst-cci.org', 'creator_name': 'ESA SST CCI', 'creator_processing_institution': 'These data were produced at the Met Office as part of the ESA SST CCI project.', 'creator_url': 'http://www.esa-sst-cci.org', 'date_created': '20130309T132046Z', 'easternmost_longitude': 180.00001525878906, 'file_quality_level': 3, 'gds_version_id': '2.0', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': 0.05000000074505806, 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': 0.05000000074505806, 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': -0.20000000298023224, 'geospatial_vertical_min': -0.20000000298023224, 'history': 'Created using OSTIA reanalysis system v2.0', 'id': 'OSTIA-ESACCI-L4-v01.1', 'institution': 'ESACCI', 'keywords': 'Oceans > Ocean Temperature > Sea Surface Temperature', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'license': 'GHRSST protocol describes data use as free and open', 'long_name': 'sea/land/lake/ice field composite mask', 'metadata_link': 'http://www.esa-cci.org', 'naming_authority': 'org.ghrsst', 'netcdf_version_id': '4.1.3', 'northernmost_latitude': 90.0, 'orig_attrs': {}, 'platform': 'ERS-<1,2>, Envisat, NOAA-<12,14,15,16,17,18>, MetOpA', 'processing_level': 'L4', 'product_version': '1.1', 'project': 'Climate Change Initiative - European Space Agency', 'publisher_email': 'science.leader@esa-sst-cci.org', 'publisher_name': 'ESACCI', 'publisher_url': 'http://www.esa-sst-cci.org', 'references': 'http://www.esa-sst-cci.org', 'sensor': 'ATSR, AATSR, AVHRR_GAC', 'source': 'ATSR<1,2>-ESACCI-L3U-v1.0, AATSR-ESACCI-L3U-v1.0, AVHRR<12,14,15,16,17,18>_G-ESACCI-L2P-v1.0, AVHRRMTA-ESACCI-L2P-v1.0, EUMETSAT_OSI-SAF-ICE-v1.1, EUMETSAT_OSI-SAF-ICE-v2.2', 'source_dir': '/neodc/esacci/sst/data/lt/Analysis/L4/v01.1', 'source_name': 'mask', 'source_version': 'v01.1', 'southernmost_latitude': -90.0, 'spatial_resolution': '0.05 degree', 'standard_name': 'mask', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention', 'start_time': '20100101T000000Z', 'stop_time': '20100101T235959Z', 'summary': 'OSTIA L4 product from the ESA SST CCI project, produced using OSTIA reanalysis system v2.0. Ice field corrected in v1.1 (v1.0 had ice from day-1). Static ice field between 20080101-20080229 and 20080501-20080521 also fixed in v1.1', 'time_coverage_duration': 'P1D', 'time_coverage_end': '20100101T235959Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '20100101T000000Z', 'title': 'ESA SST CCI OSTIA L4 product', 'tracking_id': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'url': 'http://www.esa-sst-cci.org', 'uuid': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'valid_max': 31.0, 'valid_min': 1.0, 'westernmost_longitude': -180.0} orig_version : v01.1 project_name : ESA Sea Surface Temperature Climate Change Initiative (ESA SST CCI) time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 1991-09-02 url : http://www.esa-sst-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 max_air_temperature_2m (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 6 esa_cci_path : nan long_name : Maximum 2 Metre Air Temperature orig_attrs : {'comment': 'Air temperature at 2m from the ERA5 reanalysis product.', 'long_name': 'Maximum 2 metre air temperature', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'max_air_temperature_2m', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 min_air_temperature_2m (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 7 esa_cci_path : nan long_name : Minimum 2 Metre Air Temperature orig_attrs : {'comment': 'Air temperature at 2m from the ERA5 reanalysis product.', 'long_name': 'Minimum 2 metre air temperature', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'min_air_temperature_2m', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 msr_flt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 9 esa_cci_path : nan long_name : Residual MSR-FLT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-FLT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'msr_flt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 msr_lrt (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 11 esa_cci_path : nan long_name : Residual MSR-LRT (Stratospheric Part Partial) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'residual MSR-LRT_stratospheric_part partial ozone column in mole per square meter', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'msr_lrt', 'standard_name': 'troposphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 net_ecosystem_exchange (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 49 esa_cci_path : nan long_name : Net Ecosystem Exchange orig_attrs : {'comment': 'Net carbon exchange between the ecosystem and the atmopshere.', 'long_name': 'Net Ecosystem Exchange', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'NEE', 'standard_name': 'net_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 net_radiation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 50 esa_cci_path : nan long_name : Net Radiation orig_attrs : {'comment': 'Net radiation to the surface', 'long_name': 'Net Radiation', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'Rn', 'standard_name': 'surface_net_radiation_flux', 'units': 'W m-2', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 open_water_evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 59 esa_cci_path : nan long_name : Open-Water Evaporation orig_attrs : {'long_name': 'Open-water Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Ew', 'standard_name': 'water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 ozone (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 72 esa_cci_path : /neodc/esacci/ozone/data/total_columns/l3/merged/v0100/ long_name : Mean Total Ozone Column in dobson units orig_attrs : {'comment': 'Atmospheric ozone based on the Ozone CCI data.', 'long_name': 'Mean total ozone column in dobson units', 'orig_attrs': {}, 'project_name': 'Ozone CCI', 'references': 'Laeng, A., et al. \"The ozone climate change initiative: Comparison of four Level-2 processors for the Michelson Interferometer for Passive Atmospheric Sounding (MIPAS).\" Remote Sensing of Environment 162 (2015): 316-343.', 'source_name': 'atmosphere_mole_content_of_ozone', 'standard_name': 'atmosphere_mole_content_of_ozone', 'units': 'DU', 'url': 'http://www.esa-ozone-cci.org/'} orig_version : v0100 project_name : Ozone CCI time_coverage_end : 2011-06-30 time_coverage_resolution : P8D time_coverage_start : 1996-03-09 url : http://www.esa-ozone-cci.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 par (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 3 esa_cci_path : nan long_name : Photosynthetically Active Radiation orig_attrs : {'long_name': 'Photosynthetically active radiation', 'orig_attrs': {}, 'project_name': 'BESS', 'references': 'Ryu, Y.*, Jiang, C., Kobayashi, H., & Detto, M. (2018). MODIS-derived global land products of shortwave radiation and diffuse and total photosynthetically active radiation at 5 km resolution from 2000. Remote Sensing of Environment, 204, 812-825', 'source_name': 'surface_downwelling_photosynthetic_radiative_flux_in_air', 'standard_name': 'surface_downwelling_photosynthetic_radiative_flux_in_air', 'units': 'W m-2', 'url': 'http://environment.snu.ac.kr/bess_rad/'} orig_version : 15.10.2017 project_name : BESS time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-03-01 url : http://environment.snu.ac.kr/bess_rad/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 pardiff (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 1 esa_cci_path : nan long_name : Diffuse Photosynthetically Active Radiation orig_attrs : {'long_name': 'Diffuse Photosynthetically active radiation', 'orig_attrs': {}, 'project_name': 'BESS', 'references': 'Ryu, Y.*, Jiang, C., Kobayashi, H., & Detto, M. (2018). MODIS-derived global land products of shortwave radiation and diffuse and total photosynthetically active radiation at 5 km resolution from 2000. Remote Sensing of Environment, 204, 812-825', 'source_name': 'surface_diffuse_downwelling_photosynthetic_radiative_flux_in_air', 'standard_name': 'surface_diffuse_downwelling_photosynthetic_radiative_flux_in_air', 'units': 'W m-2', 'url': 'http://environment.snu.ac.kr/bess_rad/'} orig_version : 15.10.2017 project_name : BESS time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-03-01 url : http://environment.snu.ac.kr/bess_rad/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 potential_evaporation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 60 esa_cci_path : nan long_name : Potential Evaporation orig_attrs : {'long_name': 'Potential Evaporation', 'orig_attrs': {}, 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Ep', 'standard_name': 'potential_water_evaporation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 precipitation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 71 esa_cci_path : nan long_name : Precipitation orig_attrs : {'comment': 'Precipitation based on the GPCP dataset.', 'long_name': 'Precip - RealTime [RT] (see documentation for more information)', 'orig_attrs': {}, 'project_name': 'GPCP', 'references': 'Adler, Robert F., et al. \"The version-2 global precipitation climatology project (GPCP) monthly precipitation analysis (1979-present).\" Journal of hydrometeorology 4.6 (2003): 1147-1167.', 'source_name': 'Precip', 'standard_name': 'precipitation_flux', 'units': 'mm/day', 'url': 'http://precip.gsfc.nasa.gov/'} orig_version : nan project_name : GPCP time_coverage_end : 2015-01-05 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 url : http://precip.gsfc.nasa.gov/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 precipitation_era5 (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 5 esa_cci_path : nan long_name : ERA5 Precipitation orig_attrs : {'comment': 'Total precipitation from the ERA5 reanalysis product.', 'long_name': 'ERA 5 Precipitation', 'orig_attrs': {}, 'project_name': 'ERA5', 'references': '', 'source_name': 'precipitation', 'units': 'K', 'url': 'https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation'} orig_version : ERA5 project_name : ERA5 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2000-01-05 url : https://confluence.ecmwf.int//display/CKB/ERA5+data+documentation Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 psurf (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 12 esa_cci_path : nan long_name : Surface Air Pressure orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'surface_air_pressure', 'orig_attrs': {}, 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'psurf', 'standard_name': 'surface_air_pressure', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'hPa', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 root_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 61 esa_cci_path : nan long_name : Root-Zone Soil Moisture orig_attrs : {'long_name': 'Root-Zone Soil Moisture', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'SMroot', 'standard_name': 'soil_moisture_content', 'units': 'm3/m3', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 sea_ice_fraction (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 45 esa_cci_path : /neodc/esacci/sst/data/lt/Analysis/L4/v01.1/ long_name : Sea Ice Area Fraction orig_attrs : {'Conventions': 'CF-1.5, Unidata Observation Dataset v1.0', 'Metadata_Conventions': 'Unidata Dataset Discovery v1.0', 'acknowledgment': 'Funded by ESA', 'cdm_data_type': 'grid', 'comment': 'WARNING Some applications are unable to properly handle signed byte values. If values are encountered > 127, please subtract 256 from this reported value', 'creator_email': 'science.leader@esa-sst-cci.org', 'creator_name': 'ESA SST CCI', 'creator_processing_institution': 'These data were produced at the Met Office as part of the ESA SST CCI project.', 'creator_url': 'http://www.esa-sst-cci.org', 'date_created': '20130309T132046Z', 'easternmost_longitude': 180.00001525878906, 'file_quality_level': 3, 'gds_version_id': '2.0', 'geospatial_lat_max': 90.0, 'geospatial_lat_min': -90.0, 'geospatial_lat_resolution': 0.05000000074505806, 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 180.0, 'geospatial_lon_min': -180.0, 'geospatial_lon_resolution': 0.05000000074505806, 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': -0.20000000298023224, 'geospatial_vertical_min': -0.20000000298023224, 'history': 'Created using OSTIA reanalysis system v2.0', 'id': 'OSTIA-ESACCI-L4-v01.1', 'institution': 'ESACCI', 'keywords': 'Oceans > Ocean Temperature > Sea Surface Temperature', 'keywords_vocabulary': 'NASA Global Change Master Directory (GCMD) Science Keywords', 'license': 'GHRSST protocol describes data use as free and open', 'long_name': 'sea ice area fraction', 'metadata_link': 'http://www.esa-cci.org', 'naming_authority': 'org.ghrsst', 'netcdf_version_id': '4.1.3', 'northernmost_latitude': 90.0, 'platform': 'ERS-<1,2>, Envisat, NOAA-<12,14,15,16,17,18>, MetOpA', 'processing_level': 'L4', 'product_version': '1.1', 'project': 'Climate Change Initiative - European Space Agency', 'publisher_email': 'science.leader@esa-sst-cci.org', 'publisher_name': 'ESACCI', 'publisher_url': 'http://www.esa-sst-cci.org', 'references': 'http://www.esa-sst-cci.org', 'sensor': 'ATSR, AATSR, AVHRR_GAC', 'source': 'ATSR<1,2>-ESACCI-L3U-v1.0, AATSR-ESACCI-L3U-v1.0, AVHRR<12,14,15,16,17,18>_G-ESACCI-L2P-v1.0, AVHRRMTA-ESACCI-L2P-v1.0, EUMETSAT_OSI-SAF-ICE-v1.1, EUMETSAT_OSI-SAF-ICE-v2.2', 'source_dir': '/neodc/esacci/sst/data/lt/Analysis/L4/v01.1', 'source_name': 'sea_ice_fraction', 'source_version': 'v01.1', 'southernmost_latitude': -90.0, 'spatial_resolution': '0.05 degree', 'standard_name': 'sea_ice_area_fraction', 'standard_name_vocabulary': 'NetCDF Climate and Forecast (CF) Metadata Convention', 'summary': 'OSTIA L4 product from the ESA SST CCI project, produced using OSTIA reanalysis system v2.0. Ice field corrected in v1.1 (v1.0 had ice from day-1). Static ice field between 20080101-20080229 and 20080501-20080521 also fixed in v1.1', 'time_coverage_duration': 'P1D', 'time_coverage_end': '20100101T235959Z', 'time_coverage_resolution': 'P1D', 'time_coverage_start': '20100101T000000Z', 'title': 'ESA SST CCI OSTIA L4 product', 'tracking_id': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'units': '1', 'url': 'http://www.esa-sst-cci.org', 'uuid': '19b1f7a4-d8d1-44eb-9cfa-37cc33c4c2c1', 'valid_max': 100.0, 'valid_min': 0.0, 'westernmost_longitude': -180.0} orig_version : v01.1 project_name : ESA Sea Surface Temperature Climate Change Initiative (ESA SST CCI) time_coverage_end : 2010-12-31 time_coverage_resolution : P8D time_coverage_start : 1991-09-02 url : http://www.esa-sst-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 sensible_heat (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 51 esa_cci_path : nan long_name : Sensible Heat orig_attrs : {'comment': 'Sensible heat flux from the surface', 'long_name': 'Sensible Heat', 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'H', 'standard_name': 'surface_upward_sensible_heat_flux', 'units': 'W m-2', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 snow_sublimation (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 62 esa_cci_path : nan long_name : Snow Sublimation orig_attrs : {'long_name': 'Snow Sublimation', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Es', 'standard_name': 'snow_sublimation_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 snow_water_equivalent (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 67 esa_cci_path : nan long_name : Daily Snow Water Equivalent orig_attrs : {'certain_values': '-2 == mountains, -1 == water bodies, 0 == either SWE, or missing data in the southern hemisphere', 'comment': 'Grid cell fractional snow cover based on the Globsnow CCI product.', 'long_name': 'Daily Snow Water Equivalent', 'project_name': 'GlobSnow', 'references': 'Luojus, Kari, et al. \"ESA DUE Globsnow-Global Snow Database for Climate Research.\" ESA Special Publication. Vol. 686. 2010.', 'source_name': 'SWE', 'units': 'mm', 'url': 'http://www.globsnow.info/'} orig_version : v2.0 project_name : GlobSnow time_coverage_end : 2012-12-30 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 url : http://www.globsnow.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 soil_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 78 esa_cci_path : /neodc/esacci/soil_moisture/data/daily_files/COMBINED/v04.2/ long_name : Soil Moisture orig_attrs : {'comment': 'Soil moisture based on the SOilmoisture CCI project', 'long_name': 'Soil Moisture', 'project_name': 'SoilMoisture CCI', 'references': 'Liu, Y.Y., Parinussa, R.M., Dorigo, W.A., De Jeu, R.A.M., Wagner, W., McCabe, M.F., Evans, J.P., and van Dijk, A.I.J.M. (2012): Trend-preserving blending of passive and active microwave soil moisture retrievals; Liu, Y.Y., Parinussa, R.M., Dorigo, W.A., De Jeu, R.A.M., Wagner, W., van Dijk, A.I.J.M., McCabe, M.F., & Evans, J.P. (2011): Developing an improved soil moisture dataset by blending passive and active microwave satellite based retrievals. Hydrology and Earth System Sciences, 15, 425-436.', 'source_name': 'SoilMoisture', 'standard_name': 'soil_moisture_content', 'units': 'm3', 'url': 'http://www.esa-soilmoisture-cci.org'} orig_version : v04.2 project_name : SoilMoisture CCI time_coverage_end : 2014-01-29 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 url : http://www.esa-soilmoisture-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 srex_mask (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 77 esa_cci_path : nan long_name : Mask for SREX Regions orig_attrs : {'ds_method': 'MODE', 'long_name': 'Mask for SREX regions', 'source_name': 'layer', 'standard_name': 'srex_mask', 'units': '-'} orig_version : nan project_name : regionmask - SREX Regions time_coverage_end : 1980-01-05 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 url : https://regionmask.readthedocs.io/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 stemp (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 40 esa_cci_path : /neodc/esacci/cloud/data/phase-2/L3C/MODIS-TERRA/v2.0/ long_name : Surface Temperature orig_attrs : {'Conventions': 'CF-1.6,ACDD-1.3', 'cdm_data_type': 'Grid', 'comment': 'These data were produced at ESACCI as part of the ESA Cloud CCI project.', 'creator_email': 'contact.cloudcci@dwd.de', 'creator_name': 'Deutscher Wetterdienst', 'creator_url': 'http://www.dwd.de', 'date_created': '2016-04-25T17:07:07+0000', 'geospatial_lat_max': 89.75, 'geospatial_lat_min': -89.75, 'geospatial_lat_resolution': '0.50', 'geospatial_lat_units': 'degrees_north', 'geospatial_lon_max': 179.75, 'geospatial_lon_min': -179.75, 'geospatial_lon_resolution': '0.50', 'geospatial_lon_units': 'degrees_east', 'geospatial_vertical_max': '0.0', 'geospatial_vertical_min': '0.0', 'history': 'Dataset produced by DWDs CC4CL retrieval system installed at ECMWF in second phase of ESA Cloud CCI.', 'id': '200002-ESACCI-L3C_CLOUD-CLD_PRODUCTS-MODIS_TERRA-fv2.0.nc', 'institution': 'Deutscher Wetterdienst', 'keywords': 'EARTH SCIENCE > ATMOSPHERE > SATELLITES > CLOUDS > CLOUD PROPERTIES', 'keywords_vocabulary': 'GCMD Science Keywords, Version 8.1', 'license': 'ESA CCI Data Policy: free and open access', 'long_name': 'surface temperature', 'naming_authority': 'de.dwd', 'number_of_processed_orbits': 1516, 'platform': 'TERRA', 'product_version': '2.0', 'project': 'Climate Change Initiative - European Space Agency', 'references': 'http://www.esa-cloud-cci.info', 'sensor': 'MODIS', 'source': 'MODIS_TERRA_Collection 6', 'source_name': 'stemp', 'spatial_resolution': '0.50 degree', 'standard_name': 'stemp', 'standard_name_vocabulary': 'NetCDF Climate Forecast (CF) Metadata Convention version 18', 'summary': 'This dataset contains monthly Level-3 global cloud property products from satellite observations. Averaged onto a regular grid.', 'time_coverage_duration': 'P1M', 'time_coverage_end': '20000229T235959Z', 'time_coverage_resolution': 'P1M', 'time_coverage_start': '20000201T000000Z', 'title': 'ESA Cloud CCI Retrieval Products L3 Output File', 'tracking_id': '1b6a5bee-afad-43e5-a326-67a76df184a7', 'units': 'K', 'url': 'http://www.dwd.de', 'valid_max': 320.0, 'valid_min': 0.0} orig_version : v2.0 project_name : ESA Cloud Climate Change Initiative (Cloud_cci) time_coverage_end : 2014-12-31 time_coverage_resolution : P8D time_coverage_start : 2000-01-29 url : http://www.dwd.de Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 surface_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 63 esa_cci_path : nan long_name : Surface Soil Moisture orig_attrs : {'long_name': 'Surface Soil Moisture', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'SMsurf', 'standard_name': 'soil_moisture_content', 'units': 'mm3/mm3', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 terrestrial_ecosystem_respiration (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 52 esa_cci_path : nan long_name : Terrestrial Ecosystem Respiration orig_attrs : {'comment': 'Total carbon release of the ecosystem through respiration.', 'long_name': 'Terrestrial Ecosystem Respiration', 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'TERall', 'standard_name': 'ecosystem_respiration_carbon_flux', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2012-12-30 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 totcol_assim (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 13 esa_cci_path : nan long_name : Total Ozone Column (Assimilated TM5 data) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'total ozone column derived from assimilated TM5 data', 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'totcol_assim', 'standard_name': 'atmosphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 totcol_free (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 14 esa_cci_path : nan long_name : Total Ozone Column (Assimilated TM5 data) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'total ozone column derived from assimilated TM5 data', 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'totcol_free', 'standard_name': 'atmosphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 totcol_msr (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 15 esa_cci_path : nan long_name : Total Ozone Column (MSR data) orig_attrs : {'Conventions': 'CF-1.6', 'comment': 'The global tropospheric ozone column from 0 to 6 km is presented here. The column is derived by simultaneous assimlating ozone profiles of GOME-2 and OMI.', 'creator_email': 'peet@knmi.nl', 'creator_name': 'J.C.A. van Peet', 'creator_url': 'KNMI, http://www.kmnmi.nl/', 'institution': 'Royal Netherlands Meteorological Institute, KNMI', 'long_name': 'total ozone column derived from MSR data', 'project_name': 'Tropospheric ozone column', 'references': 'Jacob C. A. van Peet, Ronald J. van der A, Hennie M. Kelder, and Pieternel F. Levelt (2018),Simultaneous assimilation of ozone profiles from multiple UV-VIS satellite instruments, Atmospheric Chemistry and Physics, doi:10.5194/acp-18-1685-2018', 'source_name': 'totcol_msr', 'standard_name': 'atmosphere_mole_content_of_ozone', 'title': 'Tropospheric ozone columns from assimilated satellite data.', 'units': 'mol m-2', 'url': 'http://www.temis.nl/protocols/tropo.html'} orig_version : v.1.2.3.1 project_name : ESA - Near-real time total ozone column (OMI) time_coverage_end : 2011-12-31 time_coverage_resolution : P8D time_coverage_start : 2008-01-05 url : http://www.temis.nl/protocols/tropo.html Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 transpiration (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 64 esa_cci_path : nan long_name : Transpiration orig_attrs : {'long_name': 'Transpiration', 'project_name': 'GLEAM', 'references': 'Martens, B., Miralles, D.G., Lievens, H., van der Schalie, R., de Jeu, R.A.M., Fern\u00e1ndez-Prieto, D., Beck, H.E., Dorigo, W.A., and Verhoest, N.E.C.: GLEAM v3: satellite-based land evaporation and root-zone soil moisture, Geoscientific Model Development, 10, 1903\u20131925, 2017.', 'source_name': 'Et', 'standard_name': 'transpiration_flux', 'units': 'mm/day', 'url': 'http://www.gleam.eu'} orig_version : Version 3.2 project_name : GLEAM time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 2003-01-05 url : http://www.gleam.eu Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 water_mask (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 43 esa_cci_path : /neodc/esacci/land_cover/data/water_bodies/v4.0/ long_name : Terrestrial or Water Pixel Classification orig_attrs : {'long_name': 'Terrestrial or water pixel classification', 'project_name': 'Climate Change Initiative - European Space Agency', 'source_name': 'wb_class', 'standard_name': 'land_cover_lccs', 'units': '-', 'url': 'http://www.esa-landcover-cci.org'} orig_version : v4.0 project_name : ESA Land Cover Climate Change Initiative (Land_Cover_cci) time_coverage_end : 1980-01-05 time_coverage_resolution : P8D time_coverage_start : 1980-01-05 url : http://www.esa-landcover-cci.org Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 water_vapour (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 70 esa_cci_path : nan long_name : Total Column Water Vapour orig_attrs : {'comment': 'Total column water vapour based on the GlobVapour CCI product.', 'long_name': 'Total Column Water Vapour', 'project_name': 'GlobVapour', 'references': 'Schneider, Nadine, et al. \"ESA DUE GlobVapour water vapor products: Validation.\" AIP Conference Proceedings. Vol. 1531. No. 1. 2013.', 'source_name': 'tcwv_res', 'standard_name': 'atmosphere_mass_content_of_water_vapor', 'units': 'kg m-2', 'url': 'http://www.globvapour.info/'} orig_version : nan project_name : GlobVapour time_coverage_end : 2008-12-30 time_coverage_resolution : P8D time_coverage_start : 1996-01-05 url : http://www.globvapour.info/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 white_sky_albedo (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 66 esa_cci_path : nan long_name : White Sky Albedo for Visible Wavebands orig_attrs : {'comment': 'White sky albedo derived from the GlobAlbedo CCI project dataset', 'long_name': 'White Sky Albedo for Visible Wavebands', 'project_name': 'GlobAlbedo', 'references': 'Muller, Jan-Peter, et al. \"The ESA GLOBALBEDO project for mapping the Earth\u2019s land surface albedo for 15 years from European sensors.\" Geophysical Research Abstracts. Vol. 13. 2012.', 'source_name': 'BHR_VIS', 'standard_name': 'surface_albedo_white_sky', 'units': '-', 'url': 'http://www.globalbedo.org/'} orig_version : nan project_name : GlobAlbedo time_coverage_end : 2012-01-05 time_coverage_resolution : P8D time_coverage_start : 1998-01-05 url : http://www.globalbedo.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 white_sky_albedo_avhrr (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 75 esa_cci_path : nan long_name : Bi-Hemisphere Reflectance Albedo - VIS band orig_attrs : {'comment': 'White sky albedo derived from the QA4ECV Albedo Product', 'long_name': 'Bi-Hemisphere Reflectance albedo - VIS band', 'project_name': 'QA4ECV - European Union Framework Program 7', 'source_name': 'BHR_VIS', 'standard_name': 'surface_albedo_white_sky', 'units': '1', 'url': 'http://www.qa4ecv.eu/'} orig_version : nan project_name : QA4ECV - European Union Framework Program 7 time_coverage_end : 2016-12-30 time_coverage_resolution : P8D time_coverage_start : 1982-01-05 url : http://www.qa4ecv.eu/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 xch4 (time, lat, lon) float64 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 42 esa_cci_path : /neodc/esacci/ghg/data/obs4mips/crdp_3/CO2/v100/ long_name : Column Average Dry-air Mole Fraction Methane orig_attrs : {'Conventions': 'CF-1.6', 'associated_files': 'obs4mips_co2_crdp3_v100.sav', 'cell_methods': 'time: mean', 'comment': 'Satellite retrieved column-average dry-air mole fraction of atmospheric carbon dioxide (XCO2)', 'contact': 'maximilian.reuter@iup.physik.uni-bremen.de', 'creation_date': '20160303T111125Z', 'data_structure': 'grid', 'frequency': 'mon', 'institute_id': 'IUP', 'institution': 'Institute of Environmental Physics, University of Bremen', 'long_name': 'column-average dry-air mole fraction of atmospheric carbon dioxide', 'mip_specs': 'CMIP5', 'product': 'observations', 'project_id': 'obs4MIPs', 'project_name': 'Ozone CCI', 'realm': 'atmos', 'references': 'Laeng, A., et al. \"The ozone climate change initiative: Comparison of four Level-2 processors for the Michelson Interferometer for Passive Atmospheric Sounding (MIPAS).\" Remote Sensing of Environment 162 (2015): 316-343.', 'source': 'ESA GHG CCI XCO2 CRDP3', 'source_id': 'XCO2_CRDP3', 'source_name': 'xch4', 'source_type': 'satellite_retrieval', 'standard_name': 'dry_atmosphere_mole_fraction_of_carbon_dioxide', 'tracking_id': '60972082-05c2-4a04-947a-99042c642c68', 'units': '1', 'url': 'http://www.esa-ghg-cci.org/'} orig_version : v100 project_name : ESA Greenhouse Gases Climate Change Initiative (GHG_cci) time_coverage_end : 2014-12-15 time_coverage_resolution : P8D time_coverage_start : 2003-01-13 url : http://www.esa-ghg-cci.org/ Array Chunk Bytes 14.12 GB 8.29 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float64 numpy.ndarray 1440 720 1702 xco2 (time, lat, lon) float64 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 41 esa_cci_path : /neodc/esacci/ghg/data/obs4mips/crdp_3/CO2/v100/ long_name : Column Average Dry-air Mole Fraction Carbon Dioxide orig_attrs : {'Conventions': 'CF-1.6', 'associated_files': 'obs4mips_co2_crdp3_v100.sav', 'cell_methods': 'time: mean', 'comment': 'Satellite retrieved column-average dry-air mole fraction of atmospheric carbon dioxide (XCO2)', 'contact': 'maximilian.reuter@iup.physik.uni-bremen.de', 'creation_date': '20160303T111125Z', 'data_structure': 'grid', 'frequency': 'mon', 'institute_id': 'IUP', 'institution': 'Institute of Environmental Physics, University of Bremen', 'long_name': 'column-average dry-air mole fraction of atmospheric carbon dioxide', 'mip_specs': 'CMIP5', 'product': 'observations', 'project_id': 'obs4MIPs', 'project_name': 'Ozone CCI', 'realm': 'atmos', 'references': 'Laeng, A., et al. \"The ozone climate change initiative: Comparison of four Level-2 processors for the Michelson Interferometer for Passive Atmospheric Sounding (MIPAS).\" Remote Sensing of Environment 162 (2015): 316-343.', 'source': 'ESA GHG CCI XCO2 CRDP3', 'source_id': 'XCO2_CRDP3', 'source_name': 'xco2', 'source_type': 'satellite_retrieval', 'standard_name': 'dry_atmosphere_mole_fraction_of_carbon_dioxide', 'tracking_id': '60972082-05c2-4a04-947a-99042c642c68', 'units': '1', 'url': 'http://www.esa-ghg-cci.org/'} orig_version : v100 project_name : ESA Greenhouse Gases Climate Change Initiative (GHG_cci) time_coverage_end : 2014-12-15 time_coverage_resolution : P8D time_coverage_start : 2003-01-13 url : http://www.esa-ghg-cci.org/ Array Chunk Bytes 14.12 GB 8.29 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float64 numpy.ndarray 1440 720 1702 Attributes: (35) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 17.12.2018 geospatial_lat_max : 89.75 geospatial_lat_min : -89.75 geospatial_lon_max : 179.75 geospatial_lon_min : -179.75 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube # again subset only the ones we require variables = [ 'gross_primary_productivity' , 'soil_moisture' ] cubes = datacube [ variables ] cubes And we're done. That's it, there's nothing more to it.","title":"Method II - Personal Server"},{"location":"notebooks/spatial_temporal/1_experiment_walkthrough/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Spatial-Temporal Experiment \u00b6 import sys , os from pyprojroot import here # sys.path.append(here) # standard python packages import xarray as xr import pandas as pd import numpy as np # from src.models.spatemp.train_models import Metrics # # esdc tools # from src.esdc.subset import select_pixel # from src.esdc.shape import ShapeFileExtract, rasterize # from esdc.transform import DensityCubes from tqdm import tqdm import matplotlib.pyplot as plt import cartopy import cartopy.crs as ccrs plt . style . use ([ 'fivethirtyeight' , 'seaborn-poster' ]) % matplotlib inline % load_ext autoreload % autoreload 2 --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) <ipython-input-1-9a645161431b> in <module> 1 import sys , os ----> 2 from pyprojroot import here 3 # sys.path.append(here) 4 5 # standard python packages ModuleNotFoundError : No module named 'pyprojroot' ! ls / media / disk / databases / ESDC / Cube_2019highColombiaCube_184x120x120.zarr Cube_2019highColombiaCube_1x3360x2760.zarr esdc-8d-0.083deg-184x270x270-2.0.0.zarr esdc-8d-0.083deg-1x2160x4320-2.0.0.zarr esdc-8d-0.25deg-184x90x90-2.0.0.zarr esdc-8d-0.25deg-1x720x1440-2.0.0.zarr 1. Get DataCubes \u00b6 filename = '/media/disk/databases/ESDC/esdc-8d-0.25deg-1x720x1440-2.0.0.zarr' datacube = xr . open_zarr ( filename ) lst_cube = datacube [[ 'soil_moisture' , 'land_surface_temperature' ]] lst_cube <xarray.Dataset> Dimensions: (lat: 720, lon: 1440, time: 1702) Coordinates: * lat (lat) float32 89.875 89.625 ... -89.625 -89.875 * time (time) datetime64[ns] 1980-01-05 ... 2016-12-30 * lon (lon) float32 -179.875 -179.625 ... 179.875 Data variables: soil_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> land_surface_temperature (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> Attributes: Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment: The ESDL team acknowledges all data providers! chunking: 1x720x1440 comment: none. contributor_name: Max Planck Institute for Biogeochemistry contributor_role: ESDL Science Lead creator_email: info@earthsystemdatalab.net creator_name: Brockmann Consult GmbH creator_url: www.earthsystemdatalab.net date_created: 17.12.2018 date_issued: 19.12.2018 date_modified: 17.12.2018 geospatial_lat_max: 89.75 geospatial_lat_min: -89.75 geospatial_lon_max: 179.75 geospatial_lon_min: -179.75 geospatial_resolution: 1/4deg history: - processing with esdl cube v0.1 (https://git... id: v2.0.0 institution: Brockmann Consult GmbH keywords: Earth Science, Geophysical Variables license: Please refer to individual variables naming_authority: Earth System Data Lab team processing_level: Level 4 project: ESA Earth System Data Lab publisher_email: info@earthsystemdatalab.net publisher_name: Brockmann Consult GmbH & Max Planck Institute ... publisher_url: www.brockmann-consult.de standard_name_vocabulary: CF-1.7 summary: This data set contains a data cube of Earth Sy... time_coverage_duration: P37Y time_coverage_end: 30.12.2016 time_coverage_resolution: P8D time_coverage_start: 05.01.1980 title: Earth System Data Cube 2. Select Region \u00b6 europe = lst_cube . sel ( lat = slice ( 71.5 , 35.5 ), lon = slice ( - 18.0 , 60.0 )) 3. Get Density Cubes \u00b6 spatial = 7 temporal = 1 # initialize minicuber minicuber = DensityCubes ( spatial_window = spatial , time_window = temporal , ) europe_df = minicuber . get_minicubes ( europe . land_surface_temperature ) europe_df . shape (5982624, 49) print ( 7 * 7 * 1 - 1 ) print ( 5 * 5 * 2 - 1 ) print ( 4 * 4 * 3 - 1 ) print ( 3 * 3 * 5 - 1 ) print ( 2 * 2 * 11 - 1 ) print ( 1 * 1 * 46 - 1 ) 48 49 47 44 43 45 europe_df . shape (2052734, 46) 4. ML Model Framework \u00b6 4.1 Preprocessing \u00b6 4.1.1 - Training and testing \u00b6 europe_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } var_x0 var_x1 var_x2 var_x3 var_x4 var_x5 var_x6 var_x7 var_x8 var_x9 ... var_x39 var_x40 var_x41 var_x42 var_x43 var_x44 var_x45 var_x46 var_x47 var_x48 time lat lon 2002-05-21 70.625 54.375 268.145142 267.553741 267.075653 266.539734 265.585785 266.500458 270.357666 269.060791 268.648926 268.229797 ... 269.760193 271.069000 271.488525 268.266052 269.794861 270.075409 271.263397 270.822144 271.262665 269.876068 54.625 267.553741 267.075653 266.539734 265.585785 266.500458 270.357666 269.116730 268.648926 268.229797 268.201996 ... 271.069000 271.488525 270.526123 269.794861 270.075409 271.263397 270.822144 271.262665 269.876068 267.898865 54.875 267.075653 266.539734 265.585785 266.500458 270.357666 269.116730 269.217926 268.229797 268.201996 268.216003 ... 271.488525 270.526123 266.060333 270.075409 271.263397 270.822144 271.262665 269.876068 267.898865 267.227875 55.125 266.539734 265.585785 266.500458 270.357666 269.116730 269.217926 268.337921 268.201996 268.216003 268.703064 ... 270.526123 266.060333 265.535248 271.263397 270.822144 271.262665 269.876068 267.898865 267.227875 267.252319 55.375 265.585785 266.500458 270.357666 269.116730 269.217926 268.337921 269.024597 268.216003 268.703064 268.308807 ... 266.060333 265.535248 267.518524 270.822144 271.262665 269.876068 267.898865 267.227875 267.252319 267.736053 5 rows \u00d7 49 columns y = europe_df . iloc [:, 0 ][:, np . newaxis ] X = europe_df . iloc [:, 1 :] d_dimensions = X . shape [ 1 ] 4.1.2 - Train-Test Split \u00b6 from sklearn.model_selection import train_test_split train_size = 1_000 random_state = 123 xtrain , xtest , ytrain , ytest = train_test_split ( X , y , train_size = train_size , random_state = random_state ) test_size = xtest . shape [ 0 ] 4.1.1 - Normalize \u00b6 from sklearn.preprocessing import StandardScaler # normalize inputs x_normalizer = StandardScaler ( with_mean = True , with_std = False ) xtrain_norm = x_normalizer . fit_transform ( xtrain ) xtest_norm = x_normalizer . transform ( xtest ) # remove mean outputs y_normalizer = StandardScaler ( with_std = False ) ytrain_norm = y_normalizer . fit_transform ( ytrain ) ytest_norm = y_normalizer . transform ( ytest ) 4.2 - Training \u00b6 from gpy.sparse import SparseGPR import GPy # gp params n_dims = xtrain_norm . shape [ 1 ] kernel = GPy . kern . RBF ( input_dim = n_dims , ARD = False ) inference = 'vfe' n_inducing = 300 verbose = 1 max_iters = 5_000 n_restarts = 0 # initialize GP Model sgp_model = SparseGPR ( kernel = kernel , inference = inference , n_inducing = n_inducing , verbose = verbose , max_iters = max_iters , n_restarts = n_restarts ) # train GP model sgp_model . fit ( xtrain_norm , ytrain_norm ) var element = $('#e20c4c61-70b6-481a-b543-c7d739756fc3'); {\"model_id\": \"5177959d6e784819826fd52f757d39ae\", \"version_major\": 2, \"version_minor\": 0} SparseGPR(alpha=0.5, inference='vfe', kernel=<GPy.kern.src.rbf.RBF object at 0x7f17d6d41780>, max_iters=5000, n_inducing=300, n_restarts=0, optimizer='scg', verbose=1) sgp_model . display_model () .pd{ font-family: \"Courier New\", Courier, monospace !important; width: 100%; padding: 3px; } Model : sparse_gp Objective : 4313.986904027843 Number of Parameters : 14403 Number of Optimization Parameters : 14403 Updates : True .tg {font-family:\"Courier New\", Courier, monospace !important;padding:2px 3px;word-break:normal;border-collapse:collapse;border-spacing:0;border-color:#DCDCDC;margin:0px auto;width:100%;} .tg td{font-family:\"Courier New\", Courier, monospace !important;font-weight:bold;color:#444;background-color:#F7FDFA;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;} .tg th{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;color:#fff;background-color:#26ADE4;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;} .tg .tg-left{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:left;} .tg .tg-center{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:center;} .tg .tg-right{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:right;} sparse_gp. value constraints priors inducing inputs (300, 48) rbf.variance 2.1394880780812098e-16 +ve rbf.lengthscale 0.8861813022707942 +ve Gaussian_noise.variance 326.94648917027007 +ve 4.3 - Testing \u00b6 ypred = sgp_model . predict ( xtest_norm , return_std = False ) ypred . shape , ytest_norm . shape ((5981624, 1), (5981624, 1)) stats = Metrics () . get_all ( ypred . squeeze (), ytest_norm . squeeze ()) stats .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse r2 0 15.522687 338.304949 18.393068 -0.000268 stats [ 'r2' ] . values array([-0.00026803]) def _predict ( model , Xs , batch_size ): ms = [] n = max ( len ( Xs ) / batch_size , 1 ) # predict in small batches with tqdm ( np . array_split ( Xs , n )) as bar : for xs in bar : m = model . predict ( xs ,) ms . append ( m ) return np . vstack ( ms ) batch_size = 5_000 ms = [] n = max ( len ( xtest_norm ) / batch_size , 1 ) # predict in small batches with tqdm ( np . array_split ( xtest_norm , n )) as bar : for xs in bar : m = sgp_model . predict ( xs ,) ms . append ( m ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 598/598 [00:51<00:00, 11.56it/s] np . vstack ( ms ) . shape (5981624, 1) ypred = _predict ( sgp_model , xtest_norm , 5_000 ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1196/1196 [00:44<00:00, 27.05it/s] --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-82-585df2283be9> in <module> ----> 1 ypred = _predict ( sgp_model , xtest_norm , 5_000 ) <ipython-input-81-0f411118c22e> in _predict (model, Xs, batch_size) 7 ms . append ( m ) 8 ----> 9 return np . concatenate ( ms , 1 ) <__array_function__ internals> in concatenate (*args, **kwargs) ValueError : all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 5002 and the array at index 428 has size 5001 ypred . shape","title":"1 experiment walkthrough"},{"location":"notebooks/spatial_temporal/1_experiment_walkthrough/#spatial-temporal-experiment","text":"import sys , os from pyprojroot import here # sys.path.append(here) # standard python packages import xarray as xr import pandas as pd import numpy as np # from src.models.spatemp.train_models import Metrics # # esdc tools # from src.esdc.subset import select_pixel # from src.esdc.shape import ShapeFileExtract, rasterize # from esdc.transform import DensityCubes from tqdm import tqdm import matplotlib.pyplot as plt import cartopy import cartopy.crs as ccrs plt . style . use ([ 'fivethirtyeight' , 'seaborn-poster' ]) % matplotlib inline % load_ext autoreload % autoreload 2 --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) <ipython-input-1-9a645161431b> in <module> 1 import sys , os ----> 2 from pyprojroot import here 3 # sys.path.append(here) 4 5 # standard python packages ModuleNotFoundError : No module named 'pyprojroot' ! ls / media / disk / databases / ESDC / Cube_2019highColombiaCube_184x120x120.zarr Cube_2019highColombiaCube_1x3360x2760.zarr esdc-8d-0.083deg-184x270x270-2.0.0.zarr esdc-8d-0.083deg-1x2160x4320-2.0.0.zarr esdc-8d-0.25deg-184x90x90-2.0.0.zarr esdc-8d-0.25deg-1x720x1440-2.0.0.zarr","title":"Spatial-Temporal Experiment"},{"location":"notebooks/spatial_temporal/1_experiment_walkthrough/#1-get-datacubes","text":"filename = '/media/disk/databases/ESDC/esdc-8d-0.25deg-1x720x1440-2.0.0.zarr' datacube = xr . open_zarr ( filename ) lst_cube = datacube [[ 'soil_moisture' , 'land_surface_temperature' ]] lst_cube <xarray.Dataset> Dimensions: (lat: 720, lon: 1440, time: 1702) Coordinates: * lat (lat) float32 89.875 89.625 ... -89.625 -89.875 * time (time) datetime64[ns] 1980-01-05 ... 2016-12-30 * lon (lon) float32 -179.875 -179.625 ... 179.875 Data variables: soil_moisture (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> land_surface_temperature (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> Attributes: Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment: The ESDL team acknowledges all data providers! chunking: 1x720x1440 comment: none. contributor_name: Max Planck Institute for Biogeochemistry contributor_role: ESDL Science Lead creator_email: info@earthsystemdatalab.net creator_name: Brockmann Consult GmbH creator_url: www.earthsystemdatalab.net date_created: 17.12.2018 date_issued: 19.12.2018 date_modified: 17.12.2018 geospatial_lat_max: 89.75 geospatial_lat_min: -89.75 geospatial_lon_max: 179.75 geospatial_lon_min: -179.75 geospatial_resolution: 1/4deg history: - processing with esdl cube v0.1 (https://git... id: v2.0.0 institution: Brockmann Consult GmbH keywords: Earth Science, Geophysical Variables license: Please refer to individual variables naming_authority: Earth System Data Lab team processing_level: Level 4 project: ESA Earth System Data Lab publisher_email: info@earthsystemdatalab.net publisher_name: Brockmann Consult GmbH & Max Planck Institute ... publisher_url: www.brockmann-consult.de standard_name_vocabulary: CF-1.7 summary: This data set contains a data cube of Earth Sy... time_coverage_duration: P37Y time_coverage_end: 30.12.2016 time_coverage_resolution: P8D time_coverage_start: 05.01.1980 title: Earth System Data Cube","title":"1. Get DataCubes"},{"location":"notebooks/spatial_temporal/1_experiment_walkthrough/#2-select-region","text":"europe = lst_cube . sel ( lat = slice ( 71.5 , 35.5 ), lon = slice ( - 18.0 , 60.0 ))","title":"2. Select Region"},{"location":"notebooks/spatial_temporal/1_experiment_walkthrough/#3-get-density-cubes","text":"spatial = 7 temporal = 1 # initialize minicuber minicuber = DensityCubes ( spatial_window = spatial , time_window = temporal , ) europe_df = minicuber . get_minicubes ( europe . land_surface_temperature ) europe_df . shape (5982624, 49) print ( 7 * 7 * 1 - 1 ) print ( 5 * 5 * 2 - 1 ) print ( 4 * 4 * 3 - 1 ) print ( 3 * 3 * 5 - 1 ) print ( 2 * 2 * 11 - 1 ) print ( 1 * 1 * 46 - 1 ) 48 49 47 44 43 45 europe_df . shape (2052734, 46)","title":"3. Get Density Cubes"},{"location":"notebooks/spatial_temporal/1_experiment_walkthrough/#4-ml-model-framework","text":"","title":"4. ML Model Framework"},{"location":"notebooks/spatial_temporal/1_experiment_walkthrough/#41-preprocessing","text":"","title":"4.1 Preprocessing"},{"location":"notebooks/spatial_temporal/1_experiment_walkthrough/#411-training-and-testing","text":"europe_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } var_x0 var_x1 var_x2 var_x3 var_x4 var_x5 var_x6 var_x7 var_x8 var_x9 ... var_x39 var_x40 var_x41 var_x42 var_x43 var_x44 var_x45 var_x46 var_x47 var_x48 time lat lon 2002-05-21 70.625 54.375 268.145142 267.553741 267.075653 266.539734 265.585785 266.500458 270.357666 269.060791 268.648926 268.229797 ... 269.760193 271.069000 271.488525 268.266052 269.794861 270.075409 271.263397 270.822144 271.262665 269.876068 54.625 267.553741 267.075653 266.539734 265.585785 266.500458 270.357666 269.116730 268.648926 268.229797 268.201996 ... 271.069000 271.488525 270.526123 269.794861 270.075409 271.263397 270.822144 271.262665 269.876068 267.898865 54.875 267.075653 266.539734 265.585785 266.500458 270.357666 269.116730 269.217926 268.229797 268.201996 268.216003 ... 271.488525 270.526123 266.060333 270.075409 271.263397 270.822144 271.262665 269.876068 267.898865 267.227875 55.125 266.539734 265.585785 266.500458 270.357666 269.116730 269.217926 268.337921 268.201996 268.216003 268.703064 ... 270.526123 266.060333 265.535248 271.263397 270.822144 271.262665 269.876068 267.898865 267.227875 267.252319 55.375 265.585785 266.500458 270.357666 269.116730 269.217926 268.337921 269.024597 268.216003 268.703064 268.308807 ... 266.060333 265.535248 267.518524 270.822144 271.262665 269.876068 267.898865 267.227875 267.252319 267.736053 5 rows \u00d7 49 columns y = europe_df . iloc [:, 0 ][:, np . newaxis ] X = europe_df . iloc [:, 1 :] d_dimensions = X . shape [ 1 ]","title":"4.1.1 - Training and testing"},{"location":"notebooks/spatial_temporal/1_experiment_walkthrough/#412-train-test-split","text":"from sklearn.model_selection import train_test_split train_size = 1_000 random_state = 123 xtrain , xtest , ytrain , ytest = train_test_split ( X , y , train_size = train_size , random_state = random_state ) test_size = xtest . shape [ 0 ]","title":"4.1.2 - Train-Test Split"},{"location":"notebooks/spatial_temporal/1_experiment_walkthrough/#411-normalize","text":"from sklearn.preprocessing import StandardScaler # normalize inputs x_normalizer = StandardScaler ( with_mean = True , with_std = False ) xtrain_norm = x_normalizer . fit_transform ( xtrain ) xtest_norm = x_normalizer . transform ( xtest ) # remove mean outputs y_normalizer = StandardScaler ( with_std = False ) ytrain_norm = y_normalizer . fit_transform ( ytrain ) ytest_norm = y_normalizer . transform ( ytest )","title":"4.1.1 - Normalize"},{"location":"notebooks/spatial_temporal/1_experiment_walkthrough/#42-training","text":"from gpy.sparse import SparseGPR import GPy # gp params n_dims = xtrain_norm . shape [ 1 ] kernel = GPy . kern . RBF ( input_dim = n_dims , ARD = False ) inference = 'vfe' n_inducing = 300 verbose = 1 max_iters = 5_000 n_restarts = 0 # initialize GP Model sgp_model = SparseGPR ( kernel = kernel , inference = inference , n_inducing = n_inducing , verbose = verbose , max_iters = max_iters , n_restarts = n_restarts ) # train GP model sgp_model . fit ( xtrain_norm , ytrain_norm ) var element = $('#e20c4c61-70b6-481a-b543-c7d739756fc3'); {\"model_id\": \"5177959d6e784819826fd52f757d39ae\", \"version_major\": 2, \"version_minor\": 0} SparseGPR(alpha=0.5, inference='vfe', kernel=<GPy.kern.src.rbf.RBF object at 0x7f17d6d41780>, max_iters=5000, n_inducing=300, n_restarts=0, optimizer='scg', verbose=1) sgp_model . display_model () .pd{ font-family: \"Courier New\", Courier, monospace !important; width: 100%; padding: 3px; } Model : sparse_gp Objective : 4313.986904027843 Number of Parameters : 14403 Number of Optimization Parameters : 14403 Updates : True .tg {font-family:\"Courier New\", Courier, monospace !important;padding:2px 3px;word-break:normal;border-collapse:collapse;border-spacing:0;border-color:#DCDCDC;margin:0px auto;width:100%;} .tg td{font-family:\"Courier New\", Courier, monospace !important;font-weight:bold;color:#444;background-color:#F7FDFA;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;} .tg th{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;color:#fff;background-color:#26ADE4;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#DCDCDC;} .tg .tg-left{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:left;} .tg .tg-center{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:center;} .tg .tg-right{font-family:\"Courier New\", Courier, monospace !important;font-weight:normal;text-align:right;} sparse_gp. value constraints priors inducing inputs (300, 48) rbf.variance 2.1394880780812098e-16 +ve rbf.lengthscale 0.8861813022707942 +ve Gaussian_noise.variance 326.94648917027007 +ve","title":"4.2 - Training"},{"location":"notebooks/spatial_temporal/1_experiment_walkthrough/#43-testing","text":"ypred = sgp_model . predict ( xtest_norm , return_std = False ) ypred . shape , ytest_norm . shape ((5981624, 1), (5981624, 1)) stats = Metrics () . get_all ( ypred . squeeze (), ytest_norm . squeeze ()) stats .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse r2 0 15.522687 338.304949 18.393068 -0.000268 stats [ 'r2' ] . values array([-0.00026803]) def _predict ( model , Xs , batch_size ): ms = [] n = max ( len ( Xs ) / batch_size , 1 ) # predict in small batches with tqdm ( np . array_split ( Xs , n )) as bar : for xs in bar : m = model . predict ( xs ,) ms . append ( m ) return np . vstack ( ms ) batch_size = 5_000 ms = [] n = max ( len ( xtest_norm ) / batch_size , 1 ) # predict in small batches with tqdm ( np . array_split ( xtest_norm , n )) as bar : for xs in bar : m = sgp_model . predict ( xs ,) ms . append ( m ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 598/598 [00:51<00:00, 11.56it/s] np . vstack ( ms ) . shape (5981624, 1) ypred = _predict ( sgp_model , xtest_norm , 5_000 ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1196/1196 [00:44<00:00, 27.05it/s] --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-82-585df2283be9> in <module> ----> 1 ypred = _predict ( sgp_model , xtest_norm , 5_000 ) <ipython-input-81-0f411118c22e> in _predict (model, Xs, batch_size) 7 ms . append ( m ) 8 ----> 9 return np . concatenate ( ms , 1 ) <__array_function__ internals> in concatenate (*args, **kwargs) ValueError : all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 5002 and the array at index 428 has size 5001 ypred . shape","title":"4.3 - Testing"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Methods \u00b6 In this notebook, I will be walking through how we can estimate different methods based on the density cubes that we derive. import sys , os from pyprojroot import here root = here ( project_files = [ \".here\" ]) sys . path . append ( str ( here ())) import pathlib # standard python packages import xarray as xr import pandas as pd import numpy as np from xcube.core.geom import clip_dataset_by_geometry # from src.data.esdc import get_dataset from src.features import Metrics from src.features.temporal import select_period from src.features.spatial import select_region , get_europe # # esdc tools # from src.esdc.subset import select_pixel # from src.esdc.shape import ShapeFileExtract, rasterize # from esdc.transform import DensityCubes from tqdm import tqdm import cartopy import cartopy.crs as ccrs # NUMPY SETTINGS import numpy as onp onp . set_printoptions ( precision = 3 , suppress = True ) # MATPLOTLIB Settings import matplotlib as mpl import matplotlib.pyplot as plt % matplotlib inline % config InlineBackend . figure_format = 'retina' # SEABORN SETTINGS import seaborn as sns sns . set_context ( context = 'talk' , font_scale = 0.7 ) # sns.set(rc={'figure.figsize': (12, 9.)}) # sns.set_style(\"whitegrid\") # PANDAS SETTINGS import pandas as pd pd . set_option ( \"display.max_rows\" , 120 ) pd . set_option ( \"display.max_columns\" , 120 ) # LOGGING SETTINGS import sys import logging logging . basicConfig ( level = logging . INFO , stream = sys . stdout , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) logger = logging . getLogger () #logger.setLevel(logging.INFO) % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload 1. Get DataCubes \u00b6 ! ls / media / disk / databases / ESDC / Cube_2019highColombiaCube_184x120x120.zarr Cube_2019highColombiaCube_1x3360x2760.zarr esdc-8d-0.083deg-184x270x270-2.0.0.zarr esdc-8d-0.083deg-1x2160x4320-2.0.0.zarr esdc-8d-0.25deg-184x90x90-2.0.0.zarr esdc-8d-0.25deg-1x720x1440-2.0.0.zarr # Datapath DATA_PATH = pathlib . Path ( \"/media/disk/databases/ESDC/\" ) # get filename filename = DATA_PATH . joinpath ( \"esdc-8d-0.25deg-1x720x1440-2.0.0.zarr\" ) from typing import List import xarray as xr def get_dataset ( variable : str ) -> xr . Dataset : return xr . open_zarr ( str ( filename ))[[ variable ]] variable = 'gross_primary_productivity' datacube = get_dataset ( variable ) datacube Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: lat : 720 lon : 1440 time : 1702 Coordinates: (3) lon (lon) float32 -179.875 -179.625 ... 179.875 array([-179.875, -179.625, -179.375, ..., 179.375, 179.625, 179.875], dtype=float32) lat (lat) float32 89.875 89.625 ... -89.625 -89.875 array([ 89.875, 89.625, 89.375, ..., -89.375, -89.625, -89.875], dtype=float32) time (time) datetime64[ns] 1980-01-05 ... 2016-12-30 bounds : time_bnds long_name : time standard_name : time array(['1980-01-05T00:00:00.000000000', '1980-01-13T00:00:00.000000000', '1980-01-21T00:00:00.000000000', ..., '2016-12-14T00:00:00.000000000', '2016-12-22T00:00:00.000000000', '2016-12-30T00:00:00.000000000'], dtype='datetime64[ns]') Data variables: (1) gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 Attributes: (35) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 17.12.2018 geospatial_lat_max : 89.75 geospatial_lat_min : -89.75 geospatial_lon_max : 179.75 geospatial_lon_min : -179.75 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube 2. Select Region \u00b6 For this task, we are going to do something simple: work with only Europe and a segment of Eurasia. I have outlined a region described the latitude and longitude coordintes. With these coordinates, we can subset a section of the cube and continue working with that region only. from src.features.temporal import select_period from src.features.spatial import select_region --------------------------------------------------------------------------- ImportError Traceback (most recent call last) <ipython-input-31-464a6a005cc2> in <module> 1 from src . features . temporal import select_period ----> 2 from src . features . spatial import select_region ImportError : cannot import name 'select_region' from 'src.features.spatial' (/home/emmanuel/projects/2020_rbig_rs/src/features/spatial.py) # get european bounding box europe_bbox = get_europe () time_period = ( 'July-2010' , 'July-2010' ) # subset region europe_datacube = subset_cube ( datacube , europe_bbox ) # subset region europe_datacube_201007 = select_period ( europe_datacube , time_period ) europe_datacube_201007 Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: lat : 144 lon : 312 time : 3 Coordinates: (3) lon (lon) float32 -17.875 -17.625 ... 59.625 59.875 array([-17.875, -17.625, -17.375, ..., 59.375, 59.625, 59.875], dtype=float32) lat (lat) float32 71.625 71.375 ... 36.125 35.875 array([71.625, 71.375, 71.125, 70.875, 70.625, 70.375, 70.125, 69.875, 69.625, 69.375, 69.125, 68.875, 68.625, 68.375, 68.125, 67.875, 67.625, 67.375, 67.125, 66.875, 66.625, 66.375, 66.125, 65.875, 65.625, 65.375, 65.125, 64.875, 64.625, 64.375, 64.125, 63.875, 63.625, 63.375, 63.125, 62.875, 62.625, 62.375, 62.125, 61.875, 61.625, 61.375, 61.125, 60.875, 60.625, 60.375, 60.125, 59.875, 59.625, 59.375, 59.125, 58.875, 58.625, 58.375, 58.125, 57.875, 57.625, 57.375, 57.125, 56.875, 56.625, 56.375, 56.125, 55.875, 55.625, 55.375, 55.125, 54.875, 54.625, 54.375, 54.125, 53.875, 53.625, 53.375, 53.125, 52.875, 52.625, 52.375, 52.125, 51.875, 51.625, 51.375, 51.125, 50.875, 50.625, 50.375, 50.125, 49.875, 49.625, 49.375, 49.125, 48.875, 48.625, 48.375, 48.125, 47.875, 47.625, 47.375, 47.125, 46.875, 46.625, 46.375, 46.125, 45.875, 45.625, 45.375, 45.125, 44.875, 44.625, 44.375, 44.125, 43.875, 43.625, 43.375, 43.125, 42.875, 42.625, 42.375, 42.125, 41.875, 41.625, 41.375, 41.125, 40.875, 40.625, 40.375, 40.125, 39.875, 39.625, 39.375, 39.125, 38.875, 38.625, 38.375, 38.125, 37.875, 37.625, 37.375, 37.125, 36.875, 36.625, 36.375, 36.125, 35.875], dtype=float32) time (time) datetime64[ns] 2010-07-08 2010-07-16 2010-07-24 bounds : time_bnds long_name : time standard_name : time array(['2010-07-08T00:00:00.000000000', '2010-07-16T00:00:00.000000000', '2010-07-24T00:00:00.000000000'], dtype='datetime64[ns]') Data variables: (1) gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 144, 312), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 539.14 kB 179.71 kB Shape (3, 144, 312) (1, 144, 312) Count 3408 Tasks 3 Chunks Type float32 numpy.ndarray 312 144 3 Attributes: (39) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 2020-05-01T10:00:44.942841 geospatial_lat_max : 71.75 geospatial_lat_min : 35.75 geospatial_lon_max : 60.0 geospatial_lon_min : -18.0 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube geospatial_lon_units : degrees_east geospatial_lon_resolution : 0.25 geospatial_lat_units : degrees_north geospatial_lat_resolution : 0.25 europe_datacube_201007 . gross_primary_productivity . mean ( 'time' ) . plot ( vmin = 0 , robust = True ) /home/emmanuel/.conda/envs/rbig_eo/lib/python3.8/site-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide x = np.divide(x1, x2, out) <matplotlib.collections.QuadMesh at 0x7f5113a06070> 3. Get Density Cubes \u00b6 Now, we are going to create some density cubes. Instead of just taking the entire amount of samples, we are going to actually construct features. These features will be the neighbouring pixels in a spatial-temporal manner. For this demonstration, we will assume that the pixels from src.features.preprocessing import DensityCubes def get_density_cubes ( data : xr . Dataset , spatial : int , temporal : int ) -> Tuple [ str , pd . DataFrame ]: \"\"\"Wrapper Function to get density cubes from a dataarray\"\"\" for ikey , idata in data . items (): yield ikey , DensityCubes ( spatial_window = spatial , time_window = temporal ) . get_minicubes ( idata ) # All samples europe_df = europe_datacube_201007 . to_dataframe () . dropna () # reorder index levels = [ 'time' , 'lon' , 'lat' ] europe_df = europe_df . reorder_levels ( levels ) # europe_df = europe_df[indx] europe_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } gross_primary_productivity time lon lat 2010-07-08 51.625 71.625 1.747494 2010-07-16 51.625 71.625 1.073725 2010-07-24 51.625 71.625 1.334421 2010-07-08 51.875 71.625 1.201953 2010-07-16 51.875 71.625 0.647814 spatial = 1 temporal = 3 ivar , europe_temp_df = next ( get_density_cubes ( europe_datacube_201007 , spatial = spatial , temporal = temporal )) levels = [ 'time' , 'lon' , 'lat' ] europe_temp_df = europe_temp_df . reorder_levels ( levels ) europe_temp_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } var_x0 var_x1 var_x2 time lon lat 2010-07-24 51.625 71.625 1.747494 1.073725 1.334421 51.875 71.625 1.201953 0.647814 0.841013 52.125 71.625 1.158344 0.755085 0.847133 52.375 71.625 1.320013 0.837758 0.829512 52.625 71.625 1.087877 0.642634 0.499693 levels = [ 'time' , 'lon' , 'lat' ] idx = europe_temp_df . index . intersection ( europe_df . index ) idx . shape , ((26694,),) X_df = europe_df . loc [ idx ,:] Y_df = europe_temp_df . loc [ idx ,:] X_df . shape , Y_df . shape ((26694, 1), (26694, 3)) 4.1 Models Framework \u00b6 4.1 Preprocessing \u00b6 4.1.1 - Training and testing \u00b6 europe_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } gross_primary_productivity time lon lat 2010-07-08 51.625 71.625 1.747494 2010-07-16 51.625 71.625 1.073725 2010-07-24 51.625 71.625 1.334421 2010-07-08 51.875 71.625 1.201953 2010-07-16 51.875 71.625 0.647814 y = europe_df . iloc [:, 0 ][:, np . newaxis ] X = europe_df . iloc [:, 1 :] d_dimensions = X . shape [ 1 ] 4.1.2 - Train-Test Split \u00b6 from sklearn.model_selection import train_test_split train_size = 2_000 random_state = 123 xtrain , xtest , ytrain , ytest = train_test_split ( X , y , train_size = train_size , random_state = random_state ) test_size = xtest . shape [ 0 ] 4.1.1 - Normalize \u00b6 from sklearn.preprocessing import StandardScaler # normalize inputs x_normalizer = StandardScaler ( with_mean = True , with_std = False ) xtrain_norm = x_normalizer . fit_transform ( xtrain ) xtest_norm = x_normalizer . transform ( xtest ) # remove mean outputs y_normalizer = StandardScaler ( with_std = False ) ytrain_norm = y_normalizer . fit_transform ( ytrain ) ytest_norm = y_normalizer . transform ( ytest ) 4.2 - Training \u00b6 # from src.models.gp import SparseGPR import GPy from scipy.cluster.vq import kmeans2 # Kernel Function (RBF) n_dims = xtrain_norm . shape [ 1 ] kernel = GPy . kern . RBF ( input_dim = n_dims , ARD = False ) # Inducing Points n_inducing = 100 z = kmeans2 ( X , n_inducing , minit = \"points\" )[ 0 ] # Initialize GP Model gp_model = GPy . models . SparseGPRegression ( X , y , kernel = kernel , Z = z ) # choose VFE inference method gp_model . inference_method = ( GPy . inference . latent_function_inference . VarDTC ()) # fix variance to be low in the beginning gp_model . Gaussian_noise . variance = 0.01 2020-04-30 19:40:34,207:INFO:initializing Y 2020-04-30 19:40:34,209:INFO:initializing inference method 2020-04-30 19:40:34,209:INFO:adding kernel and likelihood as parameters 2020-04-30 19:40:34,211:INFO:Adding Z as parameter # optimize GP Model n_restarts = 0 verbose = 1 max_iters = 1_000 # optimize gp_model . optimize ( optimizer = 'scg' , messages = verbose , max_iters = max_iters , ); var element = $('#9b65a86b-ab6d-4c00-8f34-356e74c04ecc'); {\"model_id\": \"9d0ea3598cb24f87bd592c6260c61c8c\", \"version_major\": 2, \"version_minor\": 0} --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/model.py in optimize (self, optimizer, start, messages, max_iters, ipython_notebook, clear_after_finish, **kwargs) 110 with VerboseOptimization ( self , opt , maxiters = max_iters , verbose = messages , ipython_notebook = ipython_notebook , clear_after_finish = clear_after_finish ) as vo : --> 111 opt . run ( start , f_fp = self . _objective_grads , f = self . _objective , fp = self . _grads ) 112 ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/optimization/optimization.py in run (self, x_init, **kwargs) 50 start = dt . datetime . now ( ) ---> 51 self . opt ( x_init , ** kwargs ) 52 end = dt . datetime . now ( ) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/optimization/optimization.py in opt (self, x_init, f_fp, f, fp) 233 --> 234 opt_result = SCG(f, fp, x_init, 235 maxiters = self . max_iters , ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/optimization/scg.py in SCG (f, gradf, x, optargs, maxiters, max_f_eval, xtol, ftol, gtol) 112 xnew = x + alpha * d --> 113 fnew = f ( xnew , * optargs ) 114 function_eval += 1 ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/model.py in _objective (self, x) 260 try : --> 261 self . optimizer_array = x 262 obj = self . objective_function ( ) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/parameterized.py in __setattr__ (self, name, val) 338 param [ : ] = val ; return --> 339 return object . __setattr__ ( self , name , val ) 340 ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/core/sparse_gp_mpi.py in optimizer_array (self, p) 87 self . mpi_comm . Bcast ( p , root = 0 ) ---> 88 SparseGP . optimizer_array . fset ( self , p ) 89 ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/core/parameter_core.py in optimizer_array (self, p) 123 self . _optimizer_copy_transformed = False --> 124 self . trigger_update ( ) 125 ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/core/updateable.py in trigger_update (self, trigger_parent) 78 return ---> 79 self . _trigger_params_changed ( trigger_parent ) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/core/parameter_core.py in _trigger_params_changed (self, trigger_parent) 133 [ p . _trigger_params_changed ( trigger_parent = False ) for p in self . parameters if not p . is_fixed ] --> 134 self . notify_observers ( None , None if trigger_parent else - np . inf ) 135 ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/core/observable.py in notify_observers (self, which, min_priority) 90 if min_priority is None : ---> 91 [ callble ( self , which = which ) for _ , _ , callble in self . observers ] 92 else : ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/core/observable.py in <listcomp> (.0) 90 if min_priority is None : ---> 91 [ callble ( self , which = which ) for _ , _ , callble in self . observers ] 92 else : ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/core/parameter_core.py in _parameters_changed_notification (self, me, which) 507 self . _optimizer_copy_transformed = False # tells the optimizer array to update on next request --> 508 self . parameters_changed ( ) 509 def _pass_through_notify_observers ( self , me , which = None ) : ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/models/sparse_gp_regression.py in parameters_changed (self) 65 else : ---> 66 super ( SparseGPRegression , self ) . parameters_changed ( ) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/core/sparse_gp_mpi.py in parameters_changed (self) 121 else : --> 122 super ( SparseGP_MPI , self ) . parameters_changed ( ) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/core/sparse_gp.py in parameters_changed (self) 77 self . posterior , self . _log_marginal_likelihood , self . grad_dict = \\ ---> 78 self.inference_method.inference(self.kern, self.X, self.Z, self.likelihood, 79 self . Y_normalized , Y_metadata = self . Y_metadata , ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/inference/latent_function_inference/var_dtc.py in inference (self, kern, X, Z, likelihood, Y, Y_metadata, mean_function, precision, Lm, dL_dKmm, psi0, psi1, psi2, Z_tilde) 124 if psi1 is None : --> 125 psi1 = kern . K ( X , Z ) 126 if het_noise : ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/kern/src/kernel_slice_operations.py in wrap (self, X, X2, *a, **kw) 109 with _Slice_wrap ( self , X , X2 ) as s : --> 110 ret = f ( self , s . X , s . X2 , * a , ** kw ) 111 return ret <decorator-gen-150> in K (self, X, X2) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/caching.py in g (obj, *args, **kw) 282 cacher = cache [ self . f ] = Cacher ( self . f , self . limit , self . ignore_args , self . force_kwargs , cacher_enabled = cache . caching_enabled ) --> 283 return cacher ( * args , ** kw ) 284 g . __name__ = f . __name__ ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/caching.py in __call__ (self, *args, **kw) 178 try : --> 179 new_output = self . operation ( * args , ** kw ) 180 except : ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/kern/src/stationary.py in K (self, X, X2) 113 \"\"\" --> 114 r = self . _scaled_dist ( X , X2 ) 115 return self . K_of_r ( r ) <decorator-gen-153> in _scaled_dist (self, X, X2) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/caching.py in g (obj, *args, **kw) 282 cacher = cache [ self . f ] = Cacher ( self . f , self . limit , self . ignore_args , self . force_kwargs , cacher_enabled = cache . caching_enabled ) --> 283 return cacher ( * args , ** kw ) 284 g . __name__ = f . __name__ ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/caching.py in __call__ (self, *args, **kw) 178 try : --> 179 new_output = self . operation ( * args , ** kw ) 180 except : ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/kern/src/stationary.py in _scaled_dist (self, X, X2) 167 else : --> 168 return self . _unscaled_dist ( X , X2 ) / self . lengthscale 169 ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/kern/src/stationary.py in _unscaled_dist (self, X, X2) 147 r2 = np . clip ( r2 , 0 , np . inf ) --> 148 return np . sqrt ( r2 ) 149 KeyboardInterrupt : During handling of the above exception, another exception occurred: TypeError Traceback (most recent call last) <ipython-input-77-e519e2d9a69c> in <module> 5 6 # optimize ----> 7 gp_model.optimize( 8 optimizer = 'scg' , 9 messages = verbose , ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/core/sparse_gp_mpi.py in optimize (self, optimizer, start, **kwargs) 91 self . _IN_OPTIMIZATION_ = True 92 if self . mpi_comm == None : ---> 93 ret = super ( SparseGP_MPI , self ) . optimize ( optimizer , start , ** kwargs ) 94 elif self . mpi_comm . rank == 0 : 95 ret = super ( SparseGP_MPI , self ) . optimize ( optimizer , start , ** kwargs ) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/core/gp.py in optimize (self, optimizer, start, messages, max_iters, ipython_notebook, clear_after_finish, **kwargs) 673 self . inference_method . on_optimization_start ( ) 674 try : --> 675 ret = super ( GP , self ) . optimize ( optimizer , start , messages , max_iters , ipython_notebook , clear_after_finish , ** kwargs ) 676 except KeyboardInterrupt : 677 print ( \"KeyboardInterrupt caught, calling on_optimization_end() to round things up\" ) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/model.py in optimize (self, optimizer, start, messages, max_iters, ipython_notebook, clear_after_finish, **kwargs) 109 110 with VerboseOptimization ( self , opt , maxiters = max_iters , verbose = messages , ipython_notebook = ipython_notebook , clear_after_finish = clear_after_finish ) as vo : --> 111 opt . run ( start , f_fp = self . _objective_grads , f = self . _objective , fp = self . _grads ) 112 113 self . optimizer_array = opt . x_opt ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/optimization/verbose_optimization.py in __exit__ (self, type, value, traceback) 213 self . stop = time . time ( ) 214 self . model . remove_observer ( self ) --> 215 self . print_out ( self . stop - self . start ) 216 217 if not self . ipython_notebook : ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/optimization/verbose_optimization.py in print_out (self, seconds) 146 ['||gradient||', 147 \"{: >+12.3E}\".format(float(self.current_gradient))], --> 148 [ 'status' , \"{:s}\" . format ( self . status ) ] , 149 ] 150 #message = \"Lik:{:5.3E} Grad:{:5.3E} Lik:{:5.3E} Len:{!s}\".format(float(m.log_likelihood()), np.einsum('i,i->', grads, grads), float(m.likelihood.variance), \" \".join([\"{:3.2E}\".format(l) for l in m.kern.lengthscale.values])) TypeError : unsupported format string passed to NoneType.__format__ 4.3 - Testing \u00b6 ypred = gp_model . predict ( xtest_norm , )[ 0 ] ypred . shape , ytest_norm . shape ((67414, 1), (67414, 1)) stats = Metrics () . get_all ( ypred . squeeze (), ytest_norm . squeeze ()) stats .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse r2 0 1.584058 4.35029 2.085735 0.325761 def _predict ( model , Xs , batch_size ): ms = [] n = max ( len ( Xs ) / batch_size , 1 ) # predict in small batches with tqdm ( np . array_split ( Xs , n )) as bar : for xs in bar : m = model . predict ( xs ,) ms . append ( m ) return np . vstack ( ms ) batch_size = 5_000 ms = [] n = max ( len ( xtest_norm ) / batch_size , 1 ) # predict in small batches with tqdm ( np . array_split ( xtest_norm , n )) as bar : for xs in bar : m = sgp_model . predict ( xs ,) ms . append ( m ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 598/598 [00:51<00:00, 11.56it/s] 5. Direct Measurements \u00b6 5.1 - \\rho \\rho V Coefficient \u00b6 Now, we will explore the easiest linear method. It is the multi-dimensional version of the Pearson Correlation coefficient called the \\rho \\rho V-Coefficient ( \\rho \\rho -Vector Coefficient). Most people are familiar with the correlation coefficient: \\rho(X,Y) = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)}\\sqrt{\\text{Var}(Y)}} \\rho(X,Y) = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)}\\sqrt{\\text{Var}(Y)}} This is very well-known in the literature but it doesn't directly apply to multi-dimensional data. The final result of the numerator and the denominator is a scalar value per dimension. There is no way we can summarize all of the information into a single scalar. One extension we can do is to create a matrix with the pairwise components (i.e gram matrices) for each of the variables and then take the Frobenius norm (Hilbert-Schmidt norm) of the cross term as well as the individual terms. So the equation is like so: \\rho V (\\mathbf{X,Y}) = \\frac{\\left\\langle \\mathbf{XX^\\top, YY^\\top} \\right\\rangle_\\mathbf{F}} {\\sqrt{\\left\\langle \\mathbf{XX^\\top} \\right\\rangle_\\mathbf{F}} \\sqrt{\\left\\langle \\mathbf{YY^\\top} \\right\\rangle_\\mathbf{F}}} \\rho V (\\mathbf{X,Y}) = \\frac{\\left\\langle \\mathbf{XX^\\top, YY^\\top} \\right\\rangle_\\mathbf{F}} {\\sqrt{\\left\\langle \\mathbf{XX^\\top} \\right\\rangle_\\mathbf{F}} \\sqrt{\\left\\langle \\mathbf{YY^\\top} \\right\\rangle_\\mathbf{F}}} Note : This is very similar to HSIC and Centered Kernel Alignment (CKA) but this method dates back before. CKA generalizes this method with the addition of distance measures and non-linear kernel functions. We will explore this in the next section. To code this up, we will all of the components of this equation because we will need them later. from typing import Dict from sklearn.preprocessing import KernelCenterer from sklearn.metrics.pairwise import linear_kernel def rv_coefficient ( X : np . ndarray , Y : np . ndarray ) -> Dict : \"\"\"simple function to calculate the rv coefficient\"\"\" # calculate the kernel matrices X_gram = linear_kernel ( X ) Y_gram = linear_kernel ( Y ) # center the kernels X_gram = KernelCenterer () . fit_transform ( X_gram ) Y_gram = KernelCenterer () . fit_transform ( Y_gram ) # normalizing coefficients (denomenator) x_norm = np . linalg . norm ( X_gram ) y_norm = np . linalg . norm ( Y_gram ) # frobenius norm of the cross terms (numerator) xy_norm = np . sum ( X_gram * Y_gram ) # rv coefficient pv_coeff = xy_norm / x_norm / y_norm return { 'coefficient' : pv_coeff , 'x_norm' : x_norm , 'y_norm' : y_norm , 'xy_norm' : xy_norm } X_samples = europe_temp_df . iloc [:, 0 ][:, np . newaxis ] Y_samples = europe_temp_df logging . info ( f \" Size of X_samples: { X_samples . shape } , { Y_samples . shape } \" ) d_dimensions = X . shape [ 1 ] 2020-04-30 21:10:39,726:INFO: Size of X_samples: (26694, 1), (26694, 3) # check that the coefficient is 1 if the data is the same rv_coeff = rv_coefficient ( X_samples [: 100 ], X_samples [: 100 ]) np . testing . assert_almost_equal ( rv_coeff [ 'coefficient' ], 1 ) So now, let's try when we have some a difference between the variables. %% time rv_coeff = rv_coefficient ( X_samples [:], Y_samples [:]) rv_coeff CPU times: user 23.8 s, sys: 4.9 s, total: 28.7 s Wall time: 13.3 s {'coefficient': 0.9696304, 'x_norm': 191155.58, 'y_norm': 510949.53, 'xy_norm': 94704630000.0} 5.2 - Non-Linear Kernel \u00b6 An addition that we can do is to explore how the from typing import Optional from scipy.spatial.distance import pdist , squareform def estimate_sigma ( X : np . ndarray , method : str = 'median' , percent : Optional [ int ] = None , heuristic : bool = False ) -> float : # get the squared euclidean distances if method == 'silverman' : return silvermans_factor ( X ) elif method == 'scott' : return scotts_factor ( X ) elif percent is not None : kth_sample = int (( percent / 100 ) * X . shape [ 0 ]) dists = np . sort ( squareform ( pdist ( X , 'sqeuclidean' )))[:, kth_sample ] # print(dists.shape, dists.min(), dists.max()) else : dists = np . sort ( pdist ( X , 'sqeuclidean' )) # print(dists.shape, dists.min(), dists.max()) if method == 'median' : sigma = np . median ( dists ) elif method == 'mean' : sigma = np . mean ( dists ) else : raise ValueError ( f \"Unrecognized distance measure: { method } \" ) if heuristic : sigma = np . sqrt ( sigma / 2 ) return sigma from typing import Dict from sklearn.preprocessing import KernelCenterer from sklearn.gaussian_process.kernels import RBF def cka_coefficient ( X : np . ndarray , Y : np . ndarray ) -> Dict : \"\"\"simple function to calculate the rv coefficient\"\"\" # estimate sigmas sigma_X = estimate_sigma ( X , method = 'median' , percent = 50 ) sigma_Y = estimate_sigma ( Y , method = 'median' , percent = 50 ) # calculate the kernel matrices X_gram = RBF ( sigma_X )( X ) Y_gram = RBF ( sigma_Y )( Y ) # center the kernels X_gram = KernelCenterer () . fit_transform ( X_gram ) Y_gram = KernelCenterer () . fit_transform ( Y_gram ) # normalizing coefficients (denomenator) x_norm = np . linalg . norm ( X_gram ) y_norm = np . linalg . norm ( Y_gram ) # frobenius norm of the cross terms (numerator) xy_norm = np . sum ( X_gram * Y_gram ) # rv coefficient pv_coeff = xy_norm / x_norm / y_norm return { 'coefficient' : pv_coeff , 'x_norm' : x_norm , 'y_norm' : y_norm , 'xy_norm' : xy_norm } # check that the coefficient is 1 if the data is the same cka_coeff = cka_coefficient ( X_samples [: 100 ], X_samples [: 100 ]) np . testing . assert_almost_equal ( cka_coeff [ 'coefficient' ], 1 ) %% time cka_coeff = cka_coefficient ( X_samples [: 10_000 ], Y_samples [: 10_000 ]) cka_coeff CPU times: user 20.4 s, sys: 2.89 s, total: 23.3 s Wall time: 17 s {'coefficient': 0.9576703788185938, 'x_norm': 2326.331280441253, 'y_norm': 1175.2587321221124, 'xy_norm': 2618310.2249249523} Variation of Information \u00b6 from rbig.rbig import RBIGMI , RBIG rbig_results = {} def variation_of_info ( H_X , H_Y , I_XY ): return I_XY / np . sqrt ( H_X ) / np . sqrt ( H_Y ) %% time n_layers = 10000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 pdf_extension = None pdf_resolution = None tolerance = None # Initialize RBIG class H_rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance ) # fit model to the data rbig_results [ 'H_x' ] = H_rbig_model . fit ( X_samples ) . entropy ( correction = True ); rbig_results [ 'H_y' ] = H_rbig_model . fit ( Y_samples ) . entropy ( correction = True ); # Initialize RBIG class I_rbig_model = RBIGMI ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance ) # fit model to the data rbig_results [ 'I_xy' ] = I_rbig_model . fit ( X_samples , Y_samples ) . mutual_information (); # calculate the variation of information coefficient rbig_results [ 'coefficient' ] = variation_of_info ( rbig_results [ 'H_x' ], rbig_results [ 'H_y' ], rbig_results [ 'I_xy' ] ) CPU times: user 4min 12s, sys: 36.4 s, total: 4min 49s Wall time: 11.2 s rbig_results {'H_x': 3.2281169474002924, 'H_y': 5.181635094706355, 'I_xy': 5.412206453196236, 'coefficient': 1.3233243751128443}","title":"2.0 methods walkthrough"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/#methods","text":"In this notebook, I will be walking through how we can estimate different methods based on the density cubes that we derive. import sys , os from pyprojroot import here root = here ( project_files = [ \".here\" ]) sys . path . append ( str ( here ())) import pathlib # standard python packages import xarray as xr import pandas as pd import numpy as np from xcube.core.geom import clip_dataset_by_geometry # from src.data.esdc import get_dataset from src.features import Metrics from src.features.temporal import select_period from src.features.spatial import select_region , get_europe # # esdc tools # from src.esdc.subset import select_pixel # from src.esdc.shape import ShapeFileExtract, rasterize # from esdc.transform import DensityCubes from tqdm import tqdm import cartopy import cartopy.crs as ccrs # NUMPY SETTINGS import numpy as onp onp . set_printoptions ( precision = 3 , suppress = True ) # MATPLOTLIB Settings import matplotlib as mpl import matplotlib.pyplot as plt % matplotlib inline % config InlineBackend . figure_format = 'retina' # SEABORN SETTINGS import seaborn as sns sns . set_context ( context = 'talk' , font_scale = 0.7 ) # sns.set(rc={'figure.figsize': (12, 9.)}) # sns.set_style(\"whitegrid\") # PANDAS SETTINGS import pandas as pd pd . set_option ( \"display.max_rows\" , 120 ) pd . set_option ( \"display.max_columns\" , 120 ) # LOGGING SETTINGS import sys import logging logging . basicConfig ( level = logging . INFO , stream = sys . stdout , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) logger = logging . getLogger () #logger.setLevel(logging.INFO) % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"Methods"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/#1-get-datacubes","text":"! ls / media / disk / databases / ESDC / Cube_2019highColombiaCube_184x120x120.zarr Cube_2019highColombiaCube_1x3360x2760.zarr esdc-8d-0.083deg-184x270x270-2.0.0.zarr esdc-8d-0.083deg-1x2160x4320-2.0.0.zarr esdc-8d-0.25deg-184x90x90-2.0.0.zarr esdc-8d-0.25deg-1x720x1440-2.0.0.zarr # Datapath DATA_PATH = pathlib . Path ( \"/media/disk/databases/ESDC/\" ) # get filename filename = DATA_PATH . joinpath ( \"esdc-8d-0.25deg-1x720x1440-2.0.0.zarr\" ) from typing import List import xarray as xr def get_dataset ( variable : str ) -> xr . Dataset : return xr . open_zarr ( str ( filename ))[[ variable ]] variable = 'gross_primary_productivity' datacube = get_dataset ( variable ) datacube Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: lat : 720 lon : 1440 time : 1702 Coordinates: (3) lon (lon) float32 -179.875 -179.625 ... 179.875 array([-179.875, -179.625, -179.375, ..., 179.375, 179.625, 179.875], dtype=float32) lat (lat) float32 89.875 89.625 ... -89.625 -89.875 array([ 89.875, 89.625, 89.375, ..., -89.375, -89.625, -89.875], dtype=float32) time (time) datetime64[ns] 1980-01-05 ... 2016-12-30 bounds : time_bnds long_name : time standard_name : time array(['1980-01-05T00:00:00.000000000', '1980-01-13T00:00:00.000000000', '1980-01-21T00:00:00.000000000', ..., '2016-12-14T00:00:00.000000000', '2016-12-22T00:00:00.000000000', '2016-12-30T00:00:00.000000000'], dtype='datetime64[ns]') Data variables: (1) gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 720, 1440), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 7.06 GB 4.15 MB Shape (1702, 720, 1440) (1, 720, 1440) Count 1703 Tasks 1702 Chunks Type float32 numpy.ndarray 1440 720 1702 Attributes: (35) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 17.12.2018 geospatial_lat_max : 89.75 geospatial_lat_min : -89.75 geospatial_lon_max : 179.75 geospatial_lon_min : -179.75 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube","title":"1. Get DataCubes"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/#2-select-region","text":"For this task, we are going to do something simple: work with only Europe and a segment of Eurasia. I have outlined a region described the latitude and longitude coordintes. With these coordinates, we can subset a section of the cube and continue working with that region only. from src.features.temporal import select_period from src.features.spatial import select_region --------------------------------------------------------------------------- ImportError Traceback (most recent call last) <ipython-input-31-464a6a005cc2> in <module> 1 from src . features . temporal import select_period ----> 2 from src . features . spatial import select_region ImportError : cannot import name 'select_region' from 'src.features.spatial' (/home/emmanuel/projects/2020_rbig_rs/src/features/spatial.py) # get european bounding box europe_bbox = get_europe () time_period = ( 'July-2010' , 'July-2010' ) # subset region europe_datacube = subset_cube ( datacube , europe_bbox ) # subset region europe_datacube_201007 = select_period ( europe_datacube , time_period ) europe_datacube_201007 Show/Hide data repr Show/Hide attributes /* CSS stylesheet for displaying xarray objects in jupyterlab. * */ :root { --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1)); --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54)); --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38)); --xr-border-color: var(--jp-border-color2, #e0e0e0); --xr-disabled-color: var(--jp-layout-color3, #bdbdbd); --xr-background-color: var(--jp-layout-color0, white); --xr-background-color-row-even: var(--jp-layout-color1, white); --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee); } .xr-wrap { min-width: 300px; max-width: 700px; } .xr-header { padding-top: 6px; padding-bottom: 6px; margin-bottom: 4px; border-bottom: solid 1px var(--xr-border-color); } .xr-header > div, .xr-header > ul { display: inline; margin-top: 0; margin-bottom: 0; } .xr-obj-type, .xr-array-name { margin-left: 2px; margin-right: 10px; } .xr-obj-type { color: var(--xr-font-color2); } .xr-sections { padding-left: 0 !important; display: grid; grid-template-columns: 150px auto auto 1fr 20px 20px; } .xr-section-item { display: contents; } .xr-section-item input { display: none; } .xr-section-item input + label { color: var(--xr-disabled-color); } .xr-section-item input:enabled + label { cursor: pointer; color: var(--xr-font-color2); } .xr-section-item input:enabled + label:hover { color: var(--xr-font-color0); } .xr-section-summary { grid-column: 1; color: var(--xr-font-color2); font-weight: 500; } .xr-section-summary > span { display: inline-block; padding-left: 0.5em; } .xr-section-summary-in:disabled + label { color: var(--xr-font-color2); } .xr-section-summary-in + label:before { display: inline-block; content: '\u25ba'; font-size: 11px; width: 15px; text-align: center; } .xr-section-summary-in:disabled + label:before { color: var(--xr-disabled-color); } .xr-section-summary-in:checked + label:before { content: '\u25bc'; } .xr-section-summary-in:checked + label > span { display: none; } .xr-section-summary, .xr-section-inline-details { padding-top: 4px; padding-bottom: 4px; } .xr-section-inline-details { grid-column: 2 / -1; } .xr-section-details { display: none; grid-column: 1 / -1; margin-bottom: 5px; } .xr-section-summary-in:checked ~ .xr-section-details { display: contents; } .xr-array-wrap { grid-column: 1 / -1; display: grid; grid-template-columns: 20px auto; } .xr-array-wrap > label { grid-column: 1; vertical-align: top; } .xr-preview { color: var(--xr-font-color3); } .xr-array-preview, .xr-array-data { padding: 0 5px !important; grid-column: 2; } .xr-array-data, .xr-array-in:checked ~ .xr-array-preview { display: none; } .xr-array-in:checked ~ .xr-array-data, .xr-array-preview { display: inline-block; } .xr-dim-list { display: inline-block !important; list-style: none; padding: 0 !important; margin: 0; } .xr-dim-list li { display: inline-block; padding: 0; margin: 0; } .xr-dim-list:before { content: '('; } .xr-dim-list:after { content: ')'; } .xr-dim-list li:not(:last-child):after { content: ','; padding-right: 5px; } .xr-has-index { font-weight: bold; } .xr-var-list, .xr-var-item { display: contents; } .xr-var-item > div, .xr-var-item label, .xr-var-item > .xr-var-name span { background-color: var(--xr-background-color-row-even); margin-bottom: 0; } .xr-var-item > .xr-var-name:hover span { padding-right: 5px; } .xr-var-list > li:nth-child(odd) > div, .xr-var-list > li:nth-child(odd) > label, .xr-var-list > li:nth-child(odd) > .xr-var-name span { background-color: var(--xr-background-color-row-odd); } .xr-var-name { grid-column: 1; } .xr-var-dims { grid-column: 2; } .xr-var-dtype { grid-column: 3; text-align: right; color: var(--xr-font-color2); } .xr-var-preview { grid-column: 4; } .xr-var-name, .xr-var-dims, .xr-var-dtype, .xr-preview, .xr-attrs dt { white-space: nowrap; overflow: hidden; text-overflow: ellipsis; padding-right: 10px; } .xr-var-name:hover, .xr-var-dims:hover, .xr-var-dtype:hover, .xr-attrs dt:hover { overflow: visible; width: auto; z-index: 1; } .xr-var-attrs, .xr-var-data { display: none; background-color: var(--xr-background-color) !important; padding-bottom: 5px !important; } .xr-var-attrs-in:checked ~ .xr-var-attrs, .xr-var-data-in:checked ~ .xr-var-data { display: block; } .xr-var-data > table { float: right; } .xr-var-name span, .xr-var-data, .xr-attrs { padding-left: 25px !important; } .xr-attrs, .xr-var-attrs, .xr-var-data { grid-column: 1 / -1; } dl.xr-attrs { padding: 0; margin: 0; display: grid; grid-template-columns: 125px auto; } .xr-attrs dt, dd { padding: 0; margin: 0; float: left; padding-right: 10px; width: auto; } .xr-attrs dt { font-weight: normal; grid-column: 1; } .xr-attrs dt:hover span { display: inline-block; background: var(--xr-background-color); padding-right: 10px; } .xr-attrs dd { grid-column: 2; white-space: pre-wrap; word-break: break-all; } .xr-icon-database, .xr-icon-file-text2 { display: inline-block; vertical-align: middle; width: 1em; height: 1.5em !important; stroke-width: 0; stroke: currentColor; fill: currentColor; } xarray.Dataset Dimensions: lat : 144 lon : 312 time : 3 Coordinates: (3) lon (lon) float32 -17.875 -17.625 ... 59.625 59.875 array([-17.875, -17.625, -17.375, ..., 59.375, 59.625, 59.875], dtype=float32) lat (lat) float32 71.625 71.375 ... 36.125 35.875 array([71.625, 71.375, 71.125, 70.875, 70.625, 70.375, 70.125, 69.875, 69.625, 69.375, 69.125, 68.875, 68.625, 68.375, 68.125, 67.875, 67.625, 67.375, 67.125, 66.875, 66.625, 66.375, 66.125, 65.875, 65.625, 65.375, 65.125, 64.875, 64.625, 64.375, 64.125, 63.875, 63.625, 63.375, 63.125, 62.875, 62.625, 62.375, 62.125, 61.875, 61.625, 61.375, 61.125, 60.875, 60.625, 60.375, 60.125, 59.875, 59.625, 59.375, 59.125, 58.875, 58.625, 58.375, 58.125, 57.875, 57.625, 57.375, 57.125, 56.875, 56.625, 56.375, 56.125, 55.875, 55.625, 55.375, 55.125, 54.875, 54.625, 54.375, 54.125, 53.875, 53.625, 53.375, 53.125, 52.875, 52.625, 52.375, 52.125, 51.875, 51.625, 51.375, 51.125, 50.875, 50.625, 50.375, 50.125, 49.875, 49.625, 49.375, 49.125, 48.875, 48.625, 48.375, 48.125, 47.875, 47.625, 47.375, 47.125, 46.875, 46.625, 46.375, 46.125, 45.875, 45.625, 45.375, 45.125, 44.875, 44.625, 44.375, 44.125, 43.875, 43.625, 43.375, 43.125, 42.875, 42.625, 42.375, 42.125, 41.875, 41.625, 41.375, 41.125, 40.875, 40.625, 40.375, 40.125, 39.875, 39.625, 39.375, 39.125, 38.875, 38.625, 38.375, 38.125, 37.875, 37.625, 37.375, 37.125, 36.875, 36.625, 36.375, 36.125, 35.875], dtype=float32) time (time) datetime64[ns] 2010-07-08 2010-07-16 2010-07-24 bounds : time_bnds long_name : time standard_name : time array(['2010-07-08T00:00:00.000000000', '2010-07-16T00:00:00.000000000', '2010-07-24T00:00:00.000000000'], dtype='datetime64[ns]') Data variables: (1) gross_primary_productivity (time, lat, lon) float32 dask.array<chunksize=(1, 144, 312), meta=np.ndarray> ID : 47 esa_cci_path : nan long_name : Gross Primary Productivity orig_attrs : {'comment': 'Gross Carbon uptake of of the ecosystem through photosynthesis', 'long_name': 'Gross Primary Productivity', 'orig_attrs': {}, 'project_name': 'FLUXCOM', 'references': 'Tramontana, Gianluca, et al. \"Predicting carbon dioxide and energy fluxes across global FLUXNET sites with regression algorithms.\" (2016).', 'source_name': 'GPPall', 'standard_name': 'gross_primary_productivity_of_carbon', 'units': 'gC m-2 day-1', 'url': 'http://www.fluxcom.org/'} orig_version : v1 project_name : FLUXCOM time_coverage_end : 2015-12-31 time_coverage_resolution : P8D time_coverage_start : 2001-01-05 url : http://www.fluxcom.org/ Array Chunk Bytes 539.14 kB 179.71 kB Shape (3, 144, 312) (1, 144, 312) Count 3408 Tasks 3 Chunks Type float32 numpy.ndarray 312 144 3 Attributes: (39) Metadata_conventions : Unidata Dataset Discovery v1.0 acknowledgment : The ESDL team acknowledges all data providers! chunking : 1x720x1440 comment : none. contributor_name : Max Planck Institute for Biogeochemistry contributor_role : ESDL Science Lead creator_email : info@earthsystemdatalab.net creator_name : Brockmann Consult GmbH creator_url : www.earthsystemdatalab.net date_created : 17.12.2018 date_issued : 19.12.2018 date_modified : 2020-05-01T10:00:44.942841 geospatial_lat_max : 71.75 geospatial_lat_min : 35.75 geospatial_lon_max : 60.0 geospatial_lon_min : -18.0 geospatial_resolution : 1/4deg history : - processing with esdl cube v0.1 (https://github.com/esa-esdl/esdl-core/) id : v2.0.0 institution : Brockmann Consult GmbH keywords : Earth Science, Geophysical Variables license : Please refer to individual variables naming_authority : Earth System Data Lab team processing_level : Level 4 project : ESA Earth System Data Lab publisher_email : info@earthsystemdatalab.net publisher_name : Brockmann Consult GmbH & Max Planck Institute for Biogechemistry publisher_url : www.brockmann-consult.de standard_name_vocabulary : CF-1.7 summary : This data set contains a data cube of Earth System variables created by the ESA project Earth System Data Lab. time_coverage_duration : P37Y time_coverage_end : 30.12.2016 time_coverage_resolution : P8D time_coverage_start : 05.01.1980 title : Earth System Data Cube geospatial_lon_units : degrees_east geospatial_lon_resolution : 0.25 geospatial_lat_units : degrees_north geospatial_lat_resolution : 0.25 europe_datacube_201007 . gross_primary_productivity . mean ( 'time' ) . plot ( vmin = 0 , robust = True ) /home/emmanuel/.conda/envs/rbig_eo/lib/python3.8/site-packages/dask/array/numpy_compat.py:40: RuntimeWarning: invalid value encountered in true_divide x = np.divide(x1, x2, out) <matplotlib.collections.QuadMesh at 0x7f5113a06070>","title":"2. Select Region"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/#3-get-density-cubes","text":"Now, we are going to create some density cubes. Instead of just taking the entire amount of samples, we are going to actually construct features. These features will be the neighbouring pixels in a spatial-temporal manner. For this demonstration, we will assume that the pixels from src.features.preprocessing import DensityCubes def get_density_cubes ( data : xr . Dataset , spatial : int , temporal : int ) -> Tuple [ str , pd . DataFrame ]: \"\"\"Wrapper Function to get density cubes from a dataarray\"\"\" for ikey , idata in data . items (): yield ikey , DensityCubes ( spatial_window = spatial , time_window = temporal ) . get_minicubes ( idata ) # All samples europe_df = europe_datacube_201007 . to_dataframe () . dropna () # reorder index levels = [ 'time' , 'lon' , 'lat' ] europe_df = europe_df . reorder_levels ( levels ) # europe_df = europe_df[indx] europe_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } gross_primary_productivity time lon lat 2010-07-08 51.625 71.625 1.747494 2010-07-16 51.625 71.625 1.073725 2010-07-24 51.625 71.625 1.334421 2010-07-08 51.875 71.625 1.201953 2010-07-16 51.875 71.625 0.647814 spatial = 1 temporal = 3 ivar , europe_temp_df = next ( get_density_cubes ( europe_datacube_201007 , spatial = spatial , temporal = temporal )) levels = [ 'time' , 'lon' , 'lat' ] europe_temp_df = europe_temp_df . reorder_levels ( levels ) europe_temp_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } var_x0 var_x1 var_x2 time lon lat 2010-07-24 51.625 71.625 1.747494 1.073725 1.334421 51.875 71.625 1.201953 0.647814 0.841013 52.125 71.625 1.158344 0.755085 0.847133 52.375 71.625 1.320013 0.837758 0.829512 52.625 71.625 1.087877 0.642634 0.499693 levels = [ 'time' , 'lon' , 'lat' ] idx = europe_temp_df . index . intersection ( europe_df . index ) idx . shape , ((26694,),) X_df = europe_df . loc [ idx ,:] Y_df = europe_temp_df . loc [ idx ,:] X_df . shape , Y_df . shape ((26694, 1), (26694, 3))","title":"3. Get Density Cubes"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/#41-models-framework","text":"","title":"4.1 Models Framework"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/#41-preprocessing","text":"","title":"4.1 Preprocessing"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/#411-training-and-testing","text":"europe_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } gross_primary_productivity time lon lat 2010-07-08 51.625 71.625 1.747494 2010-07-16 51.625 71.625 1.073725 2010-07-24 51.625 71.625 1.334421 2010-07-08 51.875 71.625 1.201953 2010-07-16 51.875 71.625 0.647814 y = europe_df . iloc [:, 0 ][:, np . newaxis ] X = europe_df . iloc [:, 1 :] d_dimensions = X . shape [ 1 ]","title":"4.1.1 - Training and testing"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/#412-train-test-split","text":"from sklearn.model_selection import train_test_split train_size = 2_000 random_state = 123 xtrain , xtest , ytrain , ytest = train_test_split ( X , y , train_size = train_size , random_state = random_state ) test_size = xtest . shape [ 0 ]","title":"4.1.2 - Train-Test Split"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/#411-normalize","text":"from sklearn.preprocessing import StandardScaler # normalize inputs x_normalizer = StandardScaler ( with_mean = True , with_std = False ) xtrain_norm = x_normalizer . fit_transform ( xtrain ) xtest_norm = x_normalizer . transform ( xtest ) # remove mean outputs y_normalizer = StandardScaler ( with_std = False ) ytrain_norm = y_normalizer . fit_transform ( ytrain ) ytest_norm = y_normalizer . transform ( ytest )","title":"4.1.1 - Normalize"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/#42-training","text":"# from src.models.gp import SparseGPR import GPy from scipy.cluster.vq import kmeans2 # Kernel Function (RBF) n_dims = xtrain_norm . shape [ 1 ] kernel = GPy . kern . RBF ( input_dim = n_dims , ARD = False ) # Inducing Points n_inducing = 100 z = kmeans2 ( X , n_inducing , minit = \"points\" )[ 0 ] # Initialize GP Model gp_model = GPy . models . SparseGPRegression ( X , y , kernel = kernel , Z = z ) # choose VFE inference method gp_model . inference_method = ( GPy . inference . latent_function_inference . VarDTC ()) # fix variance to be low in the beginning gp_model . Gaussian_noise . variance = 0.01 2020-04-30 19:40:34,207:INFO:initializing Y 2020-04-30 19:40:34,209:INFO:initializing inference method 2020-04-30 19:40:34,209:INFO:adding kernel and likelihood as parameters 2020-04-30 19:40:34,211:INFO:Adding Z as parameter # optimize GP Model n_restarts = 0 verbose = 1 max_iters = 1_000 # optimize gp_model . optimize ( optimizer = 'scg' , messages = verbose , max_iters = max_iters , ); var element = $('#9b65a86b-ab6d-4c00-8f34-356e74c04ecc'); {\"model_id\": \"9d0ea3598cb24f87bd592c6260c61c8c\", \"version_major\": 2, \"version_minor\": 0} --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/model.py in optimize (self, optimizer, start, messages, max_iters, ipython_notebook, clear_after_finish, **kwargs) 110 with VerboseOptimization ( self , opt , maxiters = max_iters , verbose = messages , ipython_notebook = ipython_notebook , clear_after_finish = clear_after_finish ) as vo : --> 111 opt . run ( start , f_fp = self . _objective_grads , f = self . _objective , fp = self . _grads ) 112 ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/optimization/optimization.py in run (self, x_init, **kwargs) 50 start = dt . datetime . now ( ) ---> 51 self . opt ( x_init , ** kwargs ) 52 end = dt . datetime . now ( ) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/optimization/optimization.py in opt (self, x_init, f_fp, f, fp) 233 --> 234 opt_result = SCG(f, fp, x_init, 235 maxiters = self . max_iters , ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/optimization/scg.py in SCG (f, gradf, x, optargs, maxiters, max_f_eval, xtol, ftol, gtol) 112 xnew = x + alpha * d --> 113 fnew = f ( xnew , * optargs ) 114 function_eval += 1 ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/model.py in _objective (self, x) 260 try : --> 261 self . optimizer_array = x 262 obj = self . objective_function ( ) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/parameterized.py in __setattr__ (self, name, val) 338 param [ : ] = val ; return --> 339 return object . __setattr__ ( self , name , val ) 340 ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/core/sparse_gp_mpi.py in optimizer_array (self, p) 87 self . mpi_comm . Bcast ( p , root = 0 ) ---> 88 SparseGP . optimizer_array . fset ( self , p ) 89 ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/core/parameter_core.py in optimizer_array (self, p) 123 self . _optimizer_copy_transformed = False --> 124 self . trigger_update ( ) 125 ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/core/updateable.py in trigger_update (self, trigger_parent) 78 return ---> 79 self . _trigger_params_changed ( trigger_parent ) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/core/parameter_core.py in _trigger_params_changed (self, trigger_parent) 133 [ p . _trigger_params_changed ( trigger_parent = False ) for p in self . parameters if not p . is_fixed ] --> 134 self . notify_observers ( None , None if trigger_parent else - np . inf ) 135 ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/core/observable.py in notify_observers (self, which, min_priority) 90 if min_priority is None : ---> 91 [ callble ( self , which = which ) for _ , _ , callble in self . observers ] 92 else : ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/core/observable.py in <listcomp> (.0) 90 if min_priority is None : ---> 91 [ callble ( self , which = which ) for _ , _ , callble in self . observers ] 92 else : ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/core/parameter_core.py in _parameters_changed_notification (self, me, which) 507 self . _optimizer_copy_transformed = False # tells the optimizer array to update on next request --> 508 self . parameters_changed ( ) 509 def _pass_through_notify_observers ( self , me , which = None ) : ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/models/sparse_gp_regression.py in parameters_changed (self) 65 else : ---> 66 super ( SparseGPRegression , self ) . parameters_changed ( ) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/core/sparse_gp_mpi.py in parameters_changed (self) 121 else : --> 122 super ( SparseGP_MPI , self ) . parameters_changed ( ) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/core/sparse_gp.py in parameters_changed (self) 77 self . posterior , self . _log_marginal_likelihood , self . grad_dict = \\ ---> 78 self.inference_method.inference(self.kern, self.X, self.Z, self.likelihood, 79 self . Y_normalized , Y_metadata = self . Y_metadata , ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/inference/latent_function_inference/var_dtc.py in inference (self, kern, X, Z, likelihood, Y, Y_metadata, mean_function, precision, Lm, dL_dKmm, psi0, psi1, psi2, Z_tilde) 124 if psi1 is None : --> 125 psi1 = kern . K ( X , Z ) 126 if het_noise : ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/kern/src/kernel_slice_operations.py in wrap (self, X, X2, *a, **kw) 109 with _Slice_wrap ( self , X , X2 ) as s : --> 110 ret = f ( self , s . X , s . X2 , * a , ** kw ) 111 return ret <decorator-gen-150> in K (self, X, X2) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/caching.py in g (obj, *args, **kw) 282 cacher = cache [ self . f ] = Cacher ( self . f , self . limit , self . ignore_args , self . force_kwargs , cacher_enabled = cache . caching_enabled ) --> 283 return cacher ( * args , ** kw ) 284 g . __name__ = f . __name__ ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/caching.py in __call__ (self, *args, **kw) 178 try : --> 179 new_output = self . operation ( * args , ** kw ) 180 except : ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/kern/src/stationary.py in K (self, X, X2) 113 \"\"\" --> 114 r = self . _scaled_dist ( X , X2 ) 115 return self . K_of_r ( r ) <decorator-gen-153> in _scaled_dist (self, X, X2) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/caching.py in g (obj, *args, **kw) 282 cacher = cache [ self . f ] = Cacher ( self . f , self . limit , self . ignore_args , self . force_kwargs , cacher_enabled = cache . caching_enabled ) --> 283 return cacher ( * args , ** kw ) 284 g . __name__ = f . __name__ ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/caching.py in __call__ (self, *args, **kw) 178 try : --> 179 new_output = self . operation ( * args , ** kw ) 180 except : ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/kern/src/stationary.py in _scaled_dist (self, X, X2) 167 else : --> 168 return self . _unscaled_dist ( X , X2 ) / self . lengthscale 169 ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/kern/src/stationary.py in _unscaled_dist (self, X, X2) 147 r2 = np . clip ( r2 , 0 , np . inf ) --> 148 return np . sqrt ( r2 ) 149 KeyboardInterrupt : During handling of the above exception, another exception occurred: TypeError Traceback (most recent call last) <ipython-input-77-e519e2d9a69c> in <module> 5 6 # optimize ----> 7 gp_model.optimize( 8 optimizer = 'scg' , 9 messages = verbose , ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/core/sparse_gp_mpi.py in optimize (self, optimizer, start, **kwargs) 91 self . _IN_OPTIMIZATION_ = True 92 if self . mpi_comm == None : ---> 93 ret = super ( SparseGP_MPI , self ) . optimize ( optimizer , start , ** kwargs ) 94 elif self . mpi_comm . rank == 0 : 95 ret = super ( SparseGP_MPI , self ) . optimize ( optimizer , start , ** kwargs ) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/GPy/core/gp.py in optimize (self, optimizer, start, messages, max_iters, ipython_notebook, clear_after_finish, **kwargs) 673 self . inference_method . on_optimization_start ( ) 674 try : --> 675 ret = super ( GP , self ) . optimize ( optimizer , start , messages , max_iters , ipython_notebook , clear_after_finish , ** kwargs ) 676 except KeyboardInterrupt : 677 print ( \"KeyboardInterrupt caught, calling on_optimization_end() to round things up\" ) ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/model.py in optimize (self, optimizer, start, messages, max_iters, ipython_notebook, clear_after_finish, **kwargs) 109 110 with VerboseOptimization ( self , opt , maxiters = max_iters , verbose = messages , ipython_notebook = ipython_notebook , clear_after_finish = clear_after_finish ) as vo : --> 111 opt . run ( start , f_fp = self . _objective_grads , f = self . _objective , fp = self . _grads ) 112 113 self . optimizer_array = opt . x_opt ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/optimization/verbose_optimization.py in __exit__ (self, type, value, traceback) 213 self . stop = time . time ( ) 214 self . model . remove_observer ( self ) --> 215 self . print_out ( self . stop - self . start ) 216 217 if not self . ipython_notebook : ~/.conda/envs/rbig_eo/lib/python3.8/site-packages/paramz/optimization/verbose_optimization.py in print_out (self, seconds) 146 ['||gradient||', 147 \"{: >+12.3E}\".format(float(self.current_gradient))], --> 148 [ 'status' , \"{:s}\" . format ( self . status ) ] , 149 ] 150 #message = \"Lik:{:5.3E} Grad:{:5.3E} Lik:{:5.3E} Len:{!s}\".format(float(m.log_likelihood()), np.einsum('i,i->', grads, grads), float(m.likelihood.variance), \" \".join([\"{:3.2E}\".format(l) for l in m.kern.lengthscale.values])) TypeError : unsupported format string passed to NoneType.__format__","title":"4.2 - Training"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/#43-testing","text":"ypred = gp_model . predict ( xtest_norm , )[ 0 ] ypred . shape , ytest_norm . shape ((67414, 1), (67414, 1)) stats = Metrics () . get_all ( ypred . squeeze (), ytest_norm . squeeze ()) stats .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mae mse rmse r2 0 1.584058 4.35029 2.085735 0.325761 def _predict ( model , Xs , batch_size ): ms = [] n = max ( len ( Xs ) / batch_size , 1 ) # predict in small batches with tqdm ( np . array_split ( Xs , n )) as bar : for xs in bar : m = model . predict ( xs ,) ms . append ( m ) return np . vstack ( ms ) batch_size = 5_000 ms = [] n = max ( len ( xtest_norm ) / batch_size , 1 ) # predict in small batches with tqdm ( np . array_split ( xtest_norm , n )) as bar : for xs in bar : m = sgp_model . predict ( xs ,) ms . append ( m ) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 598/598 [00:51<00:00, 11.56it/s]","title":"4.3 - Testing"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/#5-direct-measurements","text":"","title":"5. Direct Measurements"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/#51-rhorhov-coefficient","text":"Now, we will explore the easiest linear method. It is the multi-dimensional version of the Pearson Correlation coefficient called the \\rho \\rho V-Coefficient ( \\rho \\rho -Vector Coefficient). Most people are familiar with the correlation coefficient: \\rho(X,Y) = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)}\\sqrt{\\text{Var}(Y)}} \\rho(X,Y) = \\frac{\\text{Cov}(X,Y)}{\\sqrt{\\text{Var}(X)}\\sqrt{\\text{Var}(Y)}} This is very well-known in the literature but it doesn't directly apply to multi-dimensional data. The final result of the numerator and the denominator is a scalar value per dimension. There is no way we can summarize all of the information into a single scalar. One extension we can do is to create a matrix with the pairwise components (i.e gram matrices) for each of the variables and then take the Frobenius norm (Hilbert-Schmidt norm) of the cross term as well as the individual terms. So the equation is like so: \\rho V (\\mathbf{X,Y}) = \\frac{\\left\\langle \\mathbf{XX^\\top, YY^\\top} \\right\\rangle_\\mathbf{F}} {\\sqrt{\\left\\langle \\mathbf{XX^\\top} \\right\\rangle_\\mathbf{F}} \\sqrt{\\left\\langle \\mathbf{YY^\\top} \\right\\rangle_\\mathbf{F}}} \\rho V (\\mathbf{X,Y}) = \\frac{\\left\\langle \\mathbf{XX^\\top, YY^\\top} \\right\\rangle_\\mathbf{F}} {\\sqrt{\\left\\langle \\mathbf{XX^\\top} \\right\\rangle_\\mathbf{F}} \\sqrt{\\left\\langle \\mathbf{YY^\\top} \\right\\rangle_\\mathbf{F}}} Note : This is very similar to HSIC and Centered Kernel Alignment (CKA) but this method dates back before. CKA generalizes this method with the addition of distance measures and non-linear kernel functions. We will explore this in the next section. To code this up, we will all of the components of this equation because we will need them later. from typing import Dict from sklearn.preprocessing import KernelCenterer from sklearn.metrics.pairwise import linear_kernel def rv_coefficient ( X : np . ndarray , Y : np . ndarray ) -> Dict : \"\"\"simple function to calculate the rv coefficient\"\"\" # calculate the kernel matrices X_gram = linear_kernel ( X ) Y_gram = linear_kernel ( Y ) # center the kernels X_gram = KernelCenterer () . fit_transform ( X_gram ) Y_gram = KernelCenterer () . fit_transform ( Y_gram ) # normalizing coefficients (denomenator) x_norm = np . linalg . norm ( X_gram ) y_norm = np . linalg . norm ( Y_gram ) # frobenius norm of the cross terms (numerator) xy_norm = np . sum ( X_gram * Y_gram ) # rv coefficient pv_coeff = xy_norm / x_norm / y_norm return { 'coefficient' : pv_coeff , 'x_norm' : x_norm , 'y_norm' : y_norm , 'xy_norm' : xy_norm } X_samples = europe_temp_df . iloc [:, 0 ][:, np . newaxis ] Y_samples = europe_temp_df logging . info ( f \" Size of X_samples: { X_samples . shape } , { Y_samples . shape } \" ) d_dimensions = X . shape [ 1 ] 2020-04-30 21:10:39,726:INFO: Size of X_samples: (26694, 1), (26694, 3) # check that the coefficient is 1 if the data is the same rv_coeff = rv_coefficient ( X_samples [: 100 ], X_samples [: 100 ]) np . testing . assert_almost_equal ( rv_coeff [ 'coefficient' ], 1 ) So now, let's try when we have some a difference between the variables. %% time rv_coeff = rv_coefficient ( X_samples [:], Y_samples [:]) rv_coeff CPU times: user 23.8 s, sys: 4.9 s, total: 28.7 s Wall time: 13.3 s {'coefficient': 0.9696304, 'x_norm': 191155.58, 'y_norm': 510949.53, 'xy_norm': 94704630000.0}","title":"5.1 - \\rho\\rhoV Coefficient"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/#52-non-linear-kernel","text":"An addition that we can do is to explore how the from typing import Optional from scipy.spatial.distance import pdist , squareform def estimate_sigma ( X : np . ndarray , method : str = 'median' , percent : Optional [ int ] = None , heuristic : bool = False ) -> float : # get the squared euclidean distances if method == 'silverman' : return silvermans_factor ( X ) elif method == 'scott' : return scotts_factor ( X ) elif percent is not None : kth_sample = int (( percent / 100 ) * X . shape [ 0 ]) dists = np . sort ( squareform ( pdist ( X , 'sqeuclidean' )))[:, kth_sample ] # print(dists.shape, dists.min(), dists.max()) else : dists = np . sort ( pdist ( X , 'sqeuclidean' )) # print(dists.shape, dists.min(), dists.max()) if method == 'median' : sigma = np . median ( dists ) elif method == 'mean' : sigma = np . mean ( dists ) else : raise ValueError ( f \"Unrecognized distance measure: { method } \" ) if heuristic : sigma = np . sqrt ( sigma / 2 ) return sigma from typing import Dict from sklearn.preprocessing import KernelCenterer from sklearn.gaussian_process.kernels import RBF def cka_coefficient ( X : np . ndarray , Y : np . ndarray ) -> Dict : \"\"\"simple function to calculate the rv coefficient\"\"\" # estimate sigmas sigma_X = estimate_sigma ( X , method = 'median' , percent = 50 ) sigma_Y = estimate_sigma ( Y , method = 'median' , percent = 50 ) # calculate the kernel matrices X_gram = RBF ( sigma_X )( X ) Y_gram = RBF ( sigma_Y )( Y ) # center the kernels X_gram = KernelCenterer () . fit_transform ( X_gram ) Y_gram = KernelCenterer () . fit_transform ( Y_gram ) # normalizing coefficients (denomenator) x_norm = np . linalg . norm ( X_gram ) y_norm = np . linalg . norm ( Y_gram ) # frobenius norm of the cross terms (numerator) xy_norm = np . sum ( X_gram * Y_gram ) # rv coefficient pv_coeff = xy_norm / x_norm / y_norm return { 'coefficient' : pv_coeff , 'x_norm' : x_norm , 'y_norm' : y_norm , 'xy_norm' : xy_norm } # check that the coefficient is 1 if the data is the same cka_coeff = cka_coefficient ( X_samples [: 100 ], X_samples [: 100 ]) np . testing . assert_almost_equal ( cka_coeff [ 'coefficient' ], 1 ) %% time cka_coeff = cka_coefficient ( X_samples [: 10_000 ], Y_samples [: 10_000 ]) cka_coeff CPU times: user 20.4 s, sys: 2.89 s, total: 23.3 s Wall time: 17 s {'coefficient': 0.9576703788185938, 'x_norm': 2326.331280441253, 'y_norm': 1175.2587321221124, 'xy_norm': 2618310.2249249523}","title":"5.2 - Non-Linear Kernel"},{"location":"notebooks/spatial_temporal/2.0_methods_walkthrough/#variation-of-information","text":"from rbig.rbig import RBIGMI , RBIG rbig_results = {} def variation_of_info ( H_X , H_Y , I_XY ): return I_XY / np . sqrt ( H_X ) / np . sqrt ( H_Y ) %% time n_layers = 10000 rotation_type = 'PCA' random_state = 0 zero_tolerance = 60 pdf_extension = None pdf_resolution = None tolerance = None # Initialize RBIG class H_rbig_model = RBIG ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance ) # fit model to the data rbig_results [ 'H_x' ] = H_rbig_model . fit ( X_samples ) . entropy ( correction = True ); rbig_results [ 'H_y' ] = H_rbig_model . fit ( Y_samples ) . entropy ( correction = True ); # Initialize RBIG class I_rbig_model = RBIGMI ( n_layers = n_layers , rotation_type = rotation_type , random_state = random_state , zero_tolerance = zero_tolerance , tolerance = tolerance ) # fit model to the data rbig_results [ 'I_xy' ] = I_rbig_model . fit ( X_samples , Y_samples ) . mutual_information (); # calculate the variation of information coefficient rbig_results [ 'coefficient' ] = variation_of_info ( rbig_results [ 'H_x' ], rbig_results [ 'H_y' ], rbig_results [ 'I_xy' ] ) CPU times: user 4min 12s, sys: 36.4 s, total: 4min 49s Wall time: 11.2 s rbig_results {'H_x': 3.2281169474002924, 'H_y': 5.181635094706355, 'I_xy': 5.412206453196236, 'coefficient': 1.3233243751128443}","title":"Variation of Information"},{"location":"notebooks/spatial_temporal/3.0_exp1_walkthrough/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Spatial-Temporal Experiment \u00b6 In this notebook, I will be walking through how we can estimate different methods based on the density cubes that we derive. import sys , os from pyprojroot import here root = here ( project_files = [ \".here\" ]) sys . path . append ( str ( here ())) import pathlib # standard python packages import xarray as xr import pandas as pd import numpy as np from xcube.core.geom import clip_dataset_by_geometry # from src.features import Metrics from src.features.preprocessing import DensityCubes from sklearn.preprocessing import StandardScaler # # esdc tools # from src.esdc.subset import select_pixel # from src.esdc.shape import ShapeFileExtract, rasterize # from esdc.transform import DensityCubes from typing import List , Dict import xarray as xr from tqdm import tqdm import cartopy import cartopy.crs as ccrs # NUMPY SETTINGS import numpy as onp onp . set_printoptions ( precision = 3 , suppress = True ) # MATPLOTLIB Settings import matplotlib as mpl import matplotlib.pyplot as plt % matplotlib inline % config InlineBackend . figure_format = 'retina' # SEABORN SETTINGS import seaborn as sns sns . set_context ( context = 'talk' , font_scale = 0.7 ) # sns.set(rc={'figure.figsize': (12, 9.)}) # sns.set_style(\"whitegrid\") # PANDAS SETTINGS import pandas as pd pd . set_option ( \"display.max_rows\" , 120 ) pd . set_option ( \"display.max_columns\" , 120 ) # LOGGING SETTINGS import sys import logging logging . basicConfig ( level = logging . INFO , stream = sys . stdout , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) logger = logging . getLogger () #logger.setLevel(logging.INFO) % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Experiment Steps \u00b6 Global Variables \u00b6 from collections import namedtuple from typing import Union , Tuple import xarray as xr import rioxarray import shapely # global variables # Datapath DATA_PATH = pathlib . Path ( \"/media/disk/databases/ESDC/\" ) levels = [ 'time' , 'lon' , 'lat' ] # get filename filename = DATA_PATH . joinpath ( \"esdc-8d-0.25deg-1x720x1440-2.0.0.zarr\" ) Region = namedtuple ( \"Region\" , [ \"name\" , \"lonmin\" , \"lonmax\" , \"latmin\" , \"latmax\" ]) TimePeriod = namedtuple ( \"TimePeriod\" , [ \"name\" , \"start\" , \"end\" ]) def get_test_time () -> TimePeriod : return TimePeriod ( name = 'test_201007' , start = 'July-2010' , end = 'July-2010' ) def get_europe () -> Region : \"\"\"As an example, I often choose Europe. This is a decent bounding box.\"\"\" return Region ( name = \"europe\" , latmax = 35.5 , latmin = 71.5 , lonmax = 60.0 , lonmin =- 18.0 ) Parameters \u00b6 variables = [ 'gross_primary_productivity' , 'root_moisture' , 'land_surface_temperature' ] Functions \u00b6 from prefect import task , Flow , Parameter @task # get Dataset def get_dataset ( variable : str ) -> xr . Dataset : return xr . open_zarr ( str ( filename ))[[ variable ]] @task # subset datacube def cube_spatial_subset ( xr_data : xr . Dataset , bbox : Region ) -> xr . Dataset : \"\"\"Function to spatially subset an xarray dataset from a bounding box.\"\"\" # get bounding box bbox = shapely . geometry . box ( bbox . lonmin , bbox . latmin , bbox . lonmax , bbox . latmax ) # subset datacube return clip_dataset_by_geometry ( xr_data , bbox ) @task def cube_temporal_subset ( xr_data : xr . DataArray , period : Tuple [ str , str ]) -> xr . DataArray : \"\"\"Function to temporally subset an xarray dataset from a tuple of start date and end date \"\"\" return xr_data . sel ( time = slice ( period . start , period . end )) @task # get reference cube def get_reference_cube ( data : xr . DataArray ) -> pd . DataFrame : \"\"\"Wrapper Function to get reference cube\"\"\" return data . to_dataframe () . dropna () . reorder_levels ( levels ) @task # get density cubes def get_density_cubes ( data : xr . DataArray , spatial : int , temporal : int ) -> pd . DataFrame : \"\"\"Wrapper Function to get density cubes from a dataarray\"\"\" return DensityCubes ( spatial_window = spatial , time_window = temporal ) . get_minicubes ( data ) . reorder_levels ( levels ) @task # get common indices def get_common_indices ( reference_df : pd . DataFrame , density_df : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: idx = density_df . index . intersection ( reference_df . index ) return reference_df . loc [ idx ,:], density_df . loc [ idx , :] @task # standardize the data before def standardizer_data ( X : pd . DataFrame , Y : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: # standardizer normalizer = StandardScaler ( with_mean = True , with_std = True ) # standardize X values X_values = normalizer . fit_transform ( X . values ) X = pd . DataFrame ( data = X_values , index = X . index , columns = X . columns ) # standardize Y Values Y_values = normalizer . fit_transform ( Y . values ) Y = pd . DataFrame ( data = Y_values , index = Y . index , columns = Y . columns ) return X , Y @task def get_similarity_scores ( X_ref : pd . DataFrame , Y_compare : pd . DataFrame ) -> Dict : # RV Coefficient rv_results = rv_coefficient ( X_ref , Y_compare ) # # CKA Coefficient # cka_results = cka_coefficient(X_ref, Y_compare) # RBIG Coefficient rbig_results = rbig_it_measures ( X_ref , Y_compare ) results = { ** rv_results , # **cka_results, ** rbig_results } return results from src.models.similarity import cka_coefficient , rv_coefficient , rbig_it_measures Experiment Run \u00b6 # variable = 'gross_primary_productivity' # region = get_europe() # datacube = get_dataset(variable) # datacube = subset_cube(xr_data=datacube, bbox=region) logger . setLevel ( logging . INFO ) with Flow ( \"Experiment-Step\" ) as flow : # ====================== # experiment parameters # ====================== variable = Parameter ( \"variable\" , default = 'gross_primary_productivity' ) region = Parameter ( \"region\" , default = get_europe ()) period = Parameter ( \"period\" , default = get_test_time ()) spatial = Parameter ( \"spatial\" , default = 1 ) temporal = Parameter ( \"temporal\" , default = 3 ) # ====================== # experiment - Data # ====================== # Get DataCube datacube = get_dataset ( variable ) # subset datacube (spatially) datacube = cube_spatial_subset ( xr_data = datacube , bbox = region )[ variable ] # subset datacube (temporally) datacube = cube_temporal_subset ( xr_data = datacube , period = period ) # get datacubes reference_cube_df = get_reference_cube ( data = datacube ) # get density cubes density_cube_df = get_density_cubes ( data = datacube , spatial = spatial , temporal = temporal ) # get reference dataframe dfs = get_common_indices ( reference_df = reference_cube_df , density_df = density_cube_df ) # standardize data dfs = standardizer_data ( X = dfs [ 0 ], Y = dfs [ 1 ]) # ====================== # experiment - Methods # ====================== res = get_similarity_scores ( X_ref = dfs [ 0 ], Y_compare = dfs [ 1 ]) state = flow . run () [2020-05-01 10:16:21] INFO - prefect.FlowRunner | Beginning Flow run for 'Experiment-Step' 2020-05-01 12:16:21,361:INFO:Beginning Flow run for 'Experiment-Step' [2020-05-01 10:16:21] INFO - prefect.FlowRunner | Starting flow run. 2020-05-01 12:16:21,372:INFO:Starting flow run. [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'variable': Starting task run... 2020-05-01 12:16:21,411:INFO:Task 'variable': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'variable': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,424:INFO:Task 'variable': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'region': Starting task run... 2020-05-01 12:16:21,454:INFO:Task 'region': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'region': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,467:INFO:Task 'region': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'period': Starting task run... 2020-05-01 12:16:21,496:INFO:Task 'period': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'period': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,509:INFO:Task 'period': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'temporal': Starting task run... 2020-05-01 12:16:21,539:INFO:Task 'temporal': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'temporal': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,551:INFO:Task 'temporal': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'get_dataset': Starting task run... 2020-05-01 12:16:21,581:INFO:Task 'get_dataset': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_dataset': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,485:INFO:Task 'get_dataset': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_spatial_subset': Starting task run... 2020-05-01 12:16:22,505:INFO:Task 'cube_spatial_subset': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_spatial_subset': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,525:INFO:Task 'cube_spatial_subset': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:22,545:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,554:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_temporal_subset': Starting task run... 2020-05-01 12:16:22,574:INFO:Task 'cube_temporal_subset': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_temporal_subset': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,585:INFO:Task 'cube_temporal_subset': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_reference_cube': Starting task run... 2020-05-01 12:16:22,605:INFO:Task 'get_reference_cube': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_reference_cube': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,653:INFO:Task 'get_reference_cube': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'spatial': Starting task run... 2020-05-01 12:16:22,687:INFO:Task 'spatial': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'spatial': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,700:INFO:Task 'spatial': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_density_cubes': Starting task run... 2020-05-01 12:16:22,729:INFO:Task 'get_density_cubes': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_density_cubes': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,769:INFO:Task 'get_density_cubes': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_common_indices': Starting task run... 2020-05-01 12:16:22,799:INFO:Task 'get_common_indices': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'get_common_indices': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,766:INFO:Task 'get_common_indices': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,786:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,795:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,815:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,824:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'standardizer_data': Starting task run... 2020-05-01 12:16:23,843:INFO:Task 'standardizer_data': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'standardizer_data': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,855:INFO:Task 'standardizer_data': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,874:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,883:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,903:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,912:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'get_similarity_scores': Starting task run... 2020-05-01 12:16:23,931:INFO:Task 'get_similarity_scores': Starting task run... [2020-05-01 10:16:50] INFO - prefect.TaskRunner | Task 'get_similarity_scores': finished task run for task with final state: 'Success' 2020-05-01 12:16:50,094:INFO:Task 'get_similarity_scores': finished task run for task with final state: 'Success' [2020-05-01 10:16:50] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded 2020-05-01 12:16:50,097:INFO:Flow run SUCCESS: all reference tasks succeeded state . result [ res ] . result {'rv_coeff': 0.9697258, 'rv_x_norm': 26692.072, 'rv_y_norm': 77907.49, 'rv_xy_norm': 2016556900.0, 'rbig_H_x': 1.855240533094599, 'rbig_H_y': 1.1286197933913034, 'rbig_I_xy': 5.499353957238775, 'rbig_vi_coeff': 3.8004736863738287}","title":"3.0 exp1 walkthrough"},{"location":"notebooks/spatial_temporal/3.0_exp1_walkthrough/#spatial-temporal-experiment","text":"In this notebook, I will be walking through how we can estimate different methods based on the density cubes that we derive. import sys , os from pyprojroot import here root = here ( project_files = [ \".here\" ]) sys . path . append ( str ( here ())) import pathlib # standard python packages import xarray as xr import pandas as pd import numpy as np from xcube.core.geom import clip_dataset_by_geometry # from src.features import Metrics from src.features.preprocessing import DensityCubes from sklearn.preprocessing import StandardScaler # # esdc tools # from src.esdc.subset import select_pixel # from src.esdc.shape import ShapeFileExtract, rasterize # from esdc.transform import DensityCubes from typing import List , Dict import xarray as xr from tqdm import tqdm import cartopy import cartopy.crs as ccrs # NUMPY SETTINGS import numpy as onp onp . set_printoptions ( precision = 3 , suppress = True ) # MATPLOTLIB Settings import matplotlib as mpl import matplotlib.pyplot as plt % matplotlib inline % config InlineBackend . figure_format = 'retina' # SEABORN SETTINGS import seaborn as sns sns . set_context ( context = 'talk' , font_scale = 0.7 ) # sns.set(rc={'figure.figsize': (12, 9.)}) # sns.set_style(\"whitegrid\") # PANDAS SETTINGS import pandas as pd pd . set_option ( \"display.max_rows\" , 120 ) pd . set_option ( \"display.max_columns\" , 120 ) # LOGGING SETTINGS import sys import logging logging . basicConfig ( level = logging . INFO , stream = sys . stdout , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) logger = logging . getLogger () #logger.setLevel(logging.INFO) % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"Spatial-Temporal Experiment"},{"location":"notebooks/spatial_temporal/3.0_exp1_walkthrough/#experiment-steps","text":"","title":"Experiment Steps"},{"location":"notebooks/spatial_temporal/3.0_exp1_walkthrough/#global-variables","text":"from collections import namedtuple from typing import Union , Tuple import xarray as xr import rioxarray import shapely # global variables # Datapath DATA_PATH = pathlib . Path ( \"/media/disk/databases/ESDC/\" ) levels = [ 'time' , 'lon' , 'lat' ] # get filename filename = DATA_PATH . joinpath ( \"esdc-8d-0.25deg-1x720x1440-2.0.0.zarr\" ) Region = namedtuple ( \"Region\" , [ \"name\" , \"lonmin\" , \"lonmax\" , \"latmin\" , \"latmax\" ]) TimePeriod = namedtuple ( \"TimePeriod\" , [ \"name\" , \"start\" , \"end\" ]) def get_test_time () -> TimePeriod : return TimePeriod ( name = 'test_201007' , start = 'July-2010' , end = 'July-2010' ) def get_europe () -> Region : \"\"\"As an example, I often choose Europe. This is a decent bounding box.\"\"\" return Region ( name = \"europe\" , latmax = 35.5 , latmin = 71.5 , lonmax = 60.0 , lonmin =- 18.0 )","title":"Global Variables"},{"location":"notebooks/spatial_temporal/3.0_exp1_walkthrough/#parameters","text":"variables = [ 'gross_primary_productivity' , 'root_moisture' , 'land_surface_temperature' ]","title":"Parameters"},{"location":"notebooks/spatial_temporal/3.0_exp1_walkthrough/#functions","text":"from prefect import task , Flow , Parameter @task # get Dataset def get_dataset ( variable : str ) -> xr . Dataset : return xr . open_zarr ( str ( filename ))[[ variable ]] @task # subset datacube def cube_spatial_subset ( xr_data : xr . Dataset , bbox : Region ) -> xr . Dataset : \"\"\"Function to spatially subset an xarray dataset from a bounding box.\"\"\" # get bounding box bbox = shapely . geometry . box ( bbox . lonmin , bbox . latmin , bbox . lonmax , bbox . latmax ) # subset datacube return clip_dataset_by_geometry ( xr_data , bbox ) @task def cube_temporal_subset ( xr_data : xr . DataArray , period : Tuple [ str , str ]) -> xr . DataArray : \"\"\"Function to temporally subset an xarray dataset from a tuple of start date and end date \"\"\" return xr_data . sel ( time = slice ( period . start , period . end )) @task # get reference cube def get_reference_cube ( data : xr . DataArray ) -> pd . DataFrame : \"\"\"Wrapper Function to get reference cube\"\"\" return data . to_dataframe () . dropna () . reorder_levels ( levels ) @task # get density cubes def get_density_cubes ( data : xr . DataArray , spatial : int , temporal : int ) -> pd . DataFrame : \"\"\"Wrapper Function to get density cubes from a dataarray\"\"\" return DensityCubes ( spatial_window = spatial , time_window = temporal ) . get_minicubes ( data ) . reorder_levels ( levels ) @task # get common indices def get_common_indices ( reference_df : pd . DataFrame , density_df : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: idx = density_df . index . intersection ( reference_df . index ) return reference_df . loc [ idx ,:], density_df . loc [ idx , :] @task # standardize the data before def standardizer_data ( X : pd . DataFrame , Y : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: # standardizer normalizer = StandardScaler ( with_mean = True , with_std = True ) # standardize X values X_values = normalizer . fit_transform ( X . values ) X = pd . DataFrame ( data = X_values , index = X . index , columns = X . columns ) # standardize Y Values Y_values = normalizer . fit_transform ( Y . values ) Y = pd . DataFrame ( data = Y_values , index = Y . index , columns = Y . columns ) return X , Y @task def get_similarity_scores ( X_ref : pd . DataFrame , Y_compare : pd . DataFrame ) -> Dict : # RV Coefficient rv_results = rv_coefficient ( X_ref , Y_compare ) # # CKA Coefficient # cka_results = cka_coefficient(X_ref, Y_compare) # RBIG Coefficient rbig_results = rbig_it_measures ( X_ref , Y_compare ) results = { ** rv_results , # **cka_results, ** rbig_results } return results from src.models.similarity import cka_coefficient , rv_coefficient , rbig_it_measures","title":"Functions"},{"location":"notebooks/spatial_temporal/3.0_exp1_walkthrough/#experiment-run","text":"# variable = 'gross_primary_productivity' # region = get_europe() # datacube = get_dataset(variable) # datacube = subset_cube(xr_data=datacube, bbox=region) logger . setLevel ( logging . INFO ) with Flow ( \"Experiment-Step\" ) as flow : # ====================== # experiment parameters # ====================== variable = Parameter ( \"variable\" , default = 'gross_primary_productivity' ) region = Parameter ( \"region\" , default = get_europe ()) period = Parameter ( \"period\" , default = get_test_time ()) spatial = Parameter ( \"spatial\" , default = 1 ) temporal = Parameter ( \"temporal\" , default = 3 ) # ====================== # experiment - Data # ====================== # Get DataCube datacube = get_dataset ( variable ) # subset datacube (spatially) datacube = cube_spatial_subset ( xr_data = datacube , bbox = region )[ variable ] # subset datacube (temporally) datacube = cube_temporal_subset ( xr_data = datacube , period = period ) # get datacubes reference_cube_df = get_reference_cube ( data = datacube ) # get density cubes density_cube_df = get_density_cubes ( data = datacube , spatial = spatial , temporal = temporal ) # get reference dataframe dfs = get_common_indices ( reference_df = reference_cube_df , density_df = density_cube_df ) # standardize data dfs = standardizer_data ( X = dfs [ 0 ], Y = dfs [ 1 ]) # ====================== # experiment - Methods # ====================== res = get_similarity_scores ( X_ref = dfs [ 0 ], Y_compare = dfs [ 1 ]) state = flow . run () [2020-05-01 10:16:21] INFO - prefect.FlowRunner | Beginning Flow run for 'Experiment-Step' 2020-05-01 12:16:21,361:INFO:Beginning Flow run for 'Experiment-Step' [2020-05-01 10:16:21] INFO - prefect.FlowRunner | Starting flow run. 2020-05-01 12:16:21,372:INFO:Starting flow run. [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'variable': Starting task run... 2020-05-01 12:16:21,411:INFO:Task 'variable': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'variable': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,424:INFO:Task 'variable': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'region': Starting task run... 2020-05-01 12:16:21,454:INFO:Task 'region': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'region': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,467:INFO:Task 'region': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'period': Starting task run... 2020-05-01 12:16:21,496:INFO:Task 'period': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'period': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,509:INFO:Task 'period': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'temporal': Starting task run... 2020-05-01 12:16:21,539:INFO:Task 'temporal': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'temporal': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,551:INFO:Task 'temporal': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'get_dataset': Starting task run... 2020-05-01 12:16:21,581:INFO:Task 'get_dataset': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_dataset': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,485:INFO:Task 'get_dataset': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_spatial_subset': Starting task run... 2020-05-01 12:16:22,505:INFO:Task 'cube_spatial_subset': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_spatial_subset': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,525:INFO:Task 'cube_spatial_subset': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:22,545:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,554:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_temporal_subset': Starting task run... 2020-05-01 12:16:22,574:INFO:Task 'cube_temporal_subset': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_temporal_subset': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,585:INFO:Task 'cube_temporal_subset': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_reference_cube': Starting task run... 2020-05-01 12:16:22,605:INFO:Task 'get_reference_cube': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_reference_cube': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,653:INFO:Task 'get_reference_cube': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'spatial': Starting task run... 2020-05-01 12:16:22,687:INFO:Task 'spatial': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'spatial': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,700:INFO:Task 'spatial': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_density_cubes': Starting task run... 2020-05-01 12:16:22,729:INFO:Task 'get_density_cubes': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_density_cubes': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,769:INFO:Task 'get_density_cubes': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_common_indices': Starting task run... 2020-05-01 12:16:22,799:INFO:Task 'get_common_indices': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'get_common_indices': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,766:INFO:Task 'get_common_indices': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,786:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,795:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,815:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,824:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'standardizer_data': Starting task run... 2020-05-01 12:16:23,843:INFO:Task 'standardizer_data': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'standardizer_data': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,855:INFO:Task 'standardizer_data': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,874:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,883:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,903:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,912:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'get_similarity_scores': Starting task run... 2020-05-01 12:16:23,931:INFO:Task 'get_similarity_scores': Starting task run... [2020-05-01 10:16:50] INFO - prefect.TaskRunner | Task 'get_similarity_scores': finished task run for task with final state: 'Success' 2020-05-01 12:16:50,094:INFO:Task 'get_similarity_scores': finished task run for task with final state: 'Success' [2020-05-01 10:16:50] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded 2020-05-01 12:16:50,097:INFO:Flow run SUCCESS: all reference tasks succeeded state . result [ res ] . result {'rv_coeff': 0.9697258, 'rv_x_norm': 26692.072, 'rv_y_norm': 77907.49, 'rv_xy_norm': 2016556900.0, 'rbig_H_x': 1.855240533094599, 'rbig_H_y': 1.1286197933913034, 'rbig_I_xy': 5.499353957238775, 'rbig_vi_coeff': 3.8004736863738287}","title":"Experiment Run"},{"location":"notebooks/spatial_temporal/3.1_exp1_walkthrough_dask/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Spatial-Temporal Experiment \u00b6 In this notebook, I will be walking through how we can estimate different methods based on the density cubes that we derive. import sys , os from pyprojroot import here root = here ( project_files = [ \".here\" ]) sys . path . append ( str ( here ())) import pathlib # standard python packages import xarray as xr import pandas as pd import numpy as np # # Experiment Functions from src.data.esdc import get_dataset from src.features import Metrics from src.features.temporal import select_period , get_smoke_test_time from src.features.spatial import select_region , get_europe from src.models.train_models import get_similarity_scores from src.experiments.utils import dict_product , run_parallel_step from src.features import Metrics from src.features.density import get_density_cubes from src.features.preprocessing import standardizer_data , get_reference_cube , get_common_indices from src.models.similarity import cka_coefficient , rv_coefficient , rbig_it_measures # # esdc tools # from src.esdc.subset import select_pixel # from src.esdc.shape import ShapeFileExtract, rasterize # from esdc.transform import DensityCubes from typing import List , Dict import xarray as xr from tqdm import tqdm import cartopy import cartopy.crs as ccrs # NUMPY SETTINGS import numpy as onp onp . set_printoptions ( precision = 3 , suppress = True ) # MATPLOTLIB Settings import matplotlib as mpl import matplotlib.pyplot as plt % matplotlib inline % config InlineBackend . figure_format = 'retina' # SEABORN SETTINGS import seaborn as sns sns . set_context ( context = 'talk' , font_scale = 0.7 ) # sns.set(rc={'figure.figsize': (12, 9.)}) # sns.set_style(\"whitegrid\") # PANDAS SETTINGS import pandas as pd pd . set_option ( \"display.max_rows\" , 120 ) pd . set_option ( \"display.max_columns\" , 120 ) # LOGGING SETTINGS import sys import logging logging . basicConfig ( level = logging . INFO , stream = sys . stdout , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) logger = logging . getLogger () #logger.setLevel(logging.INFO) % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload Experiment Steps \u00b6 Global Variables \u00b6 Parameters \u00b6 parameters = {} parameters [ 'variable' ] = [ 'gross_primary_productivity' , 'root_moisture' , 'land_surface_temperature' ] parameters [ 'region' ] = [ get_europe ()] parameters [ 'period' ] = [ get_smoke_test_time ()] parameters [ 'spatial' ] = [ 1 , 2 , 3 , 4 , 5 , 6 ] parameters [ 'temporal' ] = [ 1 , 2 , 3 , 4 , 5 , 6 ] # params = list(dict_product(parameters)) params = list ( dict_product ( parameters )) print ( len ( params )) smoke_test = True 108 Experimental Step \u00b6 def step ( params : Dict , smoke_test : bool = False ): # ====================== # experiment - Data # ====================== # Get DataCube datacube = get_dataset ( params [ 'variable' ]) # subset datacube (spatially) datacube = select_region ( xr_data = datacube , bbox = params [ 'region' ] )[ params [ 'variable' ]] # subset datacube (temporally) datacube = select_period ( xr_data = datacube , period = params [ 'period' ]) # get datacubes reference_cube_df = get_reference_cube ( data = datacube ) # get density cubes density_cube_df = get_density_cubes ( data = datacube , spatial = params [ 'spatial' ], temporal = params [ 'temporal' ] ) # get reference dataframe X , Y = get_common_indices ( reference_df = reference_cube_df , density_df = density_cube_df ) # standardize data X , Y = standardizer_data ( X = X , Y = Y ) # ====================== # experiment - Methods # ====================== res = get_similarity_scores ( X_ref = X , Y_compare = Y , smoke_test = smoke_test ) # Save Results results = pd . DataFrame ({ 'region' : params [ 'region' ] . name , 'period' : params [ 'period' ] . name , 'variable' : params [ 'variable' ], 'spatial' : params [ 'spatial' ], 'temporal' : params [ 'temporal' ], ** res }, index = [ 0 ]) return results results = step ( params [ 0 ], smoke_test = True ) results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } region period variable spatial temporal rv_coeff rv_x_norm rv_y_norm rv_xy_norm rv_time rbig_H_x rbig_H_y rbig_H_time rbig_I_xy rbig_I_time rbig_vi_coeff 0 europe test_201007 gross_primary_productivity 1 1 1.0 100.801453 100.801453 10160.93457 0.00383 0.835917 0.835917 0.232885 25.802617 0.468356 30.867425 res {'rv_coeff': 0.9403951, 'rv_x_norm': 44288.883, 'rv_y_norm': 357065.0, 'rv_xy_norm': 14871418000.0, 'rv_time': 38.40082359313965, 'rbig_H_x': 1.9247062049009207, 'rbig_H_y': 4.4428976758211896, 'rbig_H_time': 3.0350914001464844, 'rbig_I_xy': 3.649542912651551, 'rbig_I_time': 31.18929934501648, 'rbig_vi_coeff': 1.2480244562128495} res {'rv_coeff': 0.9697245, 'rv_x_norm': 26692.102, 'rv_y_norm': 77907.13, 'rv_xy_norm': 2016547100.0, 'rbig_H_x': 1.855240533094599, 'rbig_H_y': 1.0902273375895914, 'rbig_I_xy': 5.405821100129361, 'rbig_vi_coeff': 3.801045104354525} from prefect import task , Flow , Parameter @task # get Dataset def get_dataset ( variable : str ) -> xr . Dataset : return xr . open_zarr ( str ( filename ))[[ variable ]] @task # subset datacube def cube_spatial_subset ( xr_data : xr . Dataset , bbox : Region ) -> xr . Dataset : \"\"\"Function to spatially subset an xarray dataset from a bounding box.\"\"\" # get bounding box bbox = shapely . geometry . box ( bbox . lonmin , bbox . latmin , bbox . lonmax , bbox . latmax ) # subset datacube return clip_dataset_by_geometry ( xr_data , bbox ) @task def cube_temporal_subset ( xr_data : xr . DataArray , period : Tuple [ str , str ]) -> xr . DataArray : \"\"\"Function to temporally subset an xarray dataset from a tuple of start date and end date \"\"\" return xr_data . sel ( time = slice ( period . start , period . end )) @task # get reference cube def get_reference_cube ( data : xr . DataArray ) -> pd . DataFrame : \"\"\"Wrapper Function to get reference cube\"\"\" return data . to_dataframe () . dropna () . reorder_levels ( levels ) @task # get density cubes def get_density_cubes ( data : xr . DataArray , spatial : int , temporal : int ) -> pd . DataFrame : \"\"\"Wrapper Function to get density cubes from a dataarray\"\"\" return DensityCubes ( spatial_window = spatial , time_window = temporal ) . get_minicubes ( data ) . reorder_levels ( levels ) @task # get common indices def get_common_indices ( reference_df : pd . DataFrame , density_df : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: idx = density_df . index . intersection ( reference_df . index ) return reference_df . loc [ idx ,:], density_df . loc [ idx , :] @task # standardize the data before def standardizer_data ( X : pd . DataFrame , Y : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: # standardizer normalizer = StandardScaler ( with_mean = True , with_std = True ) # standardize X values X_values = normalizer . fit_transform ( X . values ) X = pd . DataFrame ( data = X_values , index = X . index , columns = X . columns ) # standardize Y Values Y_values = normalizer . fit_transform ( Y . values ) Y = pd . DataFrame ( data = Y_values , index = Y . index , columns = Y . columns ) return X , Y @task def get_similarity_scores ( X_ref : pd . DataFrame , Y_compare : pd . DataFrame ) -> Dict : # RV Coefficient rv_results = rv_coefficient ( X_ref , Y_compare ) # # CKA Coefficient # cka_results = cka_coefficient(X_ref, Y_compare) # RBIG Coefficient rbig_results = rbig_it_measures ( X_ref , Y_compare ) results = { ** rv_results , # **cka_results, ** rbig_results } return results Experiment Run \u00b6 # variable = 'gross_primary_productivity' # region = get_europe() # datacube = get_dataset(variable) # datacube = subset_cube(xr_data=datacube, bbox=region) logger . setLevel ( logging . INFO ) with Flow ( \"Experiment-Step\" ) as flow : # ====================== # experiment parameters # ====================== variable = Parameter ( \"variable\" , default = 'gross_primary_productivity' ) region = Parameter ( \"region\" , default = get_europe ()) period = Parameter ( \"period\" , default = get_test_time ()) spatial = Parameter ( \"spatial\" , default = 1 ) temporal = Parameter ( \"temporal\" , default = 3 ) # ====================== # experiment - Data # ====================== # Get DataCube datacube = get_dataset ( variable ) # subset datacube (spatially) datacube = cube_spatial_subset ( xr_data = datacube , bbox = region )[ variable ] # subset datacube (temporally) datacube = cube_temporal_subset ( xr_data = datacube , period = period ) # get datacubes reference_cube_df = get_reference_cube ( data = datacube ) # get density cubes density_cube_df = get_density_cubes ( data = datacube , spatial = spatial , temporal = temporal ) # get reference dataframe dfs = get_common_indices ( reference_df = reference_cube_df , density_df = density_cube_df ) # standardize data dfs = standardizer_data ( X = dfs [ 0 ], Y = dfs [ 1 ]) # ====================== # experiment - Methods # ====================== res = get_similarity_scores ( X_ref = dfs [ 0 ], Y_compare = dfs [ 1 ]) state = flow . run () [2020-05-01 10:16:21] INFO - prefect.FlowRunner | Beginning Flow run for 'Experiment-Step' 2020-05-01 12:16:21,361:INFO:Beginning Flow run for 'Experiment-Step' [2020-05-01 10:16:21] INFO - prefect.FlowRunner | Starting flow run. 2020-05-01 12:16:21,372:INFO:Starting flow run. [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'variable': Starting task run... 2020-05-01 12:16:21,411:INFO:Task 'variable': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'variable': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,424:INFO:Task 'variable': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'region': Starting task run... 2020-05-01 12:16:21,454:INFO:Task 'region': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'region': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,467:INFO:Task 'region': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'period': Starting task run... 2020-05-01 12:16:21,496:INFO:Task 'period': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'period': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,509:INFO:Task 'period': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'temporal': Starting task run... 2020-05-01 12:16:21,539:INFO:Task 'temporal': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'temporal': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,551:INFO:Task 'temporal': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'get_dataset': Starting task run... 2020-05-01 12:16:21,581:INFO:Task 'get_dataset': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_dataset': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,485:INFO:Task 'get_dataset': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_spatial_subset': Starting task run... 2020-05-01 12:16:22,505:INFO:Task 'cube_spatial_subset': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_spatial_subset': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,525:INFO:Task 'cube_spatial_subset': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:22,545:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,554:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_temporal_subset': Starting task run... 2020-05-01 12:16:22,574:INFO:Task 'cube_temporal_subset': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_temporal_subset': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,585:INFO:Task 'cube_temporal_subset': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_reference_cube': Starting task run... 2020-05-01 12:16:22,605:INFO:Task 'get_reference_cube': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_reference_cube': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,653:INFO:Task 'get_reference_cube': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'spatial': Starting task run... 2020-05-01 12:16:22,687:INFO:Task 'spatial': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'spatial': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,700:INFO:Task 'spatial': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_density_cubes': Starting task run... 2020-05-01 12:16:22,729:INFO:Task 'get_density_cubes': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_density_cubes': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,769:INFO:Task 'get_density_cubes': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_common_indices': Starting task run... 2020-05-01 12:16:22,799:INFO:Task 'get_common_indices': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'get_common_indices': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,766:INFO:Task 'get_common_indices': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,786:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,795:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,815:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,824:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'standardizer_data': Starting task run... 2020-05-01 12:16:23,843:INFO:Task 'standardizer_data': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'standardizer_data': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,855:INFO:Task 'standardizer_data': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,874:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,883:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,903:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,912:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'get_similarity_scores': Starting task run... 2020-05-01 12:16:23,931:INFO:Task 'get_similarity_scores': Starting task run... [2020-05-01 10:16:50] INFO - prefect.TaskRunner | Task 'get_similarity_scores': finished task run for task with final state: 'Success' 2020-05-01 12:16:50,094:INFO:Task 'get_similarity_scores': finished task run for task with final state: 'Success' [2020-05-01 10:16:50] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded 2020-05-01 12:16:50,097:INFO:Flow run SUCCESS: all reference tasks succeeded state . result [ res ] . result {'rv_coeff': 0.9697258, 'rv_x_norm': 26692.072, 'rv_y_norm': 77907.49, 'rv_xy_norm': 2016556900.0, 'rbig_H_x': 1.855240533094599, 'rbig_H_y': 1.1286197933913034, 'rbig_I_xy': 5.499353957238775, 'rbig_vi_coeff': 3.8004736863738287}","title":"3.1 exp1 walkthrough dask"},{"location":"notebooks/spatial_temporal/3.1_exp1_walkthrough_dask/#spatial-temporal-experiment","text":"In this notebook, I will be walking through how we can estimate different methods based on the density cubes that we derive. import sys , os from pyprojroot import here root = here ( project_files = [ \".here\" ]) sys . path . append ( str ( here ())) import pathlib # standard python packages import xarray as xr import pandas as pd import numpy as np # # Experiment Functions from src.data.esdc import get_dataset from src.features import Metrics from src.features.temporal import select_period , get_smoke_test_time from src.features.spatial import select_region , get_europe from src.models.train_models import get_similarity_scores from src.experiments.utils import dict_product , run_parallel_step from src.features import Metrics from src.features.density import get_density_cubes from src.features.preprocessing import standardizer_data , get_reference_cube , get_common_indices from src.models.similarity import cka_coefficient , rv_coefficient , rbig_it_measures # # esdc tools # from src.esdc.subset import select_pixel # from src.esdc.shape import ShapeFileExtract, rasterize # from esdc.transform import DensityCubes from typing import List , Dict import xarray as xr from tqdm import tqdm import cartopy import cartopy.crs as ccrs # NUMPY SETTINGS import numpy as onp onp . set_printoptions ( precision = 3 , suppress = True ) # MATPLOTLIB Settings import matplotlib as mpl import matplotlib.pyplot as plt % matplotlib inline % config InlineBackend . figure_format = 'retina' # SEABORN SETTINGS import seaborn as sns sns . set_context ( context = 'talk' , font_scale = 0.7 ) # sns.set(rc={'figure.figsize': (12, 9.)}) # sns.set_style(\"whitegrid\") # PANDAS SETTINGS import pandas as pd pd . set_option ( \"display.max_rows\" , 120 ) pd . set_option ( \"display.max_columns\" , 120 ) # LOGGING SETTINGS import sys import logging logging . basicConfig ( level = logging . INFO , stream = sys . stdout , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) logger = logging . getLogger () #logger.setLevel(logging.INFO) % load_ext autoreload % autoreload 2 The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload","title":"Spatial-Temporal Experiment"},{"location":"notebooks/spatial_temporal/3.1_exp1_walkthrough_dask/#experiment-steps","text":"","title":"Experiment Steps"},{"location":"notebooks/spatial_temporal/3.1_exp1_walkthrough_dask/#global-variables","text":"","title":"Global Variables"},{"location":"notebooks/spatial_temporal/3.1_exp1_walkthrough_dask/#parameters","text":"parameters = {} parameters [ 'variable' ] = [ 'gross_primary_productivity' , 'root_moisture' , 'land_surface_temperature' ] parameters [ 'region' ] = [ get_europe ()] parameters [ 'period' ] = [ get_smoke_test_time ()] parameters [ 'spatial' ] = [ 1 , 2 , 3 , 4 , 5 , 6 ] parameters [ 'temporal' ] = [ 1 , 2 , 3 , 4 , 5 , 6 ] # params = list(dict_product(parameters)) params = list ( dict_product ( parameters )) print ( len ( params )) smoke_test = True 108","title":"Parameters"},{"location":"notebooks/spatial_temporal/3.1_exp1_walkthrough_dask/#experimental-step","text":"def step ( params : Dict , smoke_test : bool = False ): # ====================== # experiment - Data # ====================== # Get DataCube datacube = get_dataset ( params [ 'variable' ]) # subset datacube (spatially) datacube = select_region ( xr_data = datacube , bbox = params [ 'region' ] )[ params [ 'variable' ]] # subset datacube (temporally) datacube = select_period ( xr_data = datacube , period = params [ 'period' ]) # get datacubes reference_cube_df = get_reference_cube ( data = datacube ) # get density cubes density_cube_df = get_density_cubes ( data = datacube , spatial = params [ 'spatial' ], temporal = params [ 'temporal' ] ) # get reference dataframe X , Y = get_common_indices ( reference_df = reference_cube_df , density_df = density_cube_df ) # standardize data X , Y = standardizer_data ( X = X , Y = Y ) # ====================== # experiment - Methods # ====================== res = get_similarity_scores ( X_ref = X , Y_compare = Y , smoke_test = smoke_test ) # Save Results results = pd . DataFrame ({ 'region' : params [ 'region' ] . name , 'period' : params [ 'period' ] . name , 'variable' : params [ 'variable' ], 'spatial' : params [ 'spatial' ], 'temporal' : params [ 'temporal' ], ** res }, index = [ 0 ]) return results results = step ( params [ 0 ], smoke_test = True ) results . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } region period variable spatial temporal rv_coeff rv_x_norm rv_y_norm rv_xy_norm rv_time rbig_H_x rbig_H_y rbig_H_time rbig_I_xy rbig_I_time rbig_vi_coeff 0 europe test_201007 gross_primary_productivity 1 1 1.0 100.801453 100.801453 10160.93457 0.00383 0.835917 0.835917 0.232885 25.802617 0.468356 30.867425 res {'rv_coeff': 0.9403951, 'rv_x_norm': 44288.883, 'rv_y_norm': 357065.0, 'rv_xy_norm': 14871418000.0, 'rv_time': 38.40082359313965, 'rbig_H_x': 1.9247062049009207, 'rbig_H_y': 4.4428976758211896, 'rbig_H_time': 3.0350914001464844, 'rbig_I_xy': 3.649542912651551, 'rbig_I_time': 31.18929934501648, 'rbig_vi_coeff': 1.2480244562128495} res {'rv_coeff': 0.9697245, 'rv_x_norm': 26692.102, 'rv_y_norm': 77907.13, 'rv_xy_norm': 2016547100.0, 'rbig_H_x': 1.855240533094599, 'rbig_H_y': 1.0902273375895914, 'rbig_I_xy': 5.405821100129361, 'rbig_vi_coeff': 3.801045104354525} from prefect import task , Flow , Parameter @task # get Dataset def get_dataset ( variable : str ) -> xr . Dataset : return xr . open_zarr ( str ( filename ))[[ variable ]] @task # subset datacube def cube_spatial_subset ( xr_data : xr . Dataset , bbox : Region ) -> xr . Dataset : \"\"\"Function to spatially subset an xarray dataset from a bounding box.\"\"\" # get bounding box bbox = shapely . geometry . box ( bbox . lonmin , bbox . latmin , bbox . lonmax , bbox . latmax ) # subset datacube return clip_dataset_by_geometry ( xr_data , bbox ) @task def cube_temporal_subset ( xr_data : xr . DataArray , period : Tuple [ str , str ]) -> xr . DataArray : \"\"\"Function to temporally subset an xarray dataset from a tuple of start date and end date \"\"\" return xr_data . sel ( time = slice ( period . start , period . end )) @task # get reference cube def get_reference_cube ( data : xr . DataArray ) -> pd . DataFrame : \"\"\"Wrapper Function to get reference cube\"\"\" return data . to_dataframe () . dropna () . reorder_levels ( levels ) @task # get density cubes def get_density_cubes ( data : xr . DataArray , spatial : int , temporal : int ) -> pd . DataFrame : \"\"\"Wrapper Function to get density cubes from a dataarray\"\"\" return DensityCubes ( spatial_window = spatial , time_window = temporal ) . get_minicubes ( data ) . reorder_levels ( levels ) @task # get common indices def get_common_indices ( reference_df : pd . DataFrame , density_df : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: idx = density_df . index . intersection ( reference_df . index ) return reference_df . loc [ idx ,:], density_df . loc [ idx , :] @task # standardize the data before def standardizer_data ( X : pd . DataFrame , Y : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: # standardizer normalizer = StandardScaler ( with_mean = True , with_std = True ) # standardize X values X_values = normalizer . fit_transform ( X . values ) X = pd . DataFrame ( data = X_values , index = X . index , columns = X . columns ) # standardize Y Values Y_values = normalizer . fit_transform ( Y . values ) Y = pd . DataFrame ( data = Y_values , index = Y . index , columns = Y . columns ) return X , Y @task def get_similarity_scores ( X_ref : pd . DataFrame , Y_compare : pd . DataFrame ) -> Dict : # RV Coefficient rv_results = rv_coefficient ( X_ref , Y_compare ) # # CKA Coefficient # cka_results = cka_coefficient(X_ref, Y_compare) # RBIG Coefficient rbig_results = rbig_it_measures ( X_ref , Y_compare ) results = { ** rv_results , # **cka_results, ** rbig_results } return results","title":"Experimental Step"},{"location":"notebooks/spatial_temporal/3.1_exp1_walkthrough_dask/#experiment-run","text":"# variable = 'gross_primary_productivity' # region = get_europe() # datacube = get_dataset(variable) # datacube = subset_cube(xr_data=datacube, bbox=region) logger . setLevel ( logging . INFO ) with Flow ( \"Experiment-Step\" ) as flow : # ====================== # experiment parameters # ====================== variable = Parameter ( \"variable\" , default = 'gross_primary_productivity' ) region = Parameter ( \"region\" , default = get_europe ()) period = Parameter ( \"period\" , default = get_test_time ()) spatial = Parameter ( \"spatial\" , default = 1 ) temporal = Parameter ( \"temporal\" , default = 3 ) # ====================== # experiment - Data # ====================== # Get DataCube datacube = get_dataset ( variable ) # subset datacube (spatially) datacube = cube_spatial_subset ( xr_data = datacube , bbox = region )[ variable ] # subset datacube (temporally) datacube = cube_temporal_subset ( xr_data = datacube , period = period ) # get datacubes reference_cube_df = get_reference_cube ( data = datacube ) # get density cubes density_cube_df = get_density_cubes ( data = datacube , spatial = spatial , temporal = temporal ) # get reference dataframe dfs = get_common_indices ( reference_df = reference_cube_df , density_df = density_cube_df ) # standardize data dfs = standardizer_data ( X = dfs [ 0 ], Y = dfs [ 1 ]) # ====================== # experiment - Methods # ====================== res = get_similarity_scores ( X_ref = dfs [ 0 ], Y_compare = dfs [ 1 ]) state = flow . run () [2020-05-01 10:16:21] INFO - prefect.FlowRunner | Beginning Flow run for 'Experiment-Step' 2020-05-01 12:16:21,361:INFO:Beginning Flow run for 'Experiment-Step' [2020-05-01 10:16:21] INFO - prefect.FlowRunner | Starting flow run. 2020-05-01 12:16:21,372:INFO:Starting flow run. [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'variable': Starting task run... 2020-05-01 12:16:21,411:INFO:Task 'variable': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'variable': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,424:INFO:Task 'variable': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'region': Starting task run... 2020-05-01 12:16:21,454:INFO:Task 'region': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'region': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,467:INFO:Task 'region': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'period': Starting task run... 2020-05-01 12:16:21,496:INFO:Task 'period': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'period': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,509:INFO:Task 'period': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'temporal': Starting task run... 2020-05-01 12:16:21,539:INFO:Task 'temporal': Starting task run... [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'temporal': finished task run for task with final state: 'Success' 2020-05-01 12:16:21,551:INFO:Task 'temporal': finished task run for task with final state: 'Success' [2020-05-01 10:16:21] INFO - prefect.TaskRunner | Task 'get_dataset': Starting task run... 2020-05-01 12:16:21,581:INFO:Task 'get_dataset': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_dataset': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,485:INFO:Task 'get_dataset': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_spatial_subset': Starting task run... 2020-05-01 12:16:22,505:INFO:Task 'cube_spatial_subset': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_spatial_subset': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,525:INFO:Task 'cube_spatial_subset': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:22,545:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,554:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_temporal_subset': Starting task run... 2020-05-01 12:16:22,574:INFO:Task 'cube_temporal_subset': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'cube_temporal_subset': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,585:INFO:Task 'cube_temporal_subset': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_reference_cube': Starting task run... 2020-05-01 12:16:22,605:INFO:Task 'get_reference_cube': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_reference_cube': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,653:INFO:Task 'get_reference_cube': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'spatial': Starting task run... 2020-05-01 12:16:22,687:INFO:Task 'spatial': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'spatial': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,700:INFO:Task 'spatial': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_density_cubes': Starting task run... 2020-05-01 12:16:22,729:INFO:Task 'get_density_cubes': Starting task run... [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_density_cubes': finished task run for task with final state: 'Success' 2020-05-01 12:16:22,769:INFO:Task 'get_density_cubes': finished task run for task with final state: 'Success' [2020-05-01 10:16:22] INFO - prefect.TaskRunner | Task 'get_common_indices': Starting task run... 2020-05-01 12:16:22,799:INFO:Task 'get_common_indices': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'get_common_indices': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,766:INFO:Task 'get_common_indices': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,786:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,795:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,815:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,824:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'standardizer_data': Starting task run... 2020-05-01 12:16:23,843:INFO:Task 'standardizer_data': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'standardizer_data': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,855:INFO:Task 'standardizer_data': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,874:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,883:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': Starting task run... 2020-05-01 12:16:23,903:INFO:Task 'GetItem': Starting task run... [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'GetItem': finished task run for task with final state: 'Success' 2020-05-01 12:16:23,912:INFO:Task 'GetItem': finished task run for task with final state: 'Success' [2020-05-01 10:16:23] INFO - prefect.TaskRunner | Task 'get_similarity_scores': Starting task run... 2020-05-01 12:16:23,931:INFO:Task 'get_similarity_scores': Starting task run... [2020-05-01 10:16:50] INFO - prefect.TaskRunner | Task 'get_similarity_scores': finished task run for task with final state: 'Success' 2020-05-01 12:16:50,094:INFO:Task 'get_similarity_scores': finished task run for task with final state: 'Success' [2020-05-01 10:16:50] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded 2020-05-01 12:16:50,097:INFO:Flow run SUCCESS: all reference tasks succeeded state . result [ res ] . result {'rv_coeff': 0.9697258, 'rv_x_norm': 26692.072, 'rv_y_norm': 77907.49, 'rv_xy_norm': 2016556900.0, 'rbig_H_x': 1.855240533094599, 'rbig_H_y': 1.1286197933913034, 'rbig_I_xy': 5.499353957238775, 'rbig_vi_coeff': 3.8004736863738287}","title":"Experiment Run"},{"location":"notebooks/spatial_temporal/3.2_exp1_results/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Spatial-Temporal Experiment \u00b6 In this notebook, I will be walking through how we can estimate different methods based on the density cubes that we derive. import sys , os from pyprojroot import here root = here ( project_files = [ \".here\" ]) sys . path . append ( str ( here ())) import pathlib # standard python packages import xarray as xr import pandas as pd import numpy as np # # Experiment Functions from src.data.esdc import get_dataset from src.features import Metrics from src.features.temporal import select_period , get_smoke_test_time from src.features.spatial import select_region , get_europe from src.models.train_models import get_similarity_scores from src.experiments.utils import dict_product , run_parallel_step from src.features import Metrics from src.features.density import get_density_cubes from src.features.preprocessing import standardizer_data , get_reference_cube , get_common_indices from src.models.similarity import cka_coefficient , rv_coefficient , rbig_it_measures # # esdc tools # from src.esdc.subset import select_pixel # from src.esdc.shape import ShapeFileExtract, rasterize # from esdc.transform import DensityCubes from typing import List , Dict import xarray as xr from tqdm import tqdm import cartopy import cartopy.crs as ccrs # NUMPY SETTINGS import numpy as onp onp . set_printoptions ( precision = 3 , suppress = True ) # MATPLOTLIB Settings import matplotlib as mpl import matplotlib.pyplot as plt % matplotlib inline % config InlineBackend . figure_format = 'retina' # SEABORN SETTINGS import seaborn as sns sns . set_context ( context = 'talk' , font_scale = 0.7 ) # sns.set(rc={'figure.figsize': (12, 9.)}) # sns.set_style(\"whitegrid\") # PANDAS SETTINGS import pandas as pd pd . set_option ( \"display.max_rows\" , 120 ) pd . set_option ( \"display.max_columns\" , 120 ) # LOGGING SETTINGS import sys import logging logging . basicConfig ( level = logging . INFO , stream = sys . stdout , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) logger = logging . getLogger () #logger.setLevel(logging.INFO) % load_ext autoreload % autoreload 2 Results Plot \u00b6 For this first set of results, I'm looking at the following: Entropy of the different spatial-temporal density cube compositions. MI between just the sames with not spatial-temporal features and the density cubes with different spatiel-temporal compositions. Hypothesis Less mutual information with more features I'm not sure about entropy. RES_PATH = pathlib . Path ( str ( root )) . joinpath ( \"data/spa_temp/trial_experiment/v1_gpp_europe.csv\" ) results = pd . read_csv ( str ( RES_PATH )) # results['ratio'] = results['spatial'] / results['temporal'] # results.tail() Entropy \u00b6 sub_df = results . copy () sub_df [ 'ratio' ] = sub_df [ 'spatial' ] / sub_df [ 'temporal' ] sub_df [ 'dimensions' ] = sub_df [ 'spatial' ] ** 2 + sub_df [ 'temporal' ] sub_df [ 'nH' ] = ( sub_df [ 'rbig_H_y' ] / sub_df [ 'dimensions' ]) #* np.log(2) sub_df [ \"spatial\" ] = sub_df [ \"spatial\" ] . astype ( 'category' ) sub_df [ \"temporal\" ] = sub_df [ \"temporal\" ] . astype ( 'category' ) fig , ax = plt . subplots ( ncols = 2 , figsize = ( 10 , 5 )) sns . scatterplot ( x = 'ratio' , y = 'nH' , hue = 'spatial' , data = sub_df , ax = ax [ 0 ], label = 'Entropy' ,) sns . scatterplot ( x = 'ratio' , y = 'nH' , hue = 'temporal' , data = sub_df , ax = ax [ 1 ], label = 'Entropy' ,) # sns.scatterplot(x='ratio', y='rv_coeff', data=sub_df, ax=ax, label='RV') ax [ 0 ] . set_xscale ( 'log' ), ax [ 1 ] . set_xscale ( 'log' ) ax [ 0 ] . set_xlabel ( 'Spatial / Temporal Ratio' ), ax [ 1 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) ax [ 0 ] . set_ylabel ( 'Normalized Entropy' ), ax [ 1 ] . set_ylabel ( '' ) plt . show () So we see that the configuration with more features has less entropy? Mutual Information \u00b6 Rescaling MI \u00b6 There is a problem because the entropy has a lot of negative values. So we're going to rescale the entropy such that it is above 0. However...I'm not sure what to do about the MI because I feel like we should definitely rescale that as well. Some ideas given from this paper [pg 740, eq. 6] include: I_c(\\mathbf{X;Y}) = \\sqrt{1 - \\exp\\left( -2 I(\\mathbf{X;Y}) \\right)} I_c(\\mathbf{X;Y}) = \\sqrt{1 - \\exp\\left( -2 I(\\mathbf{X;Y}) \\right)} which takes inspiration from the equation from [Cover and Thomas, 1991]. I(X;Y) = -\\frac{1}{2} \\log(1-\\rho^2) I(X;Y) = -\\frac{1}{2} \\log(1-\\rho^2) and they essentially solved this equation for \\rho \\rho . sub_df [ 'nI2' ] = np . sqrt ( 1 - np . exp ( - 2 * sub_df [ 'rbig_I_xy' ])) fig , ax = plt . subplots ( ncols = 2 , figsize = ( 10 , 5 )) sns . scatterplot ( x = 'ratio' , y = 'nI2' , data = sub_df , hue = 'spatial' , ax = ax [ 0 ], label = 'nMI' ) sns . scatterplot ( x = 'ratio' , y = 'nI2' , data = sub_df , hue = 'temporal' , ax = ax [ 1 ], label = 'nMI' ) # sns.scatterplot(x='ratio', y='rv_coeff', data=sub_df, ax=ax[1], label='RV') # ax[0,0].set_yscale('log') ax [ 0 ] . set_xscale ( 'log' ) ax [ 0 ] . set_ylabel ( 'Normalized Mutual Information' ) ax [ 0 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) # ax[0, 1].set_yscale('log') ax [ 1 ] . set_xscale ( 'log' ) ax [ 1 ] . set_ylabel ( '' ) ax [ 1 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) Text(0.5, 0, 'Spatial / Temporal Ratio') I don't think this makes very much sense... We have points with low spatial dimensions and low temporal dimensions that Update : I guess another interpretation is technically, I'm seeing more features and thus are more realizations (with perturbations) of my observations. So maybe there should be more mutual information. Especially if it's redundant. What I don't like is that we're not using the entropy measures. So I'll try another one. This one I found in this paper [pg. 2842, table 3]. d_\\text{max}(I;Y) = 1 - \\frac{I(X;Y)}{\\text{max}\\left\\{ H(X),H(Y) \\right\\}} d_\\text{max}(I;Y) = 1 - \\frac{I(X;Y)}{\\text{max}\\left\\{ H(X),H(Y) \\right\\}} This one at least has access to the entropy measures. sub_df [ 'nI2' ] = 1 - sub_df [ 'rbig_I_xy' ] / np . maximum ( sub_df [ 'rbig_H_x' ], sub_df [ 'rbig_H_y' ]) fig , ax = plt . subplots ( ncols = 2 , figsize = ( 10 , 5 )) sns . scatterplot ( x = 'ratio' , y = 'nI2' , data = sub_df , hue = 'spatial' , ax = ax [ 0 ], label = 'nMI' ) sns . scatterplot ( x = 'ratio' , y = 'nI2' , data = sub_df , hue = 'temporal' , ax = ax [ 1 ], label = 'nMI' ) # sns.scatterplot(x='ratio', y='rv_coeff', data=sub_df, ax=ax[1], label='RV') # ax[0,0].set_yscale('log') ax [ 0 ] . set_xscale ( 'log' ) ax [ 0 ] . set_ylabel ( 'Normalized Mutual Information' ) ax [ 0 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) # ax[0, 1].set_yscale('log') ax [ 1 ] . set_xscale ( 'log' ) ax [ 1 ] . set_ylabel ( '' ) ax [ 1 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) Text(0.5, 0, 'Spatial / Temporal Ratio') So this one makes a lot more sense to me. I expected that we would get something where we see more MI amongst data already seen and less MI with more features because there are unseen cases. So I quite like this setup. Also, apparently the distance measure is an actual metric that satisfies the key properties. So this is also another plus. Now, later, I will have to figure out how this ties into the Taylor Diagram. But for now, I'm quite happy. from sklearn.preprocessing import MinMaxScaler # find the min,max of all entropy values min_H = sub_df [[ 'rbig_H_x' , 'rbig_H_y' ]] . min () . min () max_H = sub_df [[ 'rbig_H_x' , 'rbig_H_y' ]] . max () . max () # Scale between 0 and the max value? sub_df [ 'rbig_H_xs' ] = MinMaxScaler (( 0 , max_H )) . fit_transform ( sub_df [ 'rbig_H_x' ] . values [:, None ]) sub_df [ 'rbig_H_ys' ] = MinMaxScaler (( 0 , max_H )) . fit_transform ( sub_df [ 'rbig_H_y' ] . values [:, None ]) # calculate normalized MI with the scaled versions sub_df [ 'nI' ] = ( sub_df [ 'rbig_I_xy' ] / np . sqrt ( sub_df [ 'rbig_H_xs' ]) / np . sqrt ( sub_df [ 'rbig_H_ys' ])) #* np.log(2) fig , ax = plt . subplots ( ncols = 2 , nrows = 2 , figsize = ( 10 , 10 )) sns . scatterplot ( x = 'ratio' , y = 'nI2' , data = sub_df , hue = 'spatial' , ax = ax [ 0 , 0 ], label = 'nMI' ) sns . scatterplot ( x = 'ratio' , y = 'nI2' , data = sub_df , hue = 'temporal' , ax = ax [ 0 , 1 ], label = 'nMI' ) # sns.scatterplot(x='ratio', y='rv_coeff', data=sub_df, ax=ax[1], label='RV') # ax[0,0].set_yscale('log') ax [ 0 , 0 ] . set_xscale ( 'log' ) ax [ 0 , 0 ] . set_ylabel ( 'Normalized Mutual Information' ) ax [ 0 , 0 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) # ax[0, 1].set_yscale('log') ax [ 0 , 1 ] . set_xscale ( 'log' ) ax [ 0 , 1 ] . set_ylabel ( '' ) ax [ 0 , 1 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) sns . scatterplot ( x = 'ratio' , y = 'rv_coeff' , data = sub_df , hue = 'spatial' , ax = ax [ 1 , 0 ], label = 'nMI' ) sns . scatterplot ( x = 'ratio' , y = 'rv_coeff' , data = sub_df , hue = 'temporal' , ax = ax [ 1 , 1 ], label = 'nMI' ) # sns.scatterplot(x='ratio', y='rv_coeff', data=sub_df, ax=ax[1], label='RV') ax [ 1 , 0 ] . set_yscale ( 'log' ) ax [ 1 , 0 ] . set_xscale ( 'log' ) ax [ 1 , 0 ] . set_ylabel ( 'Coefficient' ) ax [ 1 , 0 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) ax [ 1 , 1 ] . set_yscale ( 'log' ) ax [ 1 , 1 ] . set_xscale ( 'log' ) ax [ 1 , 1 ] . set_ylabel ( '' ) ax [ 1 , 1 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) plt . show () RV Coefficient So right away, we see that the RV Coefficient doesn't fluxuate at all (or at least it is rather random and I don't see a clear pattern. Mutual Information MI clearly has a pattern. That's good, it's picking up something about the feature representations. The only problem is it's difficult to interpret because it's not really between 0 and 1, so I'm not sure how we're supposed to use this in a Taylor Diagram.","title":"3.2 exp1 results"},{"location":"notebooks/spatial_temporal/3.2_exp1_results/#spatial-temporal-experiment","text":"In this notebook, I will be walking through how we can estimate different methods based on the density cubes that we derive. import sys , os from pyprojroot import here root = here ( project_files = [ \".here\" ]) sys . path . append ( str ( here ())) import pathlib # standard python packages import xarray as xr import pandas as pd import numpy as np # # Experiment Functions from src.data.esdc import get_dataset from src.features import Metrics from src.features.temporal import select_period , get_smoke_test_time from src.features.spatial import select_region , get_europe from src.models.train_models import get_similarity_scores from src.experiments.utils import dict_product , run_parallel_step from src.features import Metrics from src.features.density import get_density_cubes from src.features.preprocessing import standardizer_data , get_reference_cube , get_common_indices from src.models.similarity import cka_coefficient , rv_coefficient , rbig_it_measures # # esdc tools # from src.esdc.subset import select_pixel # from src.esdc.shape import ShapeFileExtract, rasterize # from esdc.transform import DensityCubes from typing import List , Dict import xarray as xr from tqdm import tqdm import cartopy import cartopy.crs as ccrs # NUMPY SETTINGS import numpy as onp onp . set_printoptions ( precision = 3 , suppress = True ) # MATPLOTLIB Settings import matplotlib as mpl import matplotlib.pyplot as plt % matplotlib inline % config InlineBackend . figure_format = 'retina' # SEABORN SETTINGS import seaborn as sns sns . set_context ( context = 'talk' , font_scale = 0.7 ) # sns.set(rc={'figure.figsize': (12, 9.)}) # sns.set_style(\"whitegrid\") # PANDAS SETTINGS import pandas as pd pd . set_option ( \"display.max_rows\" , 120 ) pd . set_option ( \"display.max_columns\" , 120 ) # LOGGING SETTINGS import sys import logging logging . basicConfig ( level = logging . INFO , stream = sys . stdout , format = ' %(asctime)s : %(levelname)s : %(message)s ' ) logger = logging . getLogger () #logger.setLevel(logging.INFO) % load_ext autoreload % autoreload 2","title":"Spatial-Temporal Experiment"},{"location":"notebooks/spatial_temporal/3.2_exp1_results/#results-plot","text":"For this first set of results, I'm looking at the following: Entropy of the different spatial-temporal density cube compositions. MI between just the sames with not spatial-temporal features and the density cubes with different spatiel-temporal compositions. Hypothesis Less mutual information with more features I'm not sure about entropy. RES_PATH = pathlib . Path ( str ( root )) . joinpath ( \"data/spa_temp/trial_experiment/v1_gpp_europe.csv\" ) results = pd . read_csv ( str ( RES_PATH )) # results['ratio'] = results['spatial'] / results['temporal'] # results.tail()","title":"Results Plot"},{"location":"notebooks/spatial_temporal/3.2_exp1_results/#entropy","text":"sub_df = results . copy () sub_df [ 'ratio' ] = sub_df [ 'spatial' ] / sub_df [ 'temporal' ] sub_df [ 'dimensions' ] = sub_df [ 'spatial' ] ** 2 + sub_df [ 'temporal' ] sub_df [ 'nH' ] = ( sub_df [ 'rbig_H_y' ] / sub_df [ 'dimensions' ]) #* np.log(2) sub_df [ \"spatial\" ] = sub_df [ \"spatial\" ] . astype ( 'category' ) sub_df [ \"temporal\" ] = sub_df [ \"temporal\" ] . astype ( 'category' ) fig , ax = plt . subplots ( ncols = 2 , figsize = ( 10 , 5 )) sns . scatterplot ( x = 'ratio' , y = 'nH' , hue = 'spatial' , data = sub_df , ax = ax [ 0 ], label = 'Entropy' ,) sns . scatterplot ( x = 'ratio' , y = 'nH' , hue = 'temporal' , data = sub_df , ax = ax [ 1 ], label = 'Entropy' ,) # sns.scatterplot(x='ratio', y='rv_coeff', data=sub_df, ax=ax, label='RV') ax [ 0 ] . set_xscale ( 'log' ), ax [ 1 ] . set_xscale ( 'log' ) ax [ 0 ] . set_xlabel ( 'Spatial / Temporal Ratio' ), ax [ 1 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) ax [ 0 ] . set_ylabel ( 'Normalized Entropy' ), ax [ 1 ] . set_ylabel ( '' ) plt . show () So we see that the configuration with more features has less entropy?","title":"Entropy"},{"location":"notebooks/spatial_temporal/3.2_exp1_results/#mutual-information","text":"","title":"Mutual Information"},{"location":"notebooks/spatial_temporal/3.2_exp1_results/#rescaling-mi","text":"There is a problem because the entropy has a lot of negative values. So we're going to rescale the entropy such that it is above 0. However...I'm not sure what to do about the MI because I feel like we should definitely rescale that as well. Some ideas given from this paper [pg 740, eq. 6] include: I_c(\\mathbf{X;Y}) = \\sqrt{1 - \\exp\\left( -2 I(\\mathbf{X;Y}) \\right)} I_c(\\mathbf{X;Y}) = \\sqrt{1 - \\exp\\left( -2 I(\\mathbf{X;Y}) \\right)} which takes inspiration from the equation from [Cover and Thomas, 1991]. I(X;Y) = -\\frac{1}{2} \\log(1-\\rho^2) I(X;Y) = -\\frac{1}{2} \\log(1-\\rho^2) and they essentially solved this equation for \\rho \\rho . sub_df [ 'nI2' ] = np . sqrt ( 1 - np . exp ( - 2 * sub_df [ 'rbig_I_xy' ])) fig , ax = plt . subplots ( ncols = 2 , figsize = ( 10 , 5 )) sns . scatterplot ( x = 'ratio' , y = 'nI2' , data = sub_df , hue = 'spatial' , ax = ax [ 0 ], label = 'nMI' ) sns . scatterplot ( x = 'ratio' , y = 'nI2' , data = sub_df , hue = 'temporal' , ax = ax [ 1 ], label = 'nMI' ) # sns.scatterplot(x='ratio', y='rv_coeff', data=sub_df, ax=ax[1], label='RV') # ax[0,0].set_yscale('log') ax [ 0 ] . set_xscale ( 'log' ) ax [ 0 ] . set_ylabel ( 'Normalized Mutual Information' ) ax [ 0 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) # ax[0, 1].set_yscale('log') ax [ 1 ] . set_xscale ( 'log' ) ax [ 1 ] . set_ylabel ( '' ) ax [ 1 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) Text(0.5, 0, 'Spatial / Temporal Ratio') I don't think this makes very much sense... We have points with low spatial dimensions and low temporal dimensions that Update : I guess another interpretation is technically, I'm seeing more features and thus are more realizations (with perturbations) of my observations. So maybe there should be more mutual information. Especially if it's redundant. What I don't like is that we're not using the entropy measures. So I'll try another one. This one I found in this paper [pg. 2842, table 3]. d_\\text{max}(I;Y) = 1 - \\frac{I(X;Y)}{\\text{max}\\left\\{ H(X),H(Y) \\right\\}} d_\\text{max}(I;Y) = 1 - \\frac{I(X;Y)}{\\text{max}\\left\\{ H(X),H(Y) \\right\\}} This one at least has access to the entropy measures. sub_df [ 'nI2' ] = 1 - sub_df [ 'rbig_I_xy' ] / np . maximum ( sub_df [ 'rbig_H_x' ], sub_df [ 'rbig_H_y' ]) fig , ax = plt . subplots ( ncols = 2 , figsize = ( 10 , 5 )) sns . scatterplot ( x = 'ratio' , y = 'nI2' , data = sub_df , hue = 'spatial' , ax = ax [ 0 ], label = 'nMI' ) sns . scatterplot ( x = 'ratio' , y = 'nI2' , data = sub_df , hue = 'temporal' , ax = ax [ 1 ], label = 'nMI' ) # sns.scatterplot(x='ratio', y='rv_coeff', data=sub_df, ax=ax[1], label='RV') # ax[0,0].set_yscale('log') ax [ 0 ] . set_xscale ( 'log' ) ax [ 0 ] . set_ylabel ( 'Normalized Mutual Information' ) ax [ 0 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) # ax[0, 1].set_yscale('log') ax [ 1 ] . set_xscale ( 'log' ) ax [ 1 ] . set_ylabel ( '' ) ax [ 1 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) Text(0.5, 0, 'Spatial / Temporal Ratio') So this one makes a lot more sense to me. I expected that we would get something where we see more MI amongst data already seen and less MI with more features because there are unseen cases. So I quite like this setup. Also, apparently the distance measure is an actual metric that satisfies the key properties. So this is also another plus. Now, later, I will have to figure out how this ties into the Taylor Diagram. But for now, I'm quite happy. from sklearn.preprocessing import MinMaxScaler # find the min,max of all entropy values min_H = sub_df [[ 'rbig_H_x' , 'rbig_H_y' ]] . min () . min () max_H = sub_df [[ 'rbig_H_x' , 'rbig_H_y' ]] . max () . max () # Scale between 0 and the max value? sub_df [ 'rbig_H_xs' ] = MinMaxScaler (( 0 , max_H )) . fit_transform ( sub_df [ 'rbig_H_x' ] . values [:, None ]) sub_df [ 'rbig_H_ys' ] = MinMaxScaler (( 0 , max_H )) . fit_transform ( sub_df [ 'rbig_H_y' ] . values [:, None ]) # calculate normalized MI with the scaled versions sub_df [ 'nI' ] = ( sub_df [ 'rbig_I_xy' ] / np . sqrt ( sub_df [ 'rbig_H_xs' ]) / np . sqrt ( sub_df [ 'rbig_H_ys' ])) #* np.log(2) fig , ax = plt . subplots ( ncols = 2 , nrows = 2 , figsize = ( 10 , 10 )) sns . scatterplot ( x = 'ratio' , y = 'nI2' , data = sub_df , hue = 'spatial' , ax = ax [ 0 , 0 ], label = 'nMI' ) sns . scatterplot ( x = 'ratio' , y = 'nI2' , data = sub_df , hue = 'temporal' , ax = ax [ 0 , 1 ], label = 'nMI' ) # sns.scatterplot(x='ratio', y='rv_coeff', data=sub_df, ax=ax[1], label='RV') # ax[0,0].set_yscale('log') ax [ 0 , 0 ] . set_xscale ( 'log' ) ax [ 0 , 0 ] . set_ylabel ( 'Normalized Mutual Information' ) ax [ 0 , 0 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) # ax[0, 1].set_yscale('log') ax [ 0 , 1 ] . set_xscale ( 'log' ) ax [ 0 , 1 ] . set_ylabel ( '' ) ax [ 0 , 1 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) sns . scatterplot ( x = 'ratio' , y = 'rv_coeff' , data = sub_df , hue = 'spatial' , ax = ax [ 1 , 0 ], label = 'nMI' ) sns . scatterplot ( x = 'ratio' , y = 'rv_coeff' , data = sub_df , hue = 'temporal' , ax = ax [ 1 , 1 ], label = 'nMI' ) # sns.scatterplot(x='ratio', y='rv_coeff', data=sub_df, ax=ax[1], label='RV') ax [ 1 , 0 ] . set_yscale ( 'log' ) ax [ 1 , 0 ] . set_xscale ( 'log' ) ax [ 1 , 0 ] . set_ylabel ( 'Coefficient' ) ax [ 1 , 0 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) ax [ 1 , 1 ] . set_yscale ( 'log' ) ax [ 1 , 1 ] . set_xscale ( 'log' ) ax [ 1 , 1 ] . set_ylabel ( '' ) ax [ 1 , 1 ] . set_xlabel ( 'Spatial / Temporal Ratio' ) plt . show () RV Coefficient So right away, we see that the RV Coefficient doesn't fluxuate at all (or at least it is rather random and I don't see a clear pattern. Mutual Information MI clearly has a pattern. That's good, it's picking up something about the feature representations. The only problem is it's difficult to interpret because it's not really between 0 and 1, so I'm not sure how we're supposed to use this in a Taylor Diagram.","title":"Rescaling MI"},{"location":"notebooks/spatial_temporal/Untitled/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }());","title":"Untitled"}]}